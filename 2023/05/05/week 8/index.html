<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>week8&amp;9 | public blog of tianxiang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="week 8&amp;9Goal(这周的目标) rl核心  基于梯度下降的内核 q表怎么扫，环境中什么轨迹，怎么采样，怎么更新 神经网络本身特性带来的一些东西 神经网络的作用就是状态和动作空间太大，那我们给他缩小，最后我们用缩小的q表去采取动作（generalize function） generalize func的性能没有NN好   而pg只是基于价值大小提高动作概率，只是argmax的一种连">
<meta property="og:type" content="article">
<meta property="og:title" content="week8&amp;9">
<meta property="og:url" content="http://magiclucky1996.github.io/2023/05/05/week%208/index.html">
<meta property="og:site_name" content="public blog of tianxiang">
<meta property="og:description" content="week 8&amp;9Goal(这周的目标) rl核心  基于梯度下降的内核 q表怎么扫，环境中什么轨迹，怎么采样，怎么更新 神经网络本身特性带来的一些东西 神经网络的作用就是状态和动作空间太大，那我们给他缩小，最后我们用缩小的q表去采取动作（generalize function） generalize func的性能没有NN好   而pg只是基于价值大小提高动作概率，只是argmax的一种连">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230508115549166.png">
<meta property="article:published_time" content="2023-05-05T09:19:40.000Z">
<meta property="article:modified_time" content="2023-05-15T09:39:35.633Z">
<meta property="article:author" content="tianxiang">
<meta property="article:tag" content="work">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230508115549166.png">
  
    <link rel="alternate" href="/atom.xml" title="public blog of tianxiang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">public blog of tianxiang</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://magiclucky1996.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-week 8" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/week%208/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      week8&amp;9
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="week-8-amp-9"><a href="#week-8-amp-9" class="headerlink" title="week 8&amp;9"></a>week 8&amp;9</h1><h3 id="Goal-这周的目标"><a href="#Goal-这周的目标" class="headerlink" title="Goal(这周的目标)"></a>Goal(这周的目标)</h3><ul>
<li><p>rl核心</p>
<ul>
<li><strong>基于梯度下降的内核</strong></li>
<li><strong>q表怎么扫，环境中什么轨迹，怎么采样，怎么更新</strong></li>
<li><strong>神经网络本身特性带来的一些东西</strong></li>
<li><strong>神经网络的作用就是状态和动作空间太大，那我们给他缩小，最后我们用缩小的q表去采取动作（generalize function）</strong><ul>
<li>generalize func的性能没有NN好</li>
</ul>
</li>
<li>而pg只是基于价值大小提高动作概率，只是argmax的一种连续版本</li>
</ul>
</li>
<li><p>群体学习的解决方式（我的目的不是发论文，而是探索，mastering）</p>
</li>
<li><p>随机性的问题，也就是探索的问题，探索环境状态的问题</p>
</li>
<li><p>playing atari中的提到的神经网络的优点</p>
</li>
<li><p>huggingface中提到的深度神经网络的优点</p>
<ul>
<li>q表在大型空间中无效（为啥，可以缩小，模糊神经网络）</li>
</ul>
</li>
<li><p>rlbook中提到神经网络的优点</p>
</li>
<li><p>把搜索方法应用到对mdp空间的探索中，比如树搜索，比如rrt，比如采样方法</p>
<ul>
<li>我们基于简单的mdp找出最佳的探索方法，再应用到复杂的，无法画出关系图的mdp中</li>
<li><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230508115549166.png" alt="image-20230508115549166"></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left"></th>
<th>星期一</th>
<th>星期二</th>
<th>星期三</th>
<th>星期四</th>
</tr>
</thead>
<tbody><tr>
<td align="left">抱脸虫教程（往后放，先解决问题）</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">在env测试不同算法进行对比</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">做一个图对比各种采样的策略：softmax，epsilon greedy，fully explore</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">做一个图对比各种对模型的更新</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">对比使用神经网路和不使用神经网络（缩小的q表，q也可以根据价值选取动作概率，，所以为什么使用神经网络）</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Q1：-why-neural-network？"><a href="#Q1：-why-neural-network？" class="headerlink" title="Q1： why neural network？"></a>Q1： why neural network？</h3><h6 id="q-table-could-be-approximate-actor-could-be-like-argmax"><a href="#q-table-could-be-approximate-actor-could-be-like-argmax" class="headerlink" title="q table could be approximate, actor could be like argmax"></a>q table could be approximate, actor could be like argmax</h6><ul>
<li><h2 id="rlbook-Generalize-problems-has-already-been-studied-generalization-when-a-single-state-is-updated-the-change-generalizes-from-that-state-to-a↵ect-the-values-of-many-other-states-supervised-learning-from-examples-including-artiﬁcial-neural-networks-decision-trees-and-various-kinds-of-multivariate-regression-not-all-function-approximation-methods-well-suited-for-rl-If-online-needed-to-learn-efficiently-from-incrementally-acquired-data-target-is-nonstationery（policy-is-not-stationery-td-is-not-stationery）"><a href="#rlbook-Generalize-problems-has-already-been-studied-generalization-when-a-single-state-is-updated-the-change-generalizes-from-that-state-to-a↵ect-the-values-of-many-other-states-supervised-learning-from-examples-including-artiﬁcial-neural-networks-decision-trees-and-various-kinds-of-multivariate-regression-not-all-function-approximation-methods-well-suited-for-rl-If-online-needed-to-learn-efficiently-from-incrementally-acquired-data-target-is-nonstationery（policy-is-not-stationery-td-is-not-stationery）" class="headerlink" title="rlbook- Generalize problems has already been studied- generalization: when a single state is updated, the change generalizes from that state to a↵ect the values of many other states- supervised learning from examples, including artiﬁcial neural networks, decision trees, and various kinds of multivariate regression- not all function approximation methods well suited for rl  - If online, needed to learn efficiently from incrementally acquired data  - target is nonstationery（policy is not stationery, td is not stationery）"></a>rlbook<br>- Generalize problems has already been studied<br>- generalization: when a single state is updated, the change generalizes from that state to a↵ect the values of many other states<br>- supervised learning from examples, including artiﬁcial neural networks, decision trees, and various kinds of multivariate regression<br>- not all function approximation methods well suited for rl<br>  - If online, needed to learn efficiently from incrementally acquired data<br>  - target is nonstationery（policy is not stationery, td is not stationery）</h2></li>
<li><p>DQN paper</p>
</li>
<li><p>chatgpt</p>
</li>
<li><p>nn could represent more complex relationships: so it’s more complex than just 在q 表中取近似值</p>
</li>
</ul>
<h3 id="Q2-the-explore-of-state-action-space"><a href="#Q2-the-explore-of-state-action-space" class="headerlink" title="Q2: the explore of  state-action space:"></a>Q2: the explore of  state-action space:</h3><h6 id="why-actor-used-to-sample-softmax-epsilon-greedy-fully-explore-contraction-between-them"><a href="#why-actor-used-to-sample-softmax-epsilon-greedy-fully-explore-contraction-between-them" class="headerlink" title="why actor used to sample(softmax, epsilon greedy, fully explore), contraction between them"></a>why actor used to sample(softmax, epsilon greedy, fully explore), contraction between them</h6><h3 id="Q3-the-explore-of-state-action-space"><a href="#Q3-the-explore-of-state-action-space" class="headerlink" title="Q3: the explore of  state-action space:"></a>Q3: the explore of  state-action space:</h3><h6 id="Can-search-methods-be-applied-like-A-RRT"><a href="#Can-search-methods-be-applied-like-A-RRT" class="headerlink" title="Can search methods be applied? like A*, RRT"></a>Can search methods be applied? like A*, RRT</h6><h4 id="Q4-the-explore-of-state-action-space"><a href="#Q4-the-explore-of-state-action-space" class="headerlink" title="Q4: the explore of state-action space:"></a>Q4: the explore of state-action space:</h4><h6 id="how-to-scan-the-q-table"><a href="#how-to-scan-the-q-table" class="headerlink" title="how to scan the q table?"></a>how to scan the q table?</h6><h3 id="Questions"><a href="#Questions" class="headerlink" title="Questions:"></a>Questions:</h3><ol>
<li><p>why q table replaced by NN, what is the discipline to represent? （q表换成q神经网络的意义是什么）</p>
</li>
<li><p>difference b<br>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>
</li>
<li><p>fully explore 和 epsilon greedy</p>
</li>
<li><h1 id="why-actor-better-than-argmax"><a href="#why-actor-better-than-argmax" class="headerlink" title="why actor better than argmax"></a>why actor better than argmax</h1></li>
<li><p>fully explore 和 epsilon greedy（一开始完全探索）</p>
</li>
<li><p>why actor better than argmax（actor网络和argmax1的区别是什么）</p>
</li>
<li><p>神经网络本身的特性会带来哪些东西，神经网络的优点是什么</p>
</li>
<li><p>q learning能否动作是概率分布，为什么不行?</p>
<ol>
<li>可以，使用softmax作为探索的策略，<strong>和epsilon greedy的区别</strong>：会按照q值选取动作，不能保证对动作空间的充分探索</li>
<li>在探索时，选取最优动作，和选取没选取过动作的区别：</li>
<li><strong>探索的衡量指标</strong>：做一个图对比各种采样的策略</li>
</ol>
</li>
</ol>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>9c004c5 (modify week 8)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h3 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h3><ul>
<li>fully explore（完全探索）</li>
</ul>
<p>This approach is called pure exploration and exploitation (PEE) and can be used in some cases where exploration is very costly or where the environment is very simple. However, in most real-world scenarios, PEE is not an optimal approach.</p>
<p>The problem with PEE is that the agent spends a lot of time exploring and collecting data, but not enough time exploiting that data to improve its policy. This can lead to slow learning and poor performance, especially in complex environments where there are many possible actions and states.</p>
<p>In contrast, most RL algorithms use a balance between exploration and exploitation, where the agent takes actions that are likely to yield high rewards based on its current policy while also occasionally exploring new actions or states. This allows the agent to learn quickly while still exploring new possibilities, leading to faster learning and better performance.</p>
<p>Furthermore, in many RL problems, the environment is dynamic and can change over time. In such cases, it is important for the agent to continuously explore and adapt to changes in the environment to maintain optimal performance. This requires a balance between exploration and exploitation, as well as the ability to update the policy based on new data and experiences.</p>
<p>In summary, while PEE can be a useful approach in some cases, a balanced approach between exploration and exploitation is generally more effective for most RL problems.</p>
<p>我用fully explore探索，然后得到的数据存在buffer里，然后用我的策略进行学习，差策略探测到的数据和提升后的策略探索到的数据的区别是什么，数据是一个transition（s a r s ）好的策略往往会偏向于采取好的动作，转移到好的动作</p>
<ul>
<li>好的策略会选好的动作，到好的效率上，efficiency会很高，比如我一直用随机策略采样，然后用</li>
<li>坏的策略会选坏的动作，到不好的轨迹上，</li>
</ul>
<p>所以现在两个最核心的问题：</p>
<ol>
<li>Fully 随机策略就只是影响轨迹，进而影响采样效率吗</li>
<li>看看rlbook里资格迹的概念</li>
<li>看看里面q表怎么扫的</li>
<li>基于q的方法和基于policy的方法本质是一样的，都是value iteration，通过r来提升value，最后通过value去驱动actor，其实本质仍然是用r去驱动一切，核心就在于r怎么去提升策略，即每个状态做什么动作，如何用r去帮助这个状态做什么动作，本质就是看看哪个动作得到的r多，一种是累积r，一种是最终r，现实世界可能都是最终r，但是这样很难学习，所以我们一般先给累积r，然后最后通过最终r去实现，，，</li>
</ol>
<p>所以在这个状态选哪个好呢，一种是sample一下，一种是sample到尽头，sample到一下，sample到一步的r，那我必须bootstrap，来得到当前状态选取某个动作的value，也可以sample轨迹，几个轨迹平均一下，看看动作的value，都是计算动作的价值，如果是连续动作空间怎么办呢，，，，那其实两个在参数上相近的（s，a）对应的价值可能差的很大</p>
<h1 id="lt-lt-lt-lt-lt-lt-lt-HEAD-Incoporate-mappo-with-sumo"><a href="#lt-lt-lt-lt-lt-lt-lt-HEAD-Incoporate-mappo-with-sumo" class="headerlink" title="&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD# Incoporate  mappo with sumo"></a>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD<br># Incoporate  mappo with sumo</h1><blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>9c004c5 (modify week 8)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h2 id="incoporate-mappo-with-sumo（代码部分：在sumo中实验信号灯控制）"><a href="#incoporate-mappo-with-sumo（代码部分：在sumo中实验信号灯控制）" class="headerlink" title="incoporate  mappo with sumo（代码部分：在sumo中实验信号灯控制）"></a>incoporate  mappo with sumo（代码部分：在sumo中实验信号灯控制）</h2><h3 id="1-look-for-existing-mappo-sumo"><a href="#1-look-for-existing-mappo-sumo" class="headerlink" title="1. look for existing mappo + sumo"></a>1. look for existing mappo + sumo</h3><ul>
<li>reference</li>
</ul>
<ol>
<li>github</li>
<li>paper</li>
<li>resource of course era, presentation, tutorial (osint)</li>
</ol>
<ul>
<li>resources found</li>
</ul>
<p>github</p>
<ol>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control">https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control</a></p>
</li>
<li><p>light mappo  <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/light_mappo">https://github.com/magiclucky1996/light_mappo</a> 基于这个写一下试试</p>
</li>
<li><p>q&#x2F;ac + sumo <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/deeprl_signal_control">https://github.com/magiclucky1996/deeprl_signal_control</a></p>
</li>
<li><p>mappo + mujoco <a target="_blank" rel="noopener" href="https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation">https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation</a></p>
</li>
<li><p>mappo&#x2F;qmix&#x2F;maddpg + mpe <a target="_blank" rel="noopener" href="https://github.com/Lizhi-sjtu/MARL-code-pytorch">https://github.com/Lizhi-sjtu/MARL-code-pytorch</a></p>
</li>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/YanivHacker/RLTrafficManager">https://github.com/YanivHacker/RLTrafficManager</a></p>
</li>
<li><p>noisy mappo+  <a target="_blank" rel="noopener" href="https://github.com/hijkzzz/noisy-mappo">https://github.com/hijkzzz/noisy-mappo</a></p>
</li>
</ol>
<p>papers</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors">https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors</a> 东北信息学院：提到了rl奖励稀疏的问题，然后他们给rl设计更多的reward 引导它学习。但是你怎么知道什么样的reward能够引导，这不是还是在reward function设计的范围里吗？</p>
<ul>
<li>insights<ol>
<li>sumo设计的问题，如果我们设一个控制周期之后的交通流状态为reward是不是不合理，怎么样去评价智能体schedule的好坏呢，怎么去评价智能体的action改善了交通呢，我是要搞交通呢，还是要搞rl呢，还是要搞啥，</li>
<li>上次会议的要点：1. insight可以给硕士做，但是要具体可行 2. 做一个oncoming 会议的scheduling(rl 多智能体 交通 计算机 人工智能) 3. sumo的模型可以封装好给本科生用 4.</li>
</ol>
</li>
<li>今天的计划（4.24）</li>
</ul>
<p>work 到12 点半，吃中饭，吃完中饭一点消化一会儿，回实验室一点半，回来继续work,work到两点多的时候睡午觉，五点跑路，去看看有没有吃的，不行就回家supervalu,晚上回家继续work一会儿，今天的弄完走之前deploy 和 push上去</p>
<ul>
<li>how other people implement marl training</li>
</ul>
<p>make contraction between these projects</p>
<p>分解成detailed steps</p>
<p>1.首先看下mappo的代码和deeprl sumo的代码以及ppo sumo代码（只做有必要做的事情）</p>
<ol start="2">
<li><p>做完1大概知道要干嘛，可以看看ppo trpo mappo</p>
</li>
<li><p>把每周4上午留作整理时间，所以我必须两天内搞定这个代码整定的事情，然后再用剩下的时间学习ppo trpo mappo，还要学sumo部分的东西，但是得非常快</p>
</li>
<li></li>
<li></li>
</ol>
<p><strong>reading of the rlbook</strong></p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>
<p>how does dopamine give people motivation?</p>
<p>if prediction is different from truth, we will have positive feedback or negative feedback?</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h3 id="2-make-it-with-materials-found"><a href="#2-make-it-with-materials-found" class="headerlink" title="2. make it with materials found"></a>2. make it with materials found</h3><blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>9c004c5 (modify week 8)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<p><strong>生活的star：</strong></p>
<p>piano</p>
<p>上海美术厂</p>
<p>breaking</p>
<p>penetration testing</p>
<p>avoid social media</p>
<h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><h3 id="抱脸虫教程"><a href="#抱脸虫教程" class="headerlink" title="抱脸虫教程"></a>抱脸虫教程</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff?fw=pt">https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff?fw=pt</a></p>
<ul>
<li>explore&#x2F;exploit权衡：本质是什么：如果exploit，就会一直选择目前的最优解，但是目前的最优解怎么得来，初始化的时候怎么选择，，所以用epsilong decay的epsilon greedy</li>
<li>如果我一开始完全探索，像value表格一样，如何扫过去（也就是如何更新q值），看看rl book，</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/week%208/" data-id="clham94tm0004h1ot2tk2g0wv" data-title="week8&amp;9" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/05/05/week%206%20share/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          week6
        
      </div>
    </a>
  
  
    <a href="/2023/05/05/week4&5/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">week4&amp;5</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/work/" style="font-size: 20px;">work</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/05/22/%E9%98%85%E8%AF%BB%E8%A7%82%E5%BD%B1%E8%AE%B0%E5%BD%95/">阅读观影记录</a>
          </li>
        
          <li>
            <a href="/2023/05/15/try%20hack%20me-%20penetration%20tester/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/15/workshop%20pre1:%20survey%20on%20traffic/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/15/how%20to%20make%20face%20fat/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/10/%E9%81%93/">道</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 tianxiang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>