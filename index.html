<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>public blog of tianxiang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="public blog of tianxiang">
<meta property="og:url" content="http://magiclucky1996.github.io/index.html">
<meta property="og:site_name" content="public blog of tianxiang">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="tianxiang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="public blog of tianxiang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">public blog of tianxiang</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://magiclucky1996.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-dublin_remote_work" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/dublin_remote_work/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T13:51:33.114Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="dublin适合远程办公的地点合集"><a href="#dublin适合远程办公的地点合集" class="headerlink" title="dublin适合远程办公的地点合集"></a>dublin适合远程办公的地点合集</h1><p>咖啡店晒晒太阳什么的，不知道，没有那么想在家待着，也不一定要一直去实验室，找个地方待着</p>
<ol>
<li>google search： places good for study outside with sunshine in dublin<ol>
<li><a target="_blank" rel="noopener" href="https://www.fordublinlovers.com/en/best/places-to-study-outdoors-in-dublin">https://www.fordublinlovers.com/en/best/places-to-study-outdoors-in-dublin</a></li>
<li><a target="_blank" rel="noopener" href="https://livstudent.com/7-study-spots-in-dublin-to-check-out/">https://livstudent.com/7-study-spots-in-dublin-to-check-out/</a></li>
<li><a target="_blank" rel="noopener" href="https://lovindublin.com/best-of/10-best-places-to-work-or-study-in-when-you-find-yourself-in-dublin-city-centre">https://lovindublin.com/best-of/10-best-places-to-work-or-study-in-when-you-find-yourself-in-dublin-city-centre</a></li>
<li><a target="_blank" rel="noopener" href="https://universitytimes.ie/2018/04/five-of-the-best-alternative-study-spaces/">https://universitytimes.ie/2018/04/five-of-the-best-alternative-study-spaces/</a></li>
</ol>
</li>
<li>Chatgpt,</li>
</ol>
<p>need wifi:</p>
<ol>
<li>Dublin City Libraries: Many libraries in Dublin have free Wi-Fi and offer a quiet environment to work in. Check with your local library for opening hours and availability.</li>
<li>The Bernard Shaw: This pub in Dublin has free Wi-Fi and plenty of tables to work at. It also has a garden area where you can enjoy the sunshine while you work.</li>
<li>The Fumbally: This cafe has free Wi-Fi and plenty of tables to work at. It’s a popular spot for freelancers and students in Dublin.</li>
<li>The Digital Hub: This is a co-working space in Dublin that offers Wi-Fi and desk space for a fee. It’s a great option if you need a more professional environment to work in.</li>
<li>Starbucks: Many Starbucks locations in Dublin offer free Wi-Fi, and they are a popular spot for studying or working remotely.</li>
<li>Costa Coffee: Similar to Starbucks, many Costa Coffee locations in Dublin also offer free Wi-Fi.</li>
</ol>
<p>no need wifi:</p>
<ol>
<li><p>Bewley’s Cafe: This cafe has several locations in Dublin and is known for its cozy atmosphere. It’s a great spot to grab a coffee or tea and enjoy a warm drink while people-watching.</p>
</li>
<li><p>The Pepper Pot Cafe: This is a quaint cafe located inside the Powerscourt Centre in Dublin. It’s a great spot to sit and relax with a book or chat with friends while enjoying a warm drink and some delicious food.</p>
</li>
<li><p>The Bald Barista: This cafe is a popular spot in Dublin for its cozy atmosphere and great coffee. It’s a great place to relax and enjoy some good company.</p>
</li>
<li><p>Brother Hubbard: This is another great cafe in Dublin that offers a cozy atmosphere and delicious food. It’s a popular spot for brunch, but you can also visit during the day to relax and unwind.</p>
</li>
<li><p>The Cake Cafe: This is a charming cafe located in a converted redbrick building in Dublin. It’s known for its homemade cakes and friendly atmosphere, making it a great spot to sit and chat with friends.</p>
</li>
<li><p>小红书</p>
</li>
</ol>
<p>2区星巴克</p>
<p>central library</p>
<p>han sung旁边的mind the step</p>
<p>dun laoghaire图书馆</p>
<p>blackrock library</p>
<p>google search library cafe等等</p>
<p>tcd exam hall</p>
<ol start="4">
<li><p>自己想到的</p>
<p>市中心教堂旁边的library</p>
<p>tcd学校里面</p>
<p>家里</p>
</li>
<li><p>补充</p>
<p>怀旧茶馆（一排小桌子对着窗外</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/dublin_remote_work/" data-id="clhamvswr0000p1ot4zj91qgb" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-dublin可做的事情" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T12:06:22.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/">dublin可做的事情+ tcd设施调研</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="dublin-可做的事情"><a href="#dublin-可做的事情" class="headerlink" title="dublin 可做的事情"></a>dublin 可做的事情</h1><h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li>love dublin：<a target="_blank" rel="noopener" href="https://lovindublin.com/">https://lovindublin.com/</a></li>
<li>totally dublin <a target="_blank" rel="noopener" href="https://www.totallydublin.ie/">https://www.totallydublin.ie/</a></li>
<li>visitdublin  <a target="_blank" rel="noopener" href="https://www.visitdublin.com/">https://www.visitdublin.com/</a></li>
<li>meetup <a target="_blank" rel="noopener" href="https://www.meetup.com/home/?suggested=true&source=EVENTS">https://www.meetup.com/home/?suggested=true&amp;source=EVENTS</a></li>
</ul>
<p>	</p>
<h3 id="活动"><a href="#活动" class="headerlink" title="活动"></a>活动</h3><ul>
<li><p>游客线：viking + 啤酒厂 + arts 博物馆</p>
</li>
<li><p>喝啤酒： 老头bar </p>
</li>
<li><p>tcd: 攀岩( the wall, gravity , ) + 图书馆， 游泳馆，桑拿</p>
</li>
<li><p>dun laoghaire + howth看海</p>
</li>
<li><p>meetup+couchsurfing 找活动</p>
</li>
</ul>
<h3 id="地点"><a href="#地点" class="headerlink" title="地点"></a>地点</h3><ul>
<li><p>公园： 凤凰公园 + herbert 公园</p>
</li>
<li><p>山： dublin山 + 威克洛山</p>
</li>
<li><p>海滩：bray + howth</p>
</li>
<li></li>
</ul>
<h3 id="组织"><a href="#组织" class="headerlink" title="组织"></a>组织</h3><ul>
<li>教堂：church city center</li>
<li>飞盘社团+棒球社团+meetup社团</li>
</ul>
<h1 id="tcd-facility-research"><a href="#tcd-facility-research" class="headerlink" title="tcd facility research"></a>tcd facility research</h1><h3 id="reference-1"><a href="#reference-1" class="headerlink" title="reference"></a>reference</h3><ul>
<li>tcd  <a target="_blank" rel="noopener" href="https://www.tcd.ie/">https://www.tcd.ie/</a></li>
<li>library <a target="_blank" rel="noopener" href="https://www.tcd.ie/library/">https://www.tcd.ie/library/</a></li>
<li>clubs and Societies <a target="_blank" rel="noopener" href="https://www.tcd.ie/students/clubs-societies/index.php">https://www.tcd.ie/students/clubs-societies/index.php</a></li>
<li>living in dublin <a target="_blank" rel="noopener" href="https://www.tcd.ie/students/living-dublin/">https://www.tcd.ie/students/living-dublin/</a></li>
</ul>
<h3 id="sports-club"><a href="#sports-club" class="headerlink" title="sports club"></a>sports club</h3><p>飞盘 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/ultimate-frisbee.php">https://www.tcd.ie/Sport/student-sport/clubs/ultimate-frisbee.php</a></p>
<p>攀岩 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/climbing.php">https://www.tcd.ie/Sport/student-sport/clubs/climbing.php</a></p>
<p>帆船 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/wind-wake.php">https://www.tcd.ie/Sport/student-sport/clubs/wind-wake.php</a></p>
<p>板球 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/mens-cricket.php">https://www.tcd.ie/Sport/student-sport/clubs/mens-cricket.php</a></p>
<p>篮球 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/basketball.php">https://www.tcd.ie/Sport/student-sport/clubs/basketball.php</a></p>
<h3 id="Societies"><a href="#Societies" class="headerlink" title="Societies"></a>Societies</h3><ul>
<li>教堂唱诗班 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=18">https://trinitysocieties.ie/society/?socid=18</a></li>
<li>合唱团 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=137">https://trinitysocieties.ie/society/?socid=137</a></li>
<li>计算机协会 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=151">https://trinitysocieties.ie/society/?socid=151</a></li>
<li>跳舞 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=29">https://trinitysocieties.ie/society/?socid=29</a></li>
<li>数字艺术 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=30">https://trinitysocieties.ie/society/?socid=30</a></li>
<li>ted <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=135">https://trinitysocieties.ie/society/?socid=135</a></li>
<li>心理学会 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=90">https://trinitysocieties.ie/society/?socid=90</a></li>
</ul>
<h3 id="校园设施"><a href="#校园设施" class="headerlink" title="校园设施"></a>校园设施</h3><ul>
<li>高性能计算： TCHPC <a target="_blank" rel="noopener" href="https://www.tchpc.tcd.ie/gettingstarted">https://www.tchpc.tcd.ie/gettingstarted</a></li>
<li>文具： stationery</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/" data-id="clhaorwbj0000dvotbmgu1cuz" data-title="dublin可做的事情+ tcd设施调研" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/" rel="tag">life</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-都柏林租房攻略-狠人版" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/">都柏林租房攻略</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="都柏林租房攻略-狠人版"><a href="#都柏林租房攻略-狠人版" class="headerlink" title="都柏林租房攻略-狠人版"></a>都柏林租房攻略-狠人版</h1><ol>
<li><p>首先选区</p>
<p>参考这篇文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/112493998">https://zhuanlan.zhihu.com/p/112493998</a></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/mapa-dublin-660x479.png" alt="img"></p>
</li>
</ol>
<p>理想居住区：2 4 6 6w 13 14 16 18</p>
<p>还行的居住区：1 3 5 7 12 15 17 20</p>
<p>从2 4 6 6w 13 14 16 18 中排除掉交通时间过远的</p>
<ul>
<li>打开hosting power <a target="_blank" rel="noopener" href="https://hostingpower.ie/">https://hostingpower.ie/</a></li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230421113427078.png" alt="image-20230421113427078"></p>
<ul>
<li>选择area, 输入刚刚筛选出的区域结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230421113539429.png" alt="image-20230421113539429"></p>
<ul>
<li><p>选择max rent 点击apply</p>
</li>
<li><p>得到一系列搜索结果后，选择自己喜欢的房子</p>
</li>
<li><p>点进去根据界面提供的关键信息google search 房东的联系方式</p>
<ul>
<li>name</li>
<li>location</li>
<li>and so on</li>
<li>检索示范：”name” + ”location“ + site: airbnb.com</li>
</ul>
</li>
<li><p>自行联系房东签订合同等，可以省去hostingpower中介费890欧</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/" data-id="clgw24ioo0003yc9f3oa462wn" data-title="都柏林租房攻略" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/" rel="tag">life</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-rl_envs_and_modeling" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/rl_envs_and_modeling/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/rl_envs_and_modeling/">rl_play</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="play-with-gym-env"><a href="#play-with-gym-env" class="headerlink" title="play with gym env"></a>play with gym env</h1><h2 id="cart-pole"><a href="#cart-pole" class="headerlink" title="cart pole"></a>cart pole</h2><p>control + pid: <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44562141/article/details/119700574">https://blog.csdn.net/weixin_44562141/article/details/119700574</a></p>
<p>document of gym <a target="_blank" rel="noopener" href="https://www.gymlibrary.dev/environments/classic_control/cart_pole/">https://www.gymlibrary.dev/environments/classic_control/cart_pole/</a></p>
<p>driving test <a target="_blank" rel="noopener" href="http://www.theory-tester.com/questions/358">http://www.theory-tester.com/questions/358</a></p>
<ul>
<li>state space:</li>
</ul>
<p>position</p>
<p>velocity</p>
<p>angle</p>
<p>angular velocity</p>
<ul>
<li><p>first understand problem, then understand reinforcement learning, u must understand the env, then you know why their study is like that, try to be a good teacher</p>
</li>
<li><p>for spare time, can play , for working time, only do things creating value to this project.</p>
<ul>
<li>first look for mappo implementation, then try doing it by myself</li>
</ul>
</li>
<li><p>if we want to control it with pid,</p>
</li>
<li><p>in this env, u are just study “action 要和夹角反着”+ 夹角和几个输入数据的关系，一部分是先验只是可以给的，所以我先原始地学习一下，再把state加工一下加进去，再试试一下把控制的东西加进去，对，我得先有想法，再实验，再读文献，再自己思考，再实验。我希望按照自己的想法来，这样我会沉迷于探索。我希望一直自己保有一些探索的时间，最后发现科研的乐趣。</p>
</li>
</ul>
<h2 id="frozen-lake"><a href="#frozen-lake" class="headerlink" title="frozen lake"></a>frozen lake</h2><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb#scrollTo=Y1tWn0tycWZ1">https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb#scrollTo=Y1tWn0tycWZ1</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qtable = np.zeros(state_space, action_space)</span><br><span class="line">action = argmax(qtable[state][:])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training parameters</span></span><br><span class="line">n_training_episodes = <span class="number">10000</span>  <span class="comment"># Total training episodes</span></span><br><span class="line">learning_rate = <span class="number">0.7</span>          <span class="comment"># Learning rate(这个不是梯度下降的learning rate,是td error的learning rate </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation parameters</span></span><br><span class="line">n_eval_episodes = <span class="number">100</span>        <span class="comment"># Total number of test episodes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">env_id = <span class="string">&quot;FrozenLake-v1&quot;</span>     <span class="comment"># Name of the environment</span></span><br><span class="line">max_steps = <span class="number">99</span>               <span class="comment"># Max steps per episode（防止回合死循环）</span></span><br><span class="line">gamma = <span class="number">0.95</span>                 <span class="comment"># Discounting rate （value的discounting）</span></span><br><span class="line">eval_seed = []               <span class="comment"># The evaluation seed of the environment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exploration parameters （刚开始探索大，后来探索小）</span></span><br><span class="line">max_epsilon = <span class="number">1.0</span>             <span class="comment"># Exploration probability at start</span></span><br><span class="line">min_epsilon = <span class="number">0.05</span>            <span class="comment"># Minimum exploration probability </span></span><br><span class="line">decay_rate = <span class="number">0.0005</span>            <span class="comment"># Exponential decay rate for exploration prob</span></span><br></pre></td></tr></table></figure>





<h2 id="lunarLand"><a href="#lunarLand" class="headerlink" title="lunarLand"></a>lunarLand</h2><p>安装</p>
<p>pip install box2d-py</p>
<h2 id="taxi"><a href="#taxi" class="headerlink" title="taxi"></a>taxi</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit3/introduction?fw=pt">https://huggingface.co/learn/deep-rl-course/unit3/introduction?fw=pt</a></p>
<h2 id="github-ball-game"><a href="#github-ball-game" class="headerlink" title="github ball game"></a>github ball game</h2><h2 id="hugging-face-tutorial"><a href="#hugging-face-tutorial" class="headerlink" title="hugging face tutorial"></a>hugging face tutorial</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/rl_envs_and_modeling/" data-id="clham94tj0000h1ot9idp3nf2" data-title="rl_play" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-week 6 share" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/week%206%20share/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/week%206%20share/">week6</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="week-6-amp-7-share"><a href="#week-6-amp-7-share" class="headerlink" title="week 6&amp;7 share"></a>week 6&amp;7 share</h1><h3 id="Algorithms-single-agent-单智能体算法总结"><a href="#Algorithms-single-agent-单智能体算法总结" class="headerlink" title="Algorithms-single agent(单智能体算法总结)"></a>Algorithms-single agent(单智能体算法总结)</h3><ul>
<li><strong>算法1： DQN</strong><ul>
<li>valued based (unstable because the update of strategy is not smooth)</li>
<li>poorly understood</li>
<li>rainbow is the best implementation version of DQN (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a>)</li>
</ul>
</li>
</ul>
<p><img src="https://www.researchgate.net/publication/333197086/figure/fig11/AS:941946201727001@1601588883526/Pseudo-code-of-DQN-with-experience-replay-method-12.png" alt="Pseudo-code of DQN with experience-replay method [12]"></p>
<ul>
<li><strong>算法2： REINFORCE</strong></li>
</ul>
<p><img src="https://i.stack.imgur.com/8Jn8l.png" alt="reinforcement learning - Why the $\gamma^t$ is needed here in REINFORCE:  Monte-Carlo Policy-Gradient Control (episodic) for $\pi_{*}$? - Cross  Validated"></p>
<ul>
<li><strong>算法3： VPG</strong><ul>
<li>poor data efficiency</li>
<li>poor robustness</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/262538f3077a7be8ce89066abbab523575132996.svg" alt="Vanilla Policy Gradient — Spinning Up documentation"></p>
<ul>
<li><strong>算法4： DDPG</strong><ul>
<li>sampled from replay buffer</li>
<li>good data sampling efficiency (why , just because of reuse of data in buffer)</li>
<li>what does the deterministic refer to?</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg" alt="Deep Deterministic Policy Gradient — Spinning Up documentation"></p>
<ul>
<li><strong>算法5： SAC</strong><ul>
<li>max entropy version of ddpg</li>
<li>entropy help to explore</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/c01f4994ae4aacf299a6b3ceceedfe0a14d4b874.svg" alt="Soft Actor-Critic — Spinning Up documentation"></p>
<ul>
<li><strong>算法6： TRPO</strong><ul>
<li>not compatible with frame including noise or parameter sharing (ppo paper)</li>
<li>a little bit complicated</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/5808864ea60ebc3702704717d9f4c3773c90540d.svg" alt="Trust Region Policy Optimization — Spinning Up documentation"></p>
<ul>
<li><p><strong>算法7 PPO</strong></p>
<ul>
<li><p>data efficiency is not as good as ddpg: why?</p>
</li>
<li><p>collect trajectory set, improve value function to be close to utility with multiple gradient descent</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg" alt="Proximal Policy Optimization — Spinning Up documentation"></p>
<h4 id="chart-for-comparison对比表格"><a href="#chart-for-comparison对比表格" class="headerlink" title="chart for comparison对比表格"></a>chart for comparison对比表格</h4><table>
<thead>
<tr>
<th></th>
<th>value and policy</th>
<th>improve method</th>
<th>sampling</th>
<th>update of model</th>
<th>facts</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>value: <strong>Q network</strong><br />policy: <strong>argmax Q</strong><br />sample: <strong>epsilon_greedy</strong></td>
<td><strong>Bootstrap</strong>: receive r, improve Q</td>
<td>sample along <strong>trajectory</strong> with <strong>epsilon_greedy</strong></td>
<td>one step interact + experience replay</td>
<td>off-policy(replay buffer)<br />discrete action</td>
</tr>
<tr>
<td>DDPG</td>
<td>value: <strong>Q network</strong><br />policy: **actor ** <br />sample: <strong>actor</strong></td>
<td><strong>Bootstrap</strong>: receive r, update Q network , imporve</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>one step interact + experience replay</td>
<td>off-policy;(replay buffer)<br />continuous action space<br /></td>
</tr>
<tr>
<td>SAC</td>
<td>value: <strong>Q network</strong><br />policy: <strong>actor</strong><br />sample: <strong>actor</strong></td>
<td><strong>Boorstrap</strong>: receive r, update Q network, improve <strong>actor</strong></td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>one step interact+experience replay</td>
<td>off-policy(buffer)<br />continuous or discrete action space</td>
</tr>
<tr>
<td>VPG</td>
<td>value: <strong>Q network</strong><br />policy: <strong>actor</strong><br />sample:<strong>actor</strong></td>
<td><strong>MC</strong>: collect set of traj, improve <strong>Actor</strong> with PG, then improve <strong>value</strong> network</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>interact for trajs+ update <strong>actor</strong> with PG(advantage by <strong>critic</strong>); update <strong>critic</strong> with MC</td>
<td><strong>on-policy</strong>;<br />discrete+continuous action(policy network output could be action)<br /><br />easy to trapped in local optima</td>
</tr>
<tr>
<td>TRPO</td>
<td>value: <strong>MC</strong><br />policy: <strong>actor</strong><br />sample: <strong>actor</strong></td>
<td><strong>MC</strong>: collect set of traj, improve <strong>Actor</strong> with PG, then improve <strong>value</strong> network</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>interact for trajs+ update <strong>actor</strong> with PG(advantage by <strong>critic</strong>); update <strong>critic</strong> with MC</td>
<td>on policy</td>
</tr>
<tr>
<td>PPO</td>
<td>value: <strong>MC</strong><br />policy: <strong>actor</strong><br />sample: <strong>actor</strong></td>
<td><strong>MC</strong>: collect set of traj, imporve <strong>actor</strong> with PG, then improve <strong>value</strong> network</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>interact for trajs+ update <strong>actor</strong> with PG(advantage by <strong>critic</strong>); update <strong>critic</strong> with MC</td>
<td>on policy</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="analysis-for-rl-frame-rl框架的分析"><a href="#analysis-for-rl-frame-rl框架的分析" class="headerlink" title="analysis for rl frame(rl框架的分析)"></a>analysis for rl frame(rl框架的分析)</h3><p><strong>Bootstrap</strong>: collect transition , store in the replay buffer, update model every state in the env, <strong>(DQN, DDPG, SAC)</strong>, it’s usually <strong>off-policy, good data efficiency</strong>(data in buffer could be reused), the update of model is more frequent( every step in the env)</p>
<p><strong>MC</strong>: Collect trajs with current policy, improve <strong>actor</strong> with utility through PG, then improve value with utility <strong>(VPG\ TRPO\PPO)</strong>, it’s usually <strong>on-policy</strong>, <strong>data efficiency</strong> is worse than <strong>ddpg and sac</strong>, update of model is not that frequent, </p>
<p><strong>solving iid problem</strong>: </p>
<ul>
<li>replay buffer: off-policy method, applied for <strong>bootstrap</strong>.</li>
</ul>
<p><strong>solving explore problem</strong>: </p>
<ul>
<li>epsilon-greedy: applied for value based rl </li>
<li>max entropy: applied for policy based rl</li>
</ul>
<h3 id="Basic-frame（rl的基础框架）"><a href="#Basic-frame（rl的基础框架）" class="headerlink" title="Basic frame（rl的基础框架）"></a>Basic frame（rl的基础框架）</h3><p><strong>Value based</strong> vs <strong>Policy gradient</strong> </p>
<p>the policy of DQN is <strong>argmax Q</strong>, which is <strong>discrete</strong></p>
<p>the policy of PG is <strong>actor network</strong>, which is <strong>continuous</strong></p>
<p><strong>Result</strong>：</p>
<p>Policy gradient <strong>lose</strong> at sample efficiency:  DQN is more sample efficient</p>
<p>policy gradient <strong>lose</strong> at stability: gradient of policy could be noisy and high-variance</p>
<p>Policy gradient <strong>win</strong> at smooth update: model update of PG is more smooth</p>
<p>policy gradient <strong>win</strong> at complex and continuous problem: env with non-differentiable reward functions and continuous action space</p>
<p><strong>Policy gradient</strong> VS <strong>Actor critic</strong>:</p>
<p>the policy update of <strong>PG</strong> is through <strong>bootstrap</strong>: which learn faster </p>
<p>the policy update of <strong>AC</strong> is through <strong>MC</strong>, which  learn slower</p>
<p><strong>Result</strong>：</p>
<p>Actor critic <strong>win</strong> at sample efficiency</p>
<h3 id="Comparasion（rl算法的对比）"><a href="#Comparasion（rl算法的对比）" class="headerlink" title="Comparasion（rl算法的对比）"></a>Comparasion（rl算法的对比）</h3><table>
<thead>
<tr>
<th>Algorithm</th>
<th>Reward</th>
<th>Convergence Speed</th>
<th>Sample Efficiency</th>
<th>Robustness</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>9</td>
<td>8</td>
<td>9</td>
<td>7</td>
</tr>
<tr>
<td>A3C</td>
<td>8</td>
<td>9</td>
<td>6</td>
<td>8</td>
</tr>
<tr>
<td>PPO</td>
<td>7</td>
<td>7</td>
<td>8</td>
<td>6</td>
</tr>
<tr>
<td>TRPO</td>
<td>7</td>
<td>6</td>
<td>7</td>
<td>9</td>
</tr>
<tr>
<td>AC</td>
<td>6</td>
<td>6</td>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>VPG</td>
<td>7</td>
<td>5</td>
<td>7</td>
<td>7</td>
</tr>
<tr>
<td>DDPG</td>
<td>8</td>
<td>7</td>
<td>9</td>
<td>8</td>
</tr>
<tr>
<td>SAC</td>
<td>9</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
</tbody></table>
<ul>
<li>comparison between ppo and sac</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504102441608.png" alt="image-20230504102441608"></p>
<ul>
<li><p>comparison between ppo and sac</p>
</li>
<li><p>DQN</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094736482.png" alt="image-20230504094736482"></p>
<ul>
<li>A3C</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094802502.png" alt="image-20230504094802502"></p>
<ul>
<li>ppo</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094818441.png" alt="image-20230504094818441"></p>
<ul>
<li>trpo</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094833674.png" alt="image-20230504094833674"></p>
<ul>
<li>AC</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094856363.png" alt="image-20230504094856363"></p>
<ul>
<li>vpg</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504095430731.png" alt="image-20230504095430731"></p>
<ul>
<li>DDPG</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504095442568.png" alt="image-20230504095442568"></p>
<ul>
<li>sac</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504095454177.png" alt="image-20230504095454177"></p>
<h2 id="Questions（问题）"><a href="#Questions（问题）" class="headerlink" title="Questions（问题）"></a>Questions（问题）</h2><p><em><strong>Q1: why cannot replay buffer be applied to ppo? why max entropy cannot be applied to ppo?</strong></em></p>
<ol>
<li>the policy optimization of ppo rely on current policy, ppo use  a rolling buffer to store the most recent trajectories and samples them</li>
<li>policy update objective in ppo already includes an entropy term</li>
</ol>
<p><em><strong>Q2: what is the influence that whether the reward function is differentiable or not in reinforcement learning?</strong></em></p>
<p>gradient-based optimization techniques are needed to update the policy and value function based on the observed rewards and states.</p>
<p><em><strong>Q3:in neural network ,how much does the update of model influence the prediction of next state, if it matters , maybe we just prefer the most important state?</strong></em></p>
<p>*<strong>Q4. when update Q,  will it be more stable that the action of next Q is chosen based on the probability distribution of action?*</strong></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504112741947.png" alt="image-20230504112741947"></p>
<p><em><strong>Q5 we use epsilon decay or entropy to explore the env, so what if we  mark the action we choose in a table, and next time choose the action we haven’t choose?</strong></em></p>
<p><em><strong>Q6. professor Vinny said MC is not same as sample a  set of trajectories</strong></em></p>
<h2 id="Insights（想法）"><a href="#Insights（想法）" class="headerlink" title="Insights（想法）"></a>Insights（想法）</h2><ul>
<li>maybe can view sample efficiency, robustness  and convergence from the perspective of machine learning: we want to learn faster and learn from useful information), then maybe absorb some experience from existing papers on gradient descent and machine learning.</li>
</ul>
<h1 id="Algorithms-multi-agent（多智能体算法）"><a href="#Algorithms-multi-agent（多智能体算法）" class="headerlink" title="Algorithms-multi agent（多智能体算法）"></a>Algorithms-multi agent（多智能体算法）</h1><p><a target="_blank" rel="noopener" href="https://jianzhnie.github.io/machine-learning-wiki/#/deep-rl/papers/Overview">https://jianzhnie.github.io/machine-learning-wiki/#/deep-rl/papers/Overview</a></p>
<p>centralized critic network</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/MARL_cooperation_algo.png" alt="../_images/MARL_cooperation_algo.png"></p>
<p>Valued-based MARL</p>
<p>roma</p>
<p>Qmix</p>
<p>Actor-Critic MARL</p>
<p>maac</p>
<p>coma</p>
<p>maddpg</p>
<p>mappo</p>
<h1 id="Upcoming-conferences（会议总结）"><a href="#Upcoming-conferences（会议总结）" class="headerlink" title="Upcoming conferences（会议总结）"></a>Upcoming conferences（会议总结）</h1><p><em><strong>later do a complete notion chart: 之后做一个完整的notion表格</strong></em></p>
<ul>
<li>machine learning</li>
</ul>
<ol>
<li>Conference on Neural Information Processing Systems (NeurIPS) (A category) <a target="_blank" rel="noopener" href="https://nips.cc/Conferences/2023/CallForPapers">https://nips.cc/Conferences/2023/CallForPapers</a></li>
<li>International Conference on Machine Learning (ICML) (A category)  <a target="_blank" rel="noopener" href="https://icml.cc/">https://icml.cc/</a></li>
<li>Conference on Robot Learning (CoRL)  <a target="_blank" rel="noopener" href="https://www.corl2023.org/">https://www.corl2023.org/</a></li>
<li>IEEE Intelligent Transportation Systems Conference (ITSC) <a target="_blank" rel="noopener" href="https://ieee-itss.org/event/itsc2023/">https://ieee-itss.org/event/itsc2023/</a></li>
<li>Transportation Research Board Annual Meeting (TRB)  <a target="_blank" rel="noopener" href="https://www.trb.org/AnnualMeeting/AnnualMeeting.aspx">https://www.trb.org/AnnualMeeting/AnnualMeeting.aspx</a></li>
<li>International Joint Conference on Artificial Intelligence (IJCAI) (A category)  <a target="_blank" rel="noopener" href="https://www.ijcai.org/">https://www.ijcai.org/</a>  IJCAI-PRICAI-24: shanghai: out of date for 2023</li>
<li>European Conference on Artificial Intelligence (ECAI) <a target="_blank" rel="noopener" href="https://ecai2023.eu/ECAI2023">https://ecai2023.eu/ECAI2023</a></li>
<li>International Conference on Automated Planning and Scheduling (ICAPS) <a target="_blank" rel="noopener" href="https://icaps23.icaps-conference.org/">https://icaps23.icaps-conference.org/</a> : out of date for 2023</li>
<li>International Conference on Learning Representations (ICLR)<a target="_blank" rel="noopener" href="https://iclr.cc/">https://iclr.cc/</a> out of date for 2023</li>
<li>International Conference on Robotics and Automation (ICRA) <a target="_blank" rel="noopener" href="https://www.icra2023.org/">https://www.icra2023.org/</a> out of date for 2023</li>
<li>International Symposium on Transportation and Traffic Theory (ISTTT) <a target="_blank" rel="noopener" href="https://limos.engin.umich.edu/isttt25/">https://limos.engin.umich.edu/isttt25/</a></li>
<li>IEEE Conference on Decision and Control (CDC) <a target="_blank" rel="noopener" href="https://cdc2023.ieeecss.org/">https://cdc2023.ieeecss.org/</a> out of date for 2023</li>
<li>IEEE International Conference on Intelligent Transportation Systems (ITSC) <a target="_blank" rel="noopener" href="https://ieee-itss.org/event/itsc2023/">https://ieee-itss.org/event/itsc2023/</a>  <a target="_blank" rel="noopener" href="https://2023.ieee-itsc.org/">https://2023.ieee-itsc.org/</a> out of date for 2023</li>
<li>International Conference on Control, Automation and Information Sciences (ICCAIS) <a target="_blank" rel="noopener" href="http://iccais2023.org/">http://iccais2023.org/</a> </li>
<li>International Conference on Control, Automation, Robotics and Vision (ICARCV) <a target="_blank" rel="noopener" href="https://www.intelligentautomation.network/events-intelligent-automation/agenda-mc?utm_campaign=27031.007_BLUE_GPPC&extTreatId=7576989&gclid=Cj0KCQjw6cKiBhD5ARIsAKXUdyY_SSzgGhuf4T7L6NxsscqfgI6HypsBEBtUoK1KGE28nwelmmOX-oIaAvveEALw_wcB">https://www.intelligentautomation.network/events-intelligent-automation/agenda-mc?utm_campaign=27031.007_BLUE_GPPC&amp;extTreatId=7576989&amp;gclid=Cj0KCQjw6cKiBhD5ARIsAKXUdyY_SSzgGhuf4T7L6NxsscqfgI6HypsBEBtUoK1KGE28nwelmmOX-oIaAvveEALw_wcB</a></li>
</ol>
<ul>
<li>traffic</li>
</ul>
<ol>
<li>World Conference on Transport Research Society (WCTRS) <a target="_blank" rel="noopener" href="http://wctr2023.ca/">http://wctr2023.ca/</a></li>
<li>International Association of Traffic and Safety Sciences (IATSS)</li>
<li>IEEE Intelligent Vehicles Symposium (IV) <a target="_blank" rel="noopener" href="https://2023.ieee-iv.org/">https://2023.ieee-iv.org/</a></li>
<li>Transportation Science and Logistics Society (TSL)</li>
<li>International Conference on Transport and Health (ICTH)</li>
<li>International Symposium on Transportation Network Reliability (INSTR) <a target="_blank" rel="noopener" href="https://easychair.org/cfp/instr2023">https://easychair.org/cfp/instr2023</a></li>
<li>IEEE International Conference on Intelligent Transportation Systems (ITSC)</li>
<li>European Transport Conference (ETC) <a target="_blank" rel="noopener" href="https://aetransport.org/etc">https://aetransport.org/etc</a></li>
<li>International Conference on Traffic and Transport Psychology (ICTTP) ICTTP 8 2024 Tel Aviv, Israel.</li>
<li>ITS World Congress <a target="_blank" rel="noopener" href="https://itsworldcongress.com/">https://itsworldcongress.com/</a> 2024 dubai</li>
</ol>
<ul>
<li>smart city</li>
</ul>
<ol>
<li>IEEE International Smart Cities Conference (ISC2)</li>
<li>ACM International Conference on Ubiquitous Computing and Communications (UbiComp)</li>
<li>International Conference on Smart Cities and Green ICT Systems (SMARTGREENS)</li>
<li>Smart Cities Symposium Prague (SCSP)</li>
<li>IEEE International Conference on Smart City Innovations (SCI)</li>
<li>International Workshop on Smart Cities and Urban Analytics (UrbanGIS)</li>
<li>International Conference on Smart Data and Smart Cities (SDSC)</li>
<li>Smart City Symposium (SCS)</li>
<li>International Conference on Sustainable Smart Cities and Territories (SSCt)</li>
<li>European Conference on Smart Objects, Systems and Technologies (Smart SysTech)</li>
</ol>
<ul>
<li>intelligent connected vehicle</li>
</ul>
<ol>
<li>International Conference on Connected Vehicles and Expo (ICCVE)</li>
<li>IEEE Vehicular Technology Conference (VTC)</li>
<li>International Conference on Vehicle Technology and Intelligent Transport Systems (VEHITS)</li>
<li>IEEE Conference on Control Technology and Applications (CCTA)</li>
<li>International Conference on Vehicle Engineering and Intelligent Transportation Systems (VEITS)</li>
<li>IEEE International Conference on Connected and Autonomous Vehicles (ICCAV)</li>
</ol>
<h4 id="May"><a href="#May" class="headerlink" title="May"></a>May</h4><p><strong>May 8</strong></p>
<p>ECAI 2023 <a target="_blank" rel="noopener" href="https://ecai2023.eu/ECAI2023">https://ecai2023.eu/ECAI2023</a></p>
<p><strong>May 11</strong></p>
<p>NIPS 2023  <a target="_blank" rel="noopener" href="https://nips.cc/Conferences/2023/CallForPapers">https://nips.cc/Conferences/2023/CallForPapers</a></p>
<p><strong>May 15</strong></p>
<p>ITSC 2023 <a target="_blank" rel="noopener" href="https://2023.ieee-itsc.org/">https://2023.ieee-itsc.org/</a></p>
<h4 id="June"><a href="#June" class="headerlink" title="June"></a>June</h4><p><strong>June  8</strong> </p>
<p>CoRL 2023 <a target="_blank" rel="noopener" href="https://www.corl2023.org/">https://www.corl2023.org/</a></p>
<h4 id="July"><a href="#July" class="headerlink" title="July"></a>July</h4><p><strong>July 15</strong></p>
<p>ICCAIS 2023 <a target="_blank" rel="noopener" href="http://iccais2023.org/">http://iccais2023.org/</a></p>
<h4 id="August"><a href="#August" class="headerlink" title="August"></a>August</h4><p><strong>August 1</strong></p>
<p>TRB 2024 <a target="_blank" rel="noopener" href="https://trb.secure-platform.com/a/page/TRBPaperReview#Instructions">https://trb.secure-platform.com/a/page/TRBPaperReview#Instructions</a></p>
<h4 id="Sepetmber"><a href="#Sepetmber" class="headerlink" title="Sepetmber"></a>Sepetmber</h4><h4 id="October"><a href="#October" class="headerlink" title="October"></a>October</h4><h4 id="November"><a href="#November" class="headerlink" title="November"></a>November</h4><h4 id="December"><a href="#December" class="headerlink" title="December"></a>December</h4><h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><ol>
<li><h4 id="Trpo-PPO-MAPPO"><a href="#Trpo-PPO-MAPPO" class="headerlink" title="Trpo PPO MAPPO"></a>Trpo PPO MAPPO</h4></li>
</ol>
<p>1.1 Trpo</p>
<p><strong>(1). Approximation</strong></p>
<ul>
<li>the cost function</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426101337008.png" alt="image-20230426101337008"></p>
<ul>
<li>the V</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426101210597.png" alt="image-20230426101210597"></p>
<ul>
<li>so it could be approximated by</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426105145352.png" alt="image-20230426105145352"></p>
<ul>
<li><p>therefore J could be transferred to </p>
<p><img src="/home/sky/.config/Typora/typora-user-images/image-20230426101435439.png" alt="image-20230426101435439"></p>
</li>
<li><p>try to find the <strong>theta</strong> which could maximize the <strong>J</strong></p>
</li>
<li><p><strong>S</strong> follows the trajectory of steps, but are seen as stochastic sampling from the env</p>
</li>
<li><p>A is also sampled by the strategy Pi</p>
</li>
<li><p>collect this trajectory by interacting with the env</p>
</li>
<li><p>then it would be like this</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426104602682.png" alt="image-20230426104602682"></p>
<h4 id="2-optimization"><a href="#2-optimization" class="headerlink" title="(2). optimization"></a>(2). optimization</h4><ul>
<li>in trust region, update parameter,</li>
</ul>
<p><img src="/home/sky/.config/Typora/typora-user-images/image-20230426105534532.png" alt="image-20230426105534532"></p>
<p>it’s an optimization problem, we construct the optimization problem, then throw to optimization solver to solve it.</p>
<h4 id="3-pseudocode"><a href="#3-pseudocode" class="headerlink" title="(3). pseudocode"></a>(3). pseudocode</h4><p><img src="/home/sky/.config/Typora/typora-user-images/image-20230426120749975.png" alt="image-20230426120749975"></p>
<p>In one cycle, the strategy network is updated each time, and one game is played to obtain a trajectory. However, in maximization, there are multiple inner cycles required by optimization problems , which are usually solved by gradient projection algorithm.</p>
<ul>
<li>2 hyperparameters 4 maximization: <ul>
<li>Step size of gradient descent, </li>
<li>radius of confidence region</li>
</ul>
</li>
</ul>
<h3 id="1-2-PPO"><a href="#1-2-PPO" class="headerlink" title="1.2 PPO"></a>1.2 PPO</h3><ul>
<li>PPO version 1: add constraint into cost function</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426120941839.png" alt="image-20230426120941839"></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426120839455.png" alt="image-20230426120839455"></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426123355644.png" alt="image-20230426123355644"></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
<table>
<thead>
<tr>
<th></th>
<th>cons</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>1. fails on simple problems; <br />2. poorly understood</td>
</tr>
<tr>
<td>VPG</td>
<td>1. poor data efficiency <br />2. poor robustness</td>
</tr>
<tr>
<td>trpo</td>
<td>1. complicated <br />2. not compatible with noise ( like dropout)+ data sharing</td>
</tr>
<tr>
<td>ppo</td>
<td>1. good data efficiency<br />2. reliable profermance<br />3. only first -order optimization</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<ol start="2">
<li><h3 id="pieter-abbeel-rl-course"><a href="#pieter-abbeel-rl-course" class="headerlink" title="pieter abbeel rl course"></a>pieter abbeel rl course</h3></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2GwBez0D20A&t=130s">https://www.youtube.com/watch?v=2GwBez0D20A&amp;t=130s</a></p>
<p>2.1 : MDP</p>
<p><em><strong>insight: group of robot learn faster: data sharing, more efficient sampling of the env</strong></em></p>
<ul>
<li>groups of robots learn faster, they can share date, more efficient sampling of the env, save wall time</li>
<li>gamma (discount factor) is also designed based on what our goal is, if we want the agent of care more about things happen in closer steps, then …</li>
<li>if gamma is introduced, state take less  steps to the reward is with higher value, it’s like the “time” of game world. but it should not be same as the future evaluated in our real world.</li>
<li>update: in grid world,  we swap a time for all the grid , what if we use different way to swap all the states?<ul>
<li>like along trajectory</li>
<li>like importance sampling</li>
</ul>
</li>
<li>Discount factor influence convergence: 0: faster 1: longer</li>
<li>why it converge</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426134554214.png" alt="image-20230426134554214"></p>
<h5 id="effect-of-discount-and-noise"><a href="#effect-of-discount-and-noise" class="headerlink" title="effect of discount and noise"></a>effect of discount and noise</h5><p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426150334001.png" alt="image-20230426150334001"></p>
<p>(a)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426171844052.png" alt="image-20230426171844052"></p>
<p>(b)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426171913044.png" alt="image-20230426171913044"></p>
<p>(c)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426171946200.png" alt="image-20230426171946200"></p>
<p>(d)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426172016159.png" alt="image-20230426172016159"></p>
<ul>
<li>update for Q*  , as default,  the agent thereafter acting optimally</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426172142184.png" alt="image-20230426172142184"></p>
<ul>
<li>policy evaluation</li>
</ul>
<h3 id="Max-entropy"><a href="#Max-entropy" class="headerlink" title="Max entropy"></a>Max entropy</h3><ul>
<li><p>how do we collect data</p>
<ul>
<li>use current policy to collect data	, if policy is deterministic , data collection would not be interesting</li>
<li>with entropy, policy will be with more variation in how the data is collected</li>
</ul>
</li>
<li><p>entropy</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426173232834.png" alt="image-20230426173232834"></p>
<ul>
<li><p>a distribution over near-optimal solution</p>
<ul>
<li><strong>robust policy</strong>:  the env could change, if it’s distribution instead of deterministic it’s more robust</li>
<li><strong>robust learning</strong> : we can keep collecting exploratory data during learning</li>
</ul>
</li>
<li><p>collect along learning, or collect with exploration , then off-policy update</p>
</li>
<li><p>insights: how about when the best action changed in a state , increase the   possibility of explore and update to former states,( like in my last paper, in board are , increase explore, in narrow area, reduce explore)</p>
</li>
<li><p>insights:  after update the value of a state , trace back and update all the former state (read the trace chapter of sutton book)</p>
</li>
</ul>
<h3 id="2-2-Q-learning"><a href="#2-2-Q-learning" class="headerlink" title="2.2 : Q learning"></a>2.2 : Q learning</h3><p><strong>properties</strong></p>
<ul>
<li>converge even if act suboptimal (epsilon greedy)</li>
<li>epsilon decay: if not do decay , latest experience will make you hop around</li>
<li>epsilon: u need to make it small eventually </li>
<li>epsilon: cannot decay too fast: cannot update enough</li>
</ul>
<p><strong>requirement</strong></p>
<ul>
<li>state and actions are visited infinitely often: doesn’t matter how u select actions</li>
<li>learning rate schedule: <ul>
<li>reference: <a target="_blank" rel="noopener" href="https://dspace.mit.edu/bitstream/handle/1721.1/7205/AIM-1441.pdf?sequence=2">On the Convergence of Stochastic Iterative Dynamic …</a></li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230501182944684.png" alt="image-20230501182944684"></p>
<h3 id="play-with-rl"><a href="#play-with-rl" class="headerlink" title="play with rl"></a>play with rl</h3><ul>
<li>rl playground</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://rlplaygrounds.com/">https://rlplaygrounds.com/</a></p>
<ul>
<li><p>openai gym</p>
</li>
<li><p>deep mind control</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/deepmind/dm_control">https://github.com/deepmind/dm_control</a></p>
<ul>
<li>Unity ML-Agents (</li>
</ul>
<p> <a target="_blank" rel="noopener" href="https://github.com/Unity-Technologies/ml-agents">https://github.com/Unity-Technologies/ml-agents</a> )</p>
<ul>
<li>course from neptune</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses">https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses</a></p>
<ul>
<li>easy game to visualize reinforcement learning</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/reinforcement-learning-and-visualisation-with-a-simple-game-a1fe725f0509">https://towardsdatascience.com/reinforcement-learning-and-visualisation-with-a-simple-game-a1fe725f0509</a></p>
<p>when state space is large, the update of qtable will be very slow…</p>
<h3 id="paper-reading-list"><a href="#paper-reading-list" class="headerlink" title="paper reading list:"></a><em>paper reading list:</em></h3><ul>
<li><em>playing atari with drl</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.5602">https://arxiv.org/abs/1312.5602</a></li>
<li><em>rainbow</em>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></li>
<li><em>ppo</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></li>
<li><em>mappo</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01955">https://arxiv.org/abs/2103.01955</a></li>
<li><em>maven</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.07483">https://arxiv.org/abs/1910.07483</a></li>
<li>qmix <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11485">https://arxiv.org/abs/1803.11485</a></li>
<li>maddpg <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02275">https://arxiv.org/abs/1706.02275</a></li>
<li>coma <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.08926">https://arxiv.org/abs/1705.08926</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/week%206%20share/" data-id="clham94tk0001h1ot9v43b4jy" data-title="week6" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-week 8" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/week%208/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/week%208/">week8&amp;9</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="week-8"><a href="#week-8" class="headerlink" title="week 8"></a>week 8</h1><h3 id="goal-of-the-week"><a href="#goal-of-the-week" class="headerlink" title="goal of the week"></a>goal of the week</h3><p>reading rl papers, marl papers, sutton rlbook2020</p>
<h3 id="Questions"><a href="#Questions" class="headerlink" title="Questions:"></a>Questions:</h3><ol>
<li>why q table replaced by NN, what is the discipline to represent?</li>
<li>difference b</li>
<li>fully explore 和 epsilon greedy</li>
<li>why actor better than argmax</li>
<li></li>
</ol>
<h3 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h3><ul>
<li>fully explore</li>
</ul>
<p>This approach is called pure exploration and exploitation (PEE) and can be used in some cases where exploration is very costly or where the environment is very simple. However, in most real-world scenarios, PEE is not an optimal approach.</p>
<p>The problem with PEE is that the agent spends a lot of time exploring and collecting data, but not enough time exploiting that data to improve its policy. This can lead to slow learning and poor performance, especially in complex environments where there are many possible actions and states.</p>
<p>In contrast, most RL algorithms use a balance between exploration and exploitation, where the agent takes actions that are likely to yield high rewards based on its current policy while also occasionally exploring new actions or states. This allows the agent to learn quickly while still exploring new possibilities, leading to faster learning and better performance.</p>
<p>Furthermore, in many RL problems, the environment is dynamic and can change over time. In such cases, it is important for the agent to continuously explore and adapt to changes in the environment to maintain optimal performance. This requires a balance between exploration and exploitation, as well as the ability to update the policy based on new data and experiences.</p>
<p>In summary, while PEE can be a useful approach in some cases, a balanced approach between exploration and exploitation is generally more effective for most RL problems.</p>
<h1 id="incoporate-mappo-with-sumo"><a href="#incoporate-mappo-with-sumo" class="headerlink" title="incoporate  mappo with sumo"></a>incoporate  mappo with sumo</h1><h2 id="1-look-for-existing-mappo-sumo"><a href="#1-look-for-existing-mappo-sumo" class="headerlink" title="1. look for existing mappo + sumo"></a>1. look for existing mappo + sumo</h2><ul>
<li>reference</li>
</ul>
<ol>
<li>github</li>
<li>paper</li>
<li>resource of course era, presentation, tutorial (osint)</li>
</ol>
<ul>
<li>resources found</li>
</ul>
<p>github</p>
<ol>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control">https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control</a></p>
</li>
<li><p>light mappo  <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/light_mappo">https://github.com/magiclucky1996/light_mappo</a> 基于这个写一下试试</p>
</li>
<li><p>q&#x2F;ac + sumo <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/deeprl_signal_control">https://github.com/magiclucky1996/deeprl_signal_control</a></p>
</li>
<li><p>mappo + mujoco <a target="_blank" rel="noopener" href="https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation">https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation</a></p>
</li>
<li><p>mappo&#x2F;qmix&#x2F;maddpg + mpe <a target="_blank" rel="noopener" href="https://github.com/Lizhi-sjtu/MARL-code-pytorch">https://github.com/Lizhi-sjtu/MARL-code-pytorch</a></p>
</li>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/YanivHacker/RLTrafficManager">https://github.com/YanivHacker/RLTrafficManager</a></p>
</li>
<li><p>noisy mappo+  <a target="_blank" rel="noopener" href="https://github.com/hijkzzz/noisy-mappo">https://github.com/hijkzzz/noisy-mappo</a></p>
</li>
</ol>
<p>papers</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors">https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors</a> 东北信息学院：提到了rl奖励稀疏的问题，然后他们给rl设计更多的reward 引导它学习。但是你怎么知道什么样的reward能够引导，这不是还是在reward function设计的范围里吗？</p>
<ul>
<li>insights<ol>
<li>sumo设计的问题，如果我们设一个控制周期之后的交通流状态为reward是不是不合理，怎么样去评价智能体schedule的好坏呢，怎么去评价智能体的action改善了交通呢，我是要搞交通呢，还是要搞rl呢，还是要搞啥，</li>
<li>上次会议的要点：1. insight可以给硕士做，但是要具体可行 2. 做一个oncoming 会议的scheduling(rl 多智能体 交通 计算机 人工智能) 3. sumo的模型可以封装好给本科生用 4.</li>
</ol>
</li>
<li>今天的计划（4.24）</li>
</ul>
<p>work 到12 点半，吃中饭，吃完中饭一点消化一会儿，回实验室一点半，回来继续work,work到两点多的时候睡午觉，五点跑路，去看看有没有吃的，不行就回家supervalu,晚上回家继续work一会儿，今天的弄完走之前deploy 和 push上去</p>
<ul>
<li>how other people implement marl training</li>
</ul>
<p>make contraction between these projects</p>
<p>分解成detailed steps</p>
<p>1.首先看下mappo的代码和deeprl sumo的代码以及ppo sumo代码（只做有必要做的事情）</p>
<ol start="2">
<li><p>做完1大概知道要干嘛，可以看看ppo trpo mappo</p>
</li>
<li><p>把每周4上午留作整理时间，所以我必须两天内搞定这个代码整定的事情，然后再用剩下的时间学习ppo trpo mappo，还要学sumo部分的东西，但是得非常快</p>
</li>
<li></li>
<li></li>
</ol>
<h2 id="2-make-it-with-materials-found"><a href="#2-make-it-with-materials-found" class="headerlink" title="2. make it with materials found"></a>2. make it with materials found</h2><p>生活的star：</p>
<p>piano</p>
<p>上海美术厂</p>
<p>breaking</p>
<p>penetration testing</p>
<p>avoid social media</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/week%208/" data-id="clham94tm0004h1ot2tk2g0wv" data-title="week8&amp;9" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-week4&amp;5" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/week4&5/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/week4&5/">week4&amp;5</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="week-4-amp-5"><a href="#week-4-amp-5" class="headerlink" title="week 4&amp;5"></a>week 4&amp;5</h1><h2 id="1-Ubuntu-setting-安装ubuntu系统"><a href="#1-Ubuntu-setting-安装ubuntu系统" class="headerlink" title="1. Ubuntu setting(安装ubuntu系统)"></a>1. Ubuntu setting(安装ubuntu系统)</h2><h3 id="Install-oh-my-zsh"><a href="#Install-oh-my-zsh" class="headerlink" title="Install oh my zsh"></a>Install oh my zsh</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install zsh</span><br><span class="line">chsh -s $(<span class="built_in">which</span> zsh)</span><br><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="setting-of-input-method-of-ubuntu"><a href="#setting-of-input-method-of-ubuntu" class="headerlink" title="setting of input method of ubuntu"></a>setting of input method of ubuntu</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/418042828">https://www.zhihu.com/question/418042828</a></p>
<h3 id="Install-nvidia-driver（安装英伟达驱动）"><a href="#Install-nvidia-driver（安装英伟达驱动）" class="headerlink" title="Install nvidia driver（安装英伟达驱动）"></a>Install nvidia driver（安装英伟达驱动）</h3><p>driver version : <code>470.63.01</code>.</p>
<p>CUDA version: CUDA Toolkit 11.4. （the highest version that 470 can support, if want to install newest version of CUDA(11.8), need to install higher version of driver( open the software update of ubuntu system)）</p>
<p>If project is written with <strong>TensorFlow 1</strong>, which not support by your CUDA and NVIDIA driver, options:</p>
<ol>
<li><p><strong>Downgrade your NVIDIA driver and CUDA Toolkit</strong> versions to be compatible with TensorFlow </p>
<ol>
<li><ol>
<li>You can follow the installation instructions for TensorFlow 1.15.4 with GPU support, which requires CUDA Toolkit 10.0 or 10.1 and NVIDIA driver version 418.x or higher.</li>
</ol>
</li>
</ol>
</li>
<li><p>Use a <strong>virtual environment or container</strong> with the specific versions of CUDA Toolkit, NVIDIA driver, and TensorFlow 1 that your project requires. You can create a new virtual environment or container with the appropriate dependencies using tools like virtualenv or Docker.</p>
</li>
<li><p><strong>Upgrade your project to use TensorFlow 2.x</strong>. Although there are some differences between TensorFlow 1 and TensorFlow 2, many of the core concepts and functionalities are similar, so the migration process may not be too difficult. You can use TensorFlow’s migration guide to help you with the process.</p>
</li>
</ol>
<p>for me, what is helpful is :</p>
<ol>
<li><strong>update the code</strong> to tf2 version</li>
<li>run code with <strong>old version tf1 and use cpu</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install nvidia-driver-&lt;VERSION&gt;</span><br></pre></td></tr></table></figure>



<h3 id="meet-problem-with-unfigured-nvidia-driver-遇到驱动的问题"><a href="#meet-problem-with-unfigured-nvidia-driver-遇到驱动的问题" class="headerlink" title="meet problem with unfigured nvidia driver(遇到驱动的问题)"></a>meet problem with unfigured nvidia driver(遇到驱动的问题)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sudo apt-get remove --purge nvidia-dkms-470 nvidia-driver-470</span><br></pre></td></tr></table></figure>



<h3 id="still-doesn’t-work（继续遇到驱动问题）"><a href="#still-doesn’t-work（继续遇到驱动问题）" class="headerlink" title="still doesn’t work（继续遇到驱动问题）"></a>still doesn’t work（继续遇到驱动问题）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dpkg -l | grep -i nvidia</span><br><span class="line">sudo apt-get remove --purge <span class="string">&#x27;^nvidia-.*&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="Problem：-gpu-driver-gt-install-nvidia-driver470（驱动安装问题）"><a href="#Problem：-gpu-driver-gt-install-nvidia-driver470（驱动安装问题）" class="headerlink" title="Problem： gpu driver -&gt; install nvidia driver470（驱动安装问题）"></a>Problem： gpu driver -&gt; install nvidia driver470（驱动安装问题）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dpkg: error processing package nvidia-driver-470 (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">No apport report written because the error message indicates its a followup error from a previous failure.</span><br><span class="line">                                                                                            Processing triggers <span class="keyword">for</span> desktop-file-utils (0.26-1ubuntu3) ...</span><br><span class="line">Processing triggers <span class="keyword">for</span> gnome-menus (3.36.0-1ubuntu3) ...</span><br><span class="line">Processing triggers <span class="keyword">for</span> libc-bin (2.35-0ubuntu3) ...</span><br><span class="line">Processing triggers <span class="keyword">for</span> man-db (2.10.2-1) ...</span><br><span class="line">Processing triggers <span class="keyword">for</span> mailcap (3.70+nmu1ubuntu1) ...</span><br><span class="line">Processing triggers <span class="keyword">for</span> initramfs-tools (0.140ubuntu13.1) ...</span><br><span class="line">update-initramfs: Generating /boot/initrd.img-5.19.0-38-generic</span><br><span class="line">Errors were encountered <span class="keyword">while</span> processing:</span><br><span class="line"> nvidia-dkms-470</span><br><span class="line"> nvidia-driver-470</span><br><span class="line">E: Sub-process /usr/bin/dpkg returned an error code (1)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>solution: set in the software and update of ubuntu and reboot</strong></li>
</ul>
<h3 id="Uninstall-old-Cuda（卸载旧的cuda）"><a href="#Uninstall-old-Cuda（卸载旧的cuda）" class="headerlink" title="Uninstall old Cuda（卸载旧的cuda）"></a>Uninstall old Cuda（卸载旧的cuda）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/cuda/bin</span><br><span class="line">./cuda-uninstaller</span><br></pre></td></tr></table></figure>





<h3 id="Install-Cuda11-4（安装cuda11-4）"><a href="#Install-Cuda11-4（安装cuda11-4）" class="headerlink" title="Install Cuda11.4（安装cuda11.4）"></a>Install Cuda11.4（安装cuda11.4）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.4.3/local_installers/cuda_11.4.3_470.82.01_linux.run</span><br><span class="line"><span class="comment">## test the install of cuda</span></span><br><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>



<h3 id="Install-cudnn（安装cudnn）"><a href="#Install-cudnn（安装cudnn）" class="headerlink" title="Install cudnn（安装cudnn）"></a>Install cudnn（安装cudnn）</h3><ul>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></li>
</ul>
<p>test the install of cudnn</p>
<hr>
<h2 id="2-MARL-env-setting（设定MARL环境）"><a href="#2-MARL-env-setting（设定MARL环境）" class="headerlink" title="2. MARL env setting（设定MARL环境）"></a>2. MARL env setting（设定MARL环境）</h2><h3 id="1-install-StarCraft-II-env（安装星级争霸）"><a href="#1-install-StarCraft-II-env（安装星级争霸）" class="headerlink" title="1. install StarCraft II env（安装星级争霸）"></a>1. install StarCraft II env（安装星级争霸）</h3><h4 id="Reference（参考）"><a href="#Reference（参考）" class="headerlink" title="Reference（参考）"></a>Reference（参考）</h4><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/on-policy">https://github.com/magiclucky1996/on-policy</a></p>
<ul>
<li><code>unzip SC2.4.10.zip</code></li>
<li><code>echo &quot;export SC2PATH=~/StarCraftII/&quot; &gt; ~/.bashrc</code></li>
<li>download SMAC Maps, and move it to <code>~/StarCraftII/Maps/</code>.</li>
<li>To use a stableid, copy <code>stableid.json</code> from <a target="_blank" rel="noopener" href="https://github.com/Blizzard/s2client-proto.git">https://github.com/Blizzard/s2client-proto.git</a> to <code>~/StarCraftII/</code>.</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/oxwhirl/smac">https://github.com/oxwhirl/smac</a></p>
<ul>
<li>Please use the Blizzard’s <a target="_blank" rel="noopener" href="https://github.com/Blizzard/s2client-proto#downloads">repository</a> to download the Linux version of StarCraft II. By default, the game is expected to be in <code>~/StarCraftII/</code> directory. This can be changed by setting the environment variable <code>SC2PATH</code>.</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/oxwhirl/pymarl/blob/master/install_sc2.sh">https://github.com/oxwhirl/pymarl/blob/master/install_sc2.sh</a></p>
</li>
</ul>
<h4 id="Detailed-steps（具体步骤）"><a href="#Detailed-steps（具体步骤）" class="headerlink" title="Detailed steps（具体步骤）"></a>Detailed steps（具体步骤）</h4><ul>
<li>follow the instruction of shell script below</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># download and install starcraft ii</span><br><span class="line"></span><br><span class="line">wget http://blzdistsc2-a.akamaihd.net/Linux/SC2.4.10.zip</span><br><span class="line">unzip -P iagreetotheeula SC2.4.10.zip</span><br><span class="line"></span><br><span class="line"># download smac map</span><br><span class="line"></span><br><span class="line">MAP_DIR=&quot;$SC2PATH/Maps/&quot;</span><br><span class="line">wget https://github.com/oxwhirl/smac/releases/download/v0.1-beta1/SMAC_Maps.zip</span><br><span class="line">unzip SMAC_Maps.zip</span><br><span class="line">mv SMAC_Maps $MAP_DIR</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="Problem1-wandb-init（wandb报错）"><a href="#Problem1-wandb-init（wandb报错）" class="headerlink" title="Problem1  wandb.init（wandb报错）"></a>Problem1  wandb.init（wandb报错）</h4><pre><code>run = wandb.init(config=all_args,
                 project=all_args.env_name,
                 entity=all_args.user_name,
                 notes=socket.gethostname(),
                 name=str(all_args.algorithm_name) + &quot;_&quot; +
                      str(all_args.experiment_name) +
                      &quot;_seed&quot; + str(all_args.seed),
                 group=all_args.map_name,
                 dir=str(run_dir),
                 job_type=&quot;training&quot;,
                 reinit=True)
</code></pre>
<ul>
<li><p>these attributes</p>
<ul>
<li><p>project</p>
</li>
<li><p>env_name</p>
</li>
<li><p>entity</p>
</li>
<li><p>user_name</p>
</li>
<li><p>notes</p>
</li>
<li><p>name</p>
</li>
<li><p>gourp</p>
</li>
<li><p>dir</p>
</li>
</ul>
</li>
</ul>
<h5 id="solution（解决方案）"><a href="#solution（解决方案）" class="headerlink" title="solution（解决方案）"></a>solution（解决方案）</h5><ol>
<li><p>wandb permission denied: modify the wandb.init </p>
</li>
<li><p>python 10 is not compatible: switch to python 3.6 (comply with origin env as much as possible)</p>
</li>
<li><p>finsh the implementation of four marl project and get some figures.</p>
<ol>
<li>understand detail of each project</li>
</ol>
</li>
<li><p>finish my explorement of fundamental rl algorithms</p>
</li>
</ol>
<hr>
<h1 id="Reading（资料阅读）"><a href="#Reading（资料阅读）" class="headerlink" title="Reading（资料阅读）"></a>Reading（资料阅读）</h1><h4 id="Reference（参考）-1"><a href="#Reference（参考）-1" class="headerlink" title="Reference（参考）"></a>Reference（参考）</h4><ul>
<li><strong>spinning up（openai spinning up 项目）</strong></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html</a> (there some recommendation of  reading here)</p>
<p><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html?highlight=rainbow">https://spinningup.openai.com/en/latest/spinningup/keypapers.html?highlight=rainbow</a> (key papers of rl)</p>
<p><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html?highlight=rainbow#model-free-rl">https://spinningup.openai.com/en/latest/spinningup/keypapers.html?highlight=rainbow#model-free-rl</a></p>
<ul>
<li><strong>github paper collection（github 多智能体论文合集）</strong></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/LantaoYu/MARL-Papers">https://github.com/LantaoYu/MARL-Papers</a></p>
<h2 id="Papers（论文阅读）"><a href="#Papers（论文阅读）" class="headerlink" title="Papers（论文阅读）"></a>Papers（论文阅读）</h2><h3 id="1-Playing-Atari-with-Deep-Reinforcement-Learning"><a href="#1-Playing-Atari-with-Deep-Reinforcement-Learning" class="headerlink" title="1. Playing Atari with Deep Reinforcement Learning"></a>1. Playing Atari with Deep Reinforcement Learning</h3><p><em>deepmind, 2013</em></p>
<p>why the data is needed to be iid, what does it mean with iid</p>
<p>(为什么需要样本是独立的，独立同分布到底是什么意思)</p>
<h3 id="2-Benchmarking-Multi-Agent-Deep-Reinforcement-Learning-Algorithms-in-Cooperative-Tasks"><a href="#2-Benchmarking-Multi-Agent-Deep-Reinforcement-Learning-Algorithms-in-Cooperative-Tasks" class="headerlink" title="2. Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks"></a>2. Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.07869">https://arxiv.org/abs/2006.07869</a></p>
<p><em>June 2020  ; Georgios Papoudakis ； Phd student, ；School of Informatics ； university of Edinburgh,</em></p>
<ul>
<li>experiment results serving as reference</li>
<li>insights regarding the effectiveness of different learning approaches</li>
<li>open-source algorithm codebase</li>
<li>open-source two rl envs</li>
</ul>
<h5 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h5><ul>
<li><p>algotithms</p>
<ul>
<li>independent<ul>
<li>iql</li>
<li>ia2c</li>
<li>ippo</li>
</ul>
</li>
<li>ctde<ul>
<li>ma ddpg</li>
<li>coma</li>
<li>ma ac</li>
<li>ma ppo</li>
</ul>
</li>
<li>value decompose<ul>
<li>vdn</li>
<li>qmix</li>
</ul>
</li>
</ul>
</li>
<li><p>envs</p>
<ul>
<li>Repeated Matrix Games</li>
<li>Multi-Agent Particle Environment(MPE)</li>
<li>StarCraft Multi-Agent Challenge</li>
<li>Level-Based Foraging(LBF)</li>
<li>Multi-Robot Warehouse</li>
</ul>
</li>
</ul>
<h3 id="insights（思考）"><a href="#insights（思考）" class="headerlink" title="insights（思考）"></a>insights（思考）</h3><ul>
<li><p><em>不同类型的game有不同的特点</em> ：the character of different game is different.</p>
</li>
<li><p><em>不同问题对合作的需求不同，需要对问题机理进行分析</em>:the need from different problem to joint action is different, need the analysis of the principle of problem.</p>
</li>
<li><p><em><strong>算法时间</strong></em>：<em><strong>模型大小，更新频率，步长</strong></em>都会影响，需要弄清楚。</p>
<ul>
<li>如果限制随机性，那么算法效果能否完全复现，</li>
<li><em>环境探索能否不随机，按特定的规律进行探索？</em></li>
</ul>
</li>
<li><p><em><strong>google</strong></em><em><strong>chase and hide env</strong></em></p>
</li>
<li><p><em>封装一些库自己用</em></p>
</li>
<li><p><em>多智能体share模型参数很难学习， 可以不同输入下看模型的输出用来协助决策</em></p>
</li>
</ul>
<h3 id="3-Rainbow-Combining-Improvements-in-Deep-Reinforcement-Learning"><a href="#3-Rainbow-Combining-Improvements-in-Deep-Reinforcement-Learning" class="headerlink" title="3. Rainbow: Combining Improvements in Deep Reinforcement Learning"></a>3. Rainbow: Combining Improvements in Deep Reinforcement Learning</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></p>
<p><em>Oct 2017</em></p>
<p><em>Matteo Hessel</em></p>
<p><em>Deepmind</em></p>
<h3 id="4-The-Surprising-Effectiveness-of-PPO-in-Cooperative-Multi-Agent-Games"><a href="#4-The-Surprising-Effectiveness-of-PPO-in-Cooperative-Multi-Agent-Games" class="headerlink" title="4. The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games"></a>4. The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01955">https://arxiv.org/abs/2103.01955</a></p>
<p><em>Nov 2022</em></p>
<p><em>chao yu</em></p>
<h2 id="Books（书籍阅读）"><a href="#Books（书籍阅读）" class="headerlink" title="Books（书籍阅读）"></a>Books（书籍阅读）</h2><h3 id="1-RLbook-by-sutton"><a href="#1-RLbook-by-sutton" class="headerlink" title="1. RLbook by sutton\\"></a>1. RLbook by sutton\\</h3><h2 id="Videos（视频学习）"><a href="#Videos（视频学习）" class="headerlink" title="Videos（视频学习）"></a>Videos（视频学习）</h2><h3 id="1-Multi-Agent-Reinforcement-Learning-Part-I"><a href="#1-Multi-Agent-Reinforcement-Learning-Part-I" class="headerlink" title="1. Multi-Agent Reinforcement Learning (Part I)"></a>1. Multi-Agent Reinforcement Learning (Part I)</h3><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=RCu-nU4_TQM">https://www.youtube.com/watch?v=RCu-nU4_TQM</a></p>
<blockquote>
<p>simon institute; chi jin, princeton</p>
</blockquote>
<h3 id="2-Shusen-wang"><a href="#2-Shusen-wang" class="headerlink" title="2. Shusen wang"></a>2. Shusen wang</h3><h2 id="Questions（疑问）"><a href="#Questions（疑问）" class="headerlink" title="Questions（疑问）"></a>Questions（疑问）</h2><h3 id="1-why-data-needs-to-be-IId-（为什么数据需要独立同分布）"><a href="#1-why-data-needs-to-be-IId-（为什么数据需要独立同分布）" class="headerlink" title="1. why data needs to be IId?（为什么数据需要独立同分布）"></a>1. why data needs to be IId?（为什么数据需要独立同分布）</h3><blockquote>
<p>可以参考分布式机器学习的一些东西，（我一直在做分布式，分布式机器学习，分布式存储）</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=STxtRucv_zo&t=1504s">https://www.youtube.com/watch?v=STxtRucv_zo&amp;t=1504s</a></p>
<p>1&#x2F; 具有相同的概率分布：分布没有波动</p>
<p>2&#x2F; 相互独立：了解一个变量的值不会提供另外一个变量的信息</p>
<h5 id="Example-1-edit"><a href="#Example-1-edit" class="headerlink" title="Example 1[edit]"></a>Example 1[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/w/index.php?title=Independent_and_identically_distributed_random_variables&action=edit&section=8">edit</a>]</h5><p>A sequence of outcomes of spins of a fair or unfair <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Roulette">roulette</a> wheel is i.i.d. One implication of this is that if the roulette ball lands on “red”, for example, 20 times in a row, the next spin is no more or less likely to be “black” than on any other spin (see the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gambler's_fallacy">Gambler’s fallacy</a>).</p>
<p>A sequence of fair or loaded dice rolls is i.i.d.</p>
<p>A sequence of fair or unfair coin flips is i.i.d.</p>
<p>In <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Signal_processing">signal processing</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Image_processing">image processing</a> the notion of transformation to i.i.d. implies two specifications, the “i.d.”part and the “i.” part:</p>
<p>(i.d.) the signal level must be balanced on the time axis;</p>
<p>(i.) the signal spectrum must be flattened, i.e. transformed by filtering (such as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deconvolution">deconvolution</a>) to a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/White_noise">white noise</a> signal (i.e. a signal where all frequencies are equally present).</p>
<h5 id="Example-2-edit"><a href="#Example-2-edit" class="headerlink" title="Example 2[edit]"></a>Example 2[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/w/index.php?title=Independent_and_identically_distributed_random_variables&action=edit&section=9">edit</a>]</h5><p>Toss a coin 10 times and record how many times does the coin lands on head.</p>
<ol>
<li>Independent – each outcome of landing will not affect the other outcome, which means the 10 results are independent from each other.</li>
<li>Identically Distributed – if the coin is a homogeneous material, each time the probability for head is 0.5, which means the probability is identical for each time.</li>
</ol>
<h5 id="Example-3-edit"><a href="#Example-3-edit" class="headerlink" title="Example 3[edit]"></a>Example 3[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/w/index.php?title=Independent_and_identically_distributed_random_variables&action=edit&section=10">edit</a>]</h5><p>Roll a dice 10 times and record how many time the result is 1.</p>
<ol>
<li>Independent – each outcome of the dice will not affect the next one, which means the 10 results are independent from each other.</li>
<li>Identically Distributed – if the dice is a homogeneous material, each time the probability for the number 1 is 1&#x2F;6, which means the probability is identical for each time.</li>
</ol>
<h5 id="Example-4-edit"><a href="#Example-4-edit" class="headerlink" title="Example 4[edit]"></a>Example 4[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/w/index.php?title=Independent_and_identically_distributed_random_variables&action=edit&section=11">edit</a>]</h5><p>Choose a card from a standard deck of cards containing 52 cards, then place the card back in the deck. Repeat it for 52 times. Record the number of King appears</p>
<ol>
<li>Independent – each outcome of the card will not affect the next one, which means the 52 results are independent from each other.</li>
<li>Identically Distributed – after drawing one card from it, each time the probability for King is 4&#x2F;52, which means the probability is identical for each time.</li>
</ol>
<h3 id="2-why-machine-learning-need-IID-（为什么机器学习需要数据独立同分布）"><a href="#2-why-machine-learning-need-IID-（为什么机器学习需要数据独立同分布）" class="headerlink" title="2. why machine learning need IID?（为什么机器学习需要数据独立同分布）"></a>2. why machine learning need IID?（为什么机器学习需要数据独立同分布）</h3><p>Machine learning uses currently acquired massive quantities of data to deliver faster, more accurate results.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables#cite_note-7">7]</a> Therefore, we need to use historical data with overall representativeness. If the data obtained is not representative of the overall situation, then the rules will be summarized badly or wrongly.</p>
<p>Through i.i.d. hypothesis, the number of individual cases in the training sample can be greatly reduced.</p>
<p>This assumption makes maximization very easy to calculate mathematically. Observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems. Because of the assumption of independence, the likelihood function can be written like this</p>
<p>In order to maximize the probability of the observed event, take the log function and maximize the parameter <em>θ</em>. </p>
<p>The computer is very efficient to calculate multiple additions, but it is not efficient to calculate the multiplication. This simplification is the core reason for the increase in computational efficiency. And this Log transformation is also in the process of maximizing, turning many exponential functions into linear functions.</p>
<p>For two reasons, this hypothesis is easy to use the central limit theorem in practical applications.</p>
<ol>
<li>Even if the sample comes from a more complex non-Gaussian distribution, it can also approximate well. Because it can be simplified from the central limit theorem to Gaussian distribution. For a large number of observable samples, “the sum of many random variables will have an approximately normal distribution”.</li>
<li>The second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit, as well as the data quality. Because the simplicity of the unit makes it easy to interpret and scale, and the representative power + scale out of the unit improves the model accuracy. Like in a deep neural network, each neuron is very simple but has strong representative power, layer by layer to represent more complex features to improve model accuracy.</li>
</ol>
<p>some user has photo full of animals, some user has photo full of views</p>
<p>iid：</p>
<p>Independent and identically distributed: the data is uniform, randomly disrupted, and the statistics of each node are similar (mean, variance)</p>
<p>If the data is disrupted, shuffle, the data becomes independent and identically distributed, which is equivalent to a node, a distribution within a set</p>
<p>The statistical nature of each mobile phone user’s data is different. Some people like to take pictures of landscapes, while others like to take selfies.</p>
<p>My understanding: to prevent crooked data science? Go up the hill evenly, don’t click left and right?</p>
<h3 id="3-why-Experience-replay（为什么使用经验回放）"><a href="#3-why-Experience-replay（为什么使用经验回放）" class="headerlink" title="3. why Experience replay（为什么使用经验回放）"></a>3. why Experience replay（为什么使用经验回放）</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rhslMPmj7SY&list=RDCMUC9qKcEgXHPFP2-ywYoA-E0Q&index=6">https://www.youtube.com/watch?v=rhslMPmj7SY&amp;list=RDCMUC9qKcEgXHPFP2-ywYoA-E0Q&amp;index=6</a></p>
</blockquote>
<h3 id="4-the-reuse-of-experience-in-experience-replay（为什么在experience-replay-中重复使用经验）"><a href="#4-the-reuse-of-experience-in-experience-replay（为什么在experience-replay-中重复使用经验）" class="headerlink" title="4. the reuse of experience in experience replay（为什么在experience replay 中重复使用经验）"></a>4. the reuse of experience in experience replay（为什么在experience replay 中重复使用经验）</h3><p>first thing: how is transition used?</p>
<p>each experience is a sampling of the real world,</p>
<p>if we want to reuse it , why not </p>
<h3 id="5-what-is-sampling-efficiency-in-rl-（强化学习的sample-efficiency代表什么）"><a href="#5-what-is-sampling-efficiency-in-rl-（强化学习的sample-efficiency代表什么）" class="headerlink" title="5. what is sampling efficiency in rl? （强化学习的sample efficiency代表什么）"></a>5. what is sampling efficiency in rl? （强化学习的sample efficiency代表什么）</h3><p>what is meaning of sampling efficiency：</p>
<p>sampling efficiency：</p>
<p>unbalance load, the amount of some user’s data is large, others are small so that some node has iterated hundreds of time, some just one time</p>
<h3 id="6-Q-learning-take-Td-error-as-loss-func-how-about-AC-and-PG-（policy-based梯度下降的loss-函数是什么）"><a href="#6-Q-learning-take-Td-error-as-loss-func-how-about-AC-and-PG-（policy-based梯度下降的loss-函数是什么）" class="headerlink" title="6. Q-learning take Td-error  as loss func, how about AC and PG? （policy based梯度下降的loss 函数是什么）"></a>6. Q-learning take Td-error  as loss func, how about AC and PG? （policy based梯度下降的loss 函数是什么）</h3><ul>
<li><strong>this is the leaning of td</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/image-20230418164141297.png" alt="image-20230418164141297"></p>
<ul>
<li><strong>Policy Gradient</strong>（<em><strong>基于策略的方法</strong></em>）</li>
</ul>
<p>Policy gradient：Instead of minimizing TD error, it maximizes V, maximizes V, directly calculates the derivative of the model parameters on A, and then performs gradient ascent to update the model parameters to maximize V,</p>
<ul>
<li>the update of policy in <em>Policy Gradient</em>*</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/image-20230418163835203.png" alt="image-20230418163835203"></p>
<ul>
<li>while the v is equal to :</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230420102406349.png" alt="image-20230420102406349"></p>
<ul>
<li>then calculate the derivative</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230420102508795.png" alt="image-20230420102508795"></p>
<ul>
<li>use chain rule to calculate form 2</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230420102536383.png" alt="image-20230420102536383"></p>
<ul>
<li>so the result is :</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/image-20230418163957262.png" alt="image-20230418163957262"></p>
<ul>
<li><p>for form 1 : at each state, summation over all actions, could also be replaced with MC </p>
</li>
<li><p>for form 2: Hard to calculate expectation, use MC instead</p>
</li>
<li><p><strong>overview of the algorithm</strong><em><strong>（算法总览）</strong></em></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/image-20230418174000255.png" alt="image-20230418174000255"></p>
<p><strong>questions</strong>（<em><strong>问题</strong></em>）</p>
<ul>
<li><h3 id="Question1-if-use-form-1-how-to-scan-over-all-actions"><a href="#Question1-if-use-form-1-how-to-scan-over-all-actions" class="headerlink" title="Question1: if use form 1: how to scan over all actions?"></a>Question1: if use form 1: how to scan over all actions?</h3></li>
<li><h3 id="Question2-if-use-form-2-how-to-guarantee-adequate-sampling"><a href="#Question2-if-use-form-2-how-to-guarantee-adequate-sampling" class="headerlink" title="Question2: if use form 2: how to guarantee adequate sampling?"></a>Question2: if use form 2: how to guarantee adequate sampling?</h3></li>
</ul>
<h3 id="6-how-about-sampling-multiple-actions-and-update-the-model-at-the-same-state-like-we-do-enough-sampling-at-the-same-state-in-multi-armed-bandit-（在一个状态重复试错学习）"><a href="#6-how-about-sampling-multiple-actions-and-update-the-model-at-the-same-state-like-we-do-enough-sampling-at-the-same-state-in-multi-armed-bandit-（在一个状态重复试错学习）" class="headerlink" title="6. how about sampling multiple actions and update the model at the same state?(like we do enough sampling at the same state in multi-armed bandit （在一个状态重复试错学习）"></a>6. how about sampling multiple actions and update the model at the same state?(like we do enough sampling at the same state in multi-armed bandit （在一个状态重复试错学习）</h3><p>because of the independence of data?</p>
<p>we know that related data and data with same distribution is harmful  </p>
<h3 id="7-how-to-Design-traffic-control-problem（多智能体信号灯控制，如何合作）"><a href="#7-how-to-Design-traffic-control-problem（多智能体信号灯控制，如何合作）" class="headerlink" title="7. how to  Design traffic control problem（多智能体信号灯控制，如何合作）"></a>7. how to  Design traffic control problem（多智能体信号灯控制，如何合作）</h3><p>fully-observed:</p>
<ol>
<li><em><strong>the action of other agents</strong></em> <ol>
<li>how to utilize the action of other agent</li>
</ol>
</li>
<li><em><strong>the strategy of other agents</strong></em> </li>
<li>how to utilized the strategy of other agents: prediction</li>
<li><em><strong>the state of other agents</strong></em></li>
</ol>
<p>the design of global reward function</p>
<p>the design of action in different levels of traffic</p>
<p>the source of random: random form the strategy; random from the state transferring of the env</p>
<hr>
<h1 id="projects-强化学习项目"><a href="#projects-强化学习项目" class="headerlink" title="projects(强化学习项目)"></a>projects(强化学习项目)</h1><h2 id="Project1-Mappo-official-implementation（mappo官方实现）"><a href="#Project1-Mappo-official-implementation（mappo官方实现）" class="headerlink" title="Project1: Mappo official implementation（mappo官方实现）"></a>Project1: Mappo official implementation（mappo官方实现）</h2><p><a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/on-policy">https://github.com/magiclucky1996/on-policy</a></p>
<h4 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h4><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01955">https://arxiv.org/abs/2103.01955</a></p>
<h4 id="bugs-when-running"><a href="#bugs-when-running" class="headerlink" title="bugs when running"></a>bugs when running</h4><ul>
<li>RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling <code>cublasSgemm( handle, opa, opb, m, n, k, &amp;alpha, a, lda, b, ldb, &amp;beta, c, ldc)</code></li>
</ul>
<h3 id="running"><a href="#running" class="headerlink" title="running"></a>running</h3><ul>
<li>first running</li>
</ul>
<hr>
<h2 id="Project2-light-mappo（mappo的轻量级实现）"><a href="#Project2-light-mappo（mappo的轻量级实现）" class="headerlink" title="Project2: light mappo（mappo的轻量级实现）"></a>Project2: light mappo（mappo的轻量级实现）</h2><p><a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/light_mappo">https://github.com/magiclucky1996/light_mappo</a></p>
<h3 id="Project-3-Marl-sumo（基于多智能体强化学习的信号灯控制）"><a href="#Project-3-Marl-sumo（基于多智能体强化学习的信号灯控制）" class="headerlink" title="Project 3: Marl-sumo（基于多智能体强化学习的信号灯控制）"></a>Project 3: Marl-sumo（基于多智能体强化学习的信号灯控制）</h3><p><a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/deeprl_signal_control">https://github.com/magiclucky1996/deeprl_signal_control</a></p>
<h4 id="install-sumo（安装sumo）"><a href="#install-sumo（安装sumo）" class="headerlink" title="install sumo（安装sumo）"></a>install sumo（安装sumo）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:sumo/stable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install sumo sumo-tools sumo-doc</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4 id="Problem1-with-sumo-cannot-find-local-schema（问题1：sumo-找不到本地文件）"><a href="#Problem1-with-sumo-cannot-find-local-schema（问题1：sumo-找不到本地文件）" class="headerlink" title="Problem1: with sumo cannot find local schema（问题1：sumo 找不到本地文件）"></a>Problem1: with sumo cannot find local schema（问题1：sumo 找不到本地文件）</h4><ul>
<li>warning</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Warning: Cannot <span class="built_in">read</span> <span class="built_in">local</span> schema <span class="string">&#x27;/usr/share/sumo/bin/data/xsd/additional_file.xsd&#x27;</span>, will try website lookup.</span><br><span class="line">Warning: Cannot <span class="built_in">read</span> <span class="built_in">local</span> schema <span class="string">&#x27;/usr/share/sumo/bin/data/xsd/routes_file.xsd&#x27;</span>, will try website lookup.</span><br><span class="line">Warning: Cannot <span class="built_in">read</span> <span class="built_in">local</span> schema <span class="string">&#x27;/usr/share/sumo/bin/data/xsd/net_file.xsd&#x27;</span>, will try website lookup.</span><br></pre></td></tr></table></figure>

<ul>
<li><em><strong>solution（解决方法：更改sumo版本）</strong></em></li>
</ul>
<p>change sumo version:turn to sumo 1.16, </p>
<h4 id="Problem2-not-using-gpu（问题2：-gpu未使用）"><a href="#Problem2-not-using-gpu（问题2：-gpu未使用）" class="headerlink" title="Problem2: not using gpu（问题2： gpu未使用）"></a>Problem2: not using gpu（问题2： gpu未使用）</h4><ul>
<li>solution</li>
</ul>
<p>reinstall all the staff related to gpu</p>
<p>after install driver, cuda, if they are set the right env variable?: this may cause not working, so now im running with cpu, i show reinstall and do a test.</p>
<h4 id="Problem-3-Code-is-written-in-tensorflow-1-but-gpu-only-support-tensorflow-2-（问题3：代码基于tf1）"><a href="#Problem-3-Code-is-written-in-tensorflow-1-but-gpu-only-support-tensorflow-2-（问题3：代码基于tf1）" class="headerlink" title="Problem 3:  Code is written in tensorflow 1 but gpu only support tensorflow  2 （问题3：代码基于tf1）"></a>Problem 3:  Code is written in tensorflow 1 but gpu only support tensorflow  2 （问题3：代码基于tf1）</h4><ol>
<li><em><strong>modify the code （对代码进行手动升级）</strong></em></li>
</ol>
<p>Replace all <code>import tensorflow as tf</code> statements with <code>import tensorflow.compat.v1 as tf</code> followed by <code>tf.disable_v2_behavior()</code>.</p>
<p>Replace all deprecated TensorFlow 1.x syntax with TensorFlow 2.x syntax. This includes changes to <code>tf.Session()</code> to <code>tf.compat.v1.Session()</code>, <code>tf.global_variables_initializer()</code> to <code>tf.compat.v1.global_variables_initializer()</code>, and so on.</p>
<p>Replace <code>tf.contrib</code> modules with equivalent modules in <code>tf.keras</code> or other TensorFlow 2.x modules. For example, <code>tf.contrib.layers</code> can be replaced with <code>tf.keras.layers</code>.</p>
<ol start="2">
<li><em><strong>migrate following the official instruction（使用官方脚本对代码进行升级）</strong></em></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/migrate">https://www.tensorflow.org/guide/migrate</a></p>
<ol start="3">
<li><em><strong>use tensorflow 1 and run with cpu(用tf1, 然后用cpu跑)</strong></em></li>
</ol>
<h4 id="Problem4-set-the-growth-of-gpu（问题4：-限制gpu内存增长）"><a href="#Problem4-set-the-growth-of-gpu（问题4：-限制gpu内存增长）" class="headerlink" title="Problem4: set the growth of gpu（问题4： 限制gpu内存增长）"></a>Problem4: set the growth of gpu（问题4： 限制gpu内存增长）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth=<span class="literal">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br></pre></td></tr></table></figure>









<h3 id="MARL-SUMO-Modeling-信号灯控制问题的马尔克夫决策过程建模"><a href="#MARL-SUMO-Modeling-信号灯控制问题的马尔克夫决策过程建模" class="headerlink" title="MARL-SUMO Modeling(信号灯控制问题的马尔克夫决策过程建模)"></a>MARL-SUMO Modeling(信号灯控制问题的马尔克夫决策过程建模)</h3><h4 id="Problem-definition（问题定义）"><a href="#Problem-definition（问题定义）" class="headerlink" title="Problem definition（问题定义）"></a>Problem definition（问题定义）</h4><h4 id="Markov-decision-process-modeling（mdp建模）"><a href="#Markov-decision-process-modeling（mdp建模）" class="headerlink" title="Markov decision process modeling（mdp建模）"></a>Markov decision process modeling（mdp建模）</h4><h4 id="System-informations（硬件信息）"><a href="#System-informations（硬件信息）" class="headerlink" title="System informations（硬件信息）"></a>System informations（硬件信息）</h4><figure class="highlight as"><figcaption><span>sdf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">system: ubuntu <span class="number">22.04</span> LTS</span><br><span class="line">python: <span class="number">3.8</span></span><br><span class="line">gpu: gtx <span class="number">3060</span> laptop version</span><br><span class="line">cuda: <span class="number">11.3</span></span><br><span class="line">cudnn: version correspongding to cuda version</span><br><span class="line">SUMO: <span class="number">1.16</span></span><br><span class="line">tensorflow: tensorflow-gpu <span class="number">1.14</span></span><br></pre></td></tr></table></figure>



<h4 id="bugs-and-problems（遇到的bug和问题）"><a href="#bugs-and-problems（遇到的bug和问题）" class="headerlink" title="bugs and problems（遇到的bug和问题）"></a>bugs and problems（遇到的bug和问题）</h4><h2 id="Training-record（训练结果统计）"><a href="#Training-record（训练结果统计）" class="headerlink" title="Training record（训练结果统计）"></a>Training record（训练结果统计）</h2><h5 id="Q-leaning-with-16-intersections（16路口-Qlearning）"><a href="#Q-leaning-with-16-intersections（16路口-Qlearning）" class="headerlink" title="Q-leaning with 16 intersections（16路口-Qlearning）"></a>Q-leaning with 16 intersections（16路口-Qlearning）</h5><ol>
<li>0413 morning</li>
</ol>
<ul>
<li>hyperparameter</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gamma = 0.99</span><br><span class="line">lr_init = 1e-4</span><br><span class="line">lr_decay = constant</span><br><span class="line">epsilon_init = 1.0</span><br><span class="line">epsilon_min = 0.01</span><br><span class="line">epsilon_decay = linear</span><br><span class="line">epsilon_ratio = 0.5</span><br><span class="line">num_fc = 128</span><br><span class="line">num_h = 64</span><br><span class="line">batch_size = 20</span><br><span class="line">buffer_size = 1000</span><br><span class="line">reward_norm = 3000.0</span><br><span class="line">reward_clip = 2.0</span><br></pre></td></tr></table></figure>

<ul>
<li><p>experiment time:  13h</p>
</li>
<li><p>experiment result:</p>
</li>
</ul>
<h5 id=""><a href="#" class="headerlink" title=""></a><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/iqld.png" alt="iqld"></h5><ol>
<li>0413 morning</li>
</ol>
<ul>
<li><p>hyperparater</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[MODEL_CONFIG]</span><br><span class="line">rmsp_alpha = 0.99</span><br><span class="line">rmsp_epsilon = 1e-5</span><br><span class="line">max_grad_norm = 40</span><br><span class="line">gamma = 0.99</span><br><span class="line">lr_init = 5e-4</span><br><span class="line">lr_decay = constant</span><br><span class="line">entropy_coef_init = 0.01</span><br><span class="line">entropy_coef_min = 0.01</span><br><span class="line">entropy_decay = constant</span><br><span class="line">entropy_ratio = 0.5</span><br><span class="line">value_coef = 0.5</span><br><span class="line">num_fw = 128</span><br><span class="line">num_ft = 32</span><br><span class="line">num_lstm = 64</span><br><span class="line">num_fp = 64</span><br><span class="line">batch_size = 120</span><br><span class="line">reward_norm = 2000.0</span><br><span class="line">reward_clip = 2.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>result</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/ac413.png" alt="ac413"></p>
</li>
<li><p>sac run before</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/test/ac.png" alt="ac"></p>
</li>
</ul>
<h2 id="Project-4-General-marl-项目4：-多智能体算法实现"><a href="#Project-4-General-marl-项目4：-多智能体算法实现" class="headerlink" title="Project 4: General marl(项目4： 多智能体算法实现)"></a>Project 4: General marl(项目4： 多智能体算法实现)</h2><p><a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/MARL-code-pytorch">https://github.com/magiclucky1996/MARL-code-pytorch</a></p>
<h1 id="what’s-next-（下一步计划）"><a href="#what’s-next-（下一步计划）" class="headerlink" title="what’s next?（下一步计划）"></a>what’s next?（下一步计划）</h1><h4 id="1-what’s-next-Incorporate-Mappo-into-sumo-simulation（mappo加到交通信号灯控制）"><a href="#1-what’s-next-Incorporate-Mappo-into-sumo-simulation（mappo加到交通信号灯控制）" class="headerlink" title="1. what’s next? Incorporate Mappo into sumo simulation（mappo加到交通信号灯控制）"></a>1. what’s next? Incorporate Mappo into sumo simulation（mappo加到交通信号灯控制）</h4><ul>
<li>actually this is similar to what we do two years ago, combine sac with sumo simulation</li>
<li>but this time, dive deeper into problem definition, modeling, network , rl learning algorithm part.</li>
</ul>
<h3 id="2-what’s-next-go-through-rl-basics（了解rl的基础）"><a href="#2-what’s-next-go-through-rl-basics（了解rl的基础）" class="headerlink" title="2. what’s next? go through rl basics（了解rl的基础）"></a>2. what’s next? go through rl basics（了解rl的基础）</h3><h4 id="3-what’s-next-go-through-markov-modeling（了解马尔克夫建模的知识）"><a href="#3-what’s-next-go-through-markov-modeling（了解马尔克夫建模的知识）" class="headerlink" title="3. what’s next? go through markov modeling（了解马尔克夫建模的知识）"></a>3. what’s next? go through markov modeling（了解马尔克夫建模的知识）</h4><h4 id="4-what’s-next-go-through-urban-traffic-modeling-and-control（了解城市交通建模）"><a href="#4-what’s-next-go-through-urban-traffic-modeling-and-control（了解城市交通建模）" class="headerlink" title="4. what’s next? go through urban traffic modeling and control（了解城市交通建模）"></a>4. what’s next? go through urban traffic modeling and control（了解城市交通建模）</h4><p><em><strong>insights</strong></em></p>
<ol>
<li><em><strong>if the initialization of env is random, so the first state is random for every episode, will this improve sample efficiency</strong></em>?<em>(环境初始化，初始化状态是不一样的，如果随机初始化，是否增加采样效率？)</em></li>
<li>***what if a state is repeated for agent to do trail and error? like in a game restart from the failed node, and maybe we remember this state and initialize there in the next episode?*<em>如果可以像游戏一样反复在一个节点重来，比如我记住复杂的那个地方，然后下个回合从那里初始化，就像打游戏，在一个关反复重开（比如在交通信号灯问题里，，一个状态的试错学习，多试几次，）</em></li>
</ol>
<hr>
<h2 id="Part6-Additional-information（附加信息）"><a href="#Part6-Additional-information（附加信息）" class="headerlink" title="Part6 : Additional information（附加信息）"></a>Part6 : Additional information（附加信息）</h2><h2 id="1-Deploy-blog-with-hexo-typora-github-and-picgo（基于hexo进行博客搭建）"><a href="#1-Deploy-blog-with-hexo-typora-github-and-picgo（基于hexo进行博客搭建）" class="headerlink" title="1. Deploy blog with hexo, typora,github and picgo（基于hexo进行博客搭建）"></a>1. Deploy blog with hexo, typora,github and picgo（基于hexo进行博客搭建）</h2><ul>
<li>install nodejs</li>
<li>install hexo</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<ul>
<li>create a blog</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init my-blog</span><br></pre></td></tr></table></figure>

<ul>
<li>change to this blog</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> my-blog	</span><br></pre></td></tr></table></figure>

<ul>
<li>install dependence</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure>

<ul>
<li>install hexo deployer</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer --save</span><br></pre></td></tr></table></figure>

<ul>
<li>modify the _config.yml</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">url:</span> <span class="string">https://&lt;your</span> <span class="string">github</span> <span class="string">user</span> <span class="string">name&gt;.github.io/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:username/username.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>link local git with github so that you can push local file to github</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t ed25519 -C <span class="string">&quot;your_email@example.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> ~/.ssh/id_ed25519.pub</span><br></pre></td></tr></table></figure>

<ul>
<li>copy this pub key to your github account: settings -&gt; ssh and gpg keys</li>
<li>write your blogs in format of .md and add it to …&#x2F;hexo&#x2F;my-blog&#x2F;source&#x2F;_posts</li>
<li>use hexo generate to generate html files</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>

<ul>
<li>use hexo server to view locally</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>

<ul>
<li>use hexo deploy to deploy to github</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>





<h3 id="2-Version-control-for-blog-source-with-multiple-devices-by-git-and-github（多个终端对博客内容进行版本控制）"><a href="#2-Version-control-for-blog-source-with-multiple-devices-by-git-and-github（多个终端对博客内容进行版本控制）" class="headerlink" title="2. Version control for blog source with multiple devices by git and github（多个终端对博客内容进行版本控制）"></a>2. Version control for blog source with multiple devices by git and github（多个终端对博客内容进行版本控制）</h3><ul>
<li>on one computer</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/hexo/my-blog/source/_posts</span><br><span class="line">git init</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&quot;first commit on linux&quot;</span></span><br><span class="line">git remote add origin &lt;<span class="built_in">link</span> of github repo&gt;</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>

<ul>
<li>on another computer</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/hexo/my-blog/source</span><br><span class="line">git <span class="built_in">clone</span> &lt;<span class="built_in">link</span> of github repo&gt;</span><br><span class="line">git pull origin main <span class="comment"># pull from remote repo</span></span><br><span class="line">git push -u origin main <span class="comment"># pull to the remote repo</span></span><br></pre></td></tr></table></figure>



<h3 id="3-Image-uploader-Picgo-and-typora（图床设置）"><a href="#3-Image-uploader-Picgo-and-typora（图床设置）" class="headerlink" title="3. Image uploader: Picgo and typora（图床设置）"></a>3. Image uploader: Picgo and typora（图床设置）</h3><h5 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h5><ul>
<li><p><a target="_blank" rel="noopener" href="https://support.typora.io/Upload-Image/">https://support.typora.io/Upload-Image/</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://picgo.github.io/PicGo-Core-Doc/zh/guide/config.html#%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">https://picgo.github.io/PicGo-Core-Doc/zh/guide/config.html#%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6</a></p>
</li>
</ul>
<h5 id="Detailed-steps"><a href="#Detailed-steps" class="headerlink" title="Detailed steps"></a>Detailed steps</h5><ul>
<li>in typora<ul>
<li>click file -&gt; preference -&gt; image: choose download to install picgo-core</li>
</ul>
</li>
<li><code>picgo set uploader</code></li>
<li><code>picgo use uploader</code></li>
<li>you can set automatically upload photo in typora -&gt; preference -&gt; image</li>
<li>it’s done! enjoy!</li>
</ul>
<h3 id="insights"><a href="#insights" class="headerlink" title="insights"></a><em><strong>insights</strong></em></h3><ol>
<li><em><strong>we don’t need to run it by ourselves if the graph run by others is enough for our need.(别人跑过的算法我不跑</strong>)</em>**</li>
</ol>
<p><em><strong>2. if have a goal or a question to figure, it will motivate u , so seperate it into some detailed small questions, and motivate urself, so there will be postive feedback from exploring.(如果以疑问驱动，或者把要做的事情分解成具体的步骤，变成办公，效率会高很多)</strong></em></p>
<h2 id="MARL-modeling（信号灯建模）"><a href="#MARL-modeling（信号灯建模）" class="headerlink" title="MARL modeling（信号灯建模）"></a>MARL modeling（信号灯建模）</h2><h2 id="traffic-control-ma2c"><a href="#traffic-control-ma2c" class="headerlink" title="traffic control + ma2c"></a>traffic control + ma2c</h2><ul>
<li>problem definition		（问题定义）</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230420114007282.png" alt="image-20230420114007282"></p>
<ul>
<li><strong>state space（状态空间）</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230420111112341.png" alt="image-20230420111112341"></p>
<ul>
<li><h2 id="action-space（动作空间）-phase-switch"><a href="#action-space（动作空间）-phase-switch" class="headerlink" title="action space（动作空间）- phase switch"></a><strong>action space（动作空间）</strong><br>- phase switch</h2><ul>
<li><p>phase duration</p>
<ul>
<li>make decision for how long the phase last</li>
</ul>
</li>
<li><p>phase itself</p>
<ul>
<li>fixed control period</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>reward function （奖励函数）</strong></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230420111340901.png" alt="image-20230420111340901"></p>
<ul>
<li><strong>training algorithm</strong>(<strong>训练算法</strong>)</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/week4&5/" data-id="clham94tn000dh1ot4tl81lai" data-title="week4&amp;5" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/work/" style="font-size: 20px;">work</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/05/05/dublin_remote_work/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/">dublin可做的事情+ tcd设施调研</a>
          </li>
        
          <li>
            <a href="/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/">都柏林租房攻略</a>
          </li>
        
          <li>
            <a href="/2023/05/05/rl_envs_and_modeling/">rl_play</a>
          </li>
        
          <li>
            <a href="/2023/05/05/week%206%20share/">week6</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 tianxiang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>