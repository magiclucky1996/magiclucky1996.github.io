<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>public blog of tianxiang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="public blog of tianxiang">
<meta property="og:url" content="http://magiclucky1996.github.io/index.html">
<meta property="og:site_name" content="public blog of tianxiang">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="tianxiang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="public blog of tianxiang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">public blog of tianxiang</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://magiclucky1996.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-try hack me- penetration tester" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/15/try%20hack%20me-%20penetration%20tester/" class="article-date">
  <time class="dt-published" datetime="2023-05-15T09:39:35.632Z" itemprop="datePublished">2023-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="try-hack-me-penetration-tester"><a href="#try-hack-me-penetration-tester" class="headerlink" title="try hack me- penetration tester"></a>try hack me- penetration tester</h1><h2 id="introduction-t-cyber-security"><a href="#introduction-t-cyber-security" class="headerlink" title="introduction t cyber security"></a>introduction t cyber security</h2><ul>
<li>offensive security<ul>
<li>find hidden page and hack the bank</li>
<li>breaking into system, exploit bug, find loopholes</li>
</ul>
</li>
</ul>
<h2 id="walk-an-application"><a href="#walk-an-application" class="headerlink" title="walk an application"></a>walk an application</h2><ul>
<li>Short breakdown<ul>
<li>view source</li>
<li>Network</li>
<li>Inspector: <ul>
<li>view the blocks</li>
</ul>
</li>
<li>debugger<ul>
<li>control JS</li>
</ul>
</li>
</ul>
</li>
<li>explore the web<ul>
<li>view web sites frame</li>
</ul>
</li>
<li>page source<ul>
<li>most are made of <strong>frame</strong></li>
</ul>
</li>
<li>inspector<ul>
<li>view the css style and change<ul>
<li>Display: block. -&gt; display: none</li>
</ul>
</li>
</ul>
</li>
<li>debugger<ul>
<li>using break point to stop the running of the js</li>
</ul>
</li>
<li>Network</li>
</ul>
<h2 id="content-discovery"><a href="#content-discovery" class="headerlink" title="content discovery"></a>content discovery</h2><ul>
<li><p><strong>Robots.txt</strong></p>
</li>
<li><p><strong>Favicon</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://wiki.owasp.org/index.php/OWASP_favicon_database">https://wiki.owasp.org/index.php/OWASP_favicon_database</a></p>
</li>
<li><p>look up the hash value of favicon</p>
</li>
<li><p>find the frame</p>
</li>
</ul>
</li>
<li><p><strong>Sitemap.xml</strong></p>
<ul>
<li>list of files the owner wish to be listed on search engine</li>
</ul>
</li>
<li><p><strong>http headers</strong></p>
<ul>
<li>Contains information like: <ul>
<li><ol>
<li>server software</li>
<li>Programming language</li>
</ol>
</li>
</ul>
</li>
<li>visit by terminal: curl “ip address” -v</li>
</ul>
</li>
<li><p><strong>framework stack</strong></p>
<ul>
<li>View the frame from the source information, </li>
<li>find some drawback from the frame documentation</li>
</ul>
</li>
<li><p><strong>google hacking</strong></p>
<ul>
<li>Site:tryhackme.com</li>
<li>Inurl: admin</li>
<li>filetype</li>
<li>Entitle: admin</li>
</ul>
</li>
<li><p><strong>Wappalyzer</strong></p>
<ul>
<li>Analyze the frame that this page use</li>
<li>has google extension</li>
</ul>
</li>
<li><p><strong>wayback machine</strong></p>
</li>
<li><p>github</p>
<ul>
<li>look for <strong>company names</strong> or <strong>website names</strong> to  locate repositories of target</li>
</ul>
</li>
<li><p><strong>S3 Buckets</strong></p>
<ul>
<li>Cloud, sometims forget to set private</li>
<li>http(s):&#x2F;&#x2F;<strong>{name}.</strong><a target="_blank" rel="noopener" href="http://s3.amazonaws.com/"><strong>s3.amazonaws.com</strong></a></li>
<li>where to find the url:<ul>
<li>Web source</li>
<li>github repo</li>
<li>Animated generate<ul>
<li><strong>{name}</strong>-assets, <strong>{name}</strong>-www, <strong>{name}</strong>-public, <strong>{name}</strong>-private</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Automated Discovery</strong></p>
<ul>
<li>Tools to automated<ul>
<li>ffuf</li>
<li>dirb</li>
<li>Robuster</li>
</ul>
</li>
</ul>
</li>
<li><p>conclusion</p>
<ul>
<li>步骤是可复用的，直接用脚本自动化就可以，</li>
</ul>
</li>
</ul>
<h3 id="Subdomain-enumeate"><a href="#Subdomain-enumeate" class="headerlink" title="Subdomain enumeate"></a>Subdomain enumeate</h3><ul>
<li>Ssl&#x2F;tsl<ul>
<li>find sub domain by ssl service look up<ul>
<li><a target="_blank" rel="noopener" href="http://crt.sh/">http://crt.sh/</a></li>
<li><a target="_blank" rel="noopener" href="https://ui.ctsearch.entrust.com/ui/ctsearchui">https://ui.ctsearch.entrust.com/ui/ctsearchui</a></li>
</ul>
</li>
</ul>
</li>
<li>google<ul>
<li><strong>-site:<a target="_blank" rel="noopener" href="http://www.tryhackme.com/">www.tryhackme.com</a> site:*.tryhackme.com</strong></li>
</ul>
</li>
<li>DNS brute force（子域爆破）</li>
<li>autamated tool<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/aboul3la/Sublist3r">https://github.com/aboul3la/Sublist3r</a></li>
</ul>
</li>
<li>Virtual host(爆破host name，替换)<ul>
<li>enumerate for host names on a server</li>
<li>when server host multiple webs on same ip, host name is used to identify</li>
</ul>
</li>
</ul>
<h3 id="authentication-bypass"><a href="#authentication-bypass" class="headerlink" title="authentication bypass"></a>authentication bypass</h3><ul>
<li>User name enumerate<ul>
<li>test which user name exists</li>
</ul>
</li>
<li>Brute force</li>
<li>logic flaw</li>
<li>Cookie tampering<ul>
<li>Cookie  会进行编码</li>
</ul>
</li>
</ul>
<h3 id="idor"><a href="#idor" class="headerlink" title="idor"></a>idor</h3><ul>
<li>detect<ul>
<li>create two accounts, swap the id between them</li>
</ul>
</li>
<li>parameter may not be address<ul>
<li>AJAX</li>
<li>JS</li>
</ul>
</li>
<li>an unreferenced parameter that may have been of some use</li>
<li><strong>&#x2F;user&#x2F;details?user_id&#x3D;123</strong></li>
<li>practical<ul>
<li>network 看到从api通过userid请求数据</li>
<li>更改链接传参中的userid来获取数据</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/15/try%20hack%20me-%20penetration%20tester/" data-id="clhonv51l0001t9ot5ts63toz" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-workshop pre1: survey on traffic" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/15/workshop%20pre1:%20survey%20on%20traffic/" class="article-date">
  <time class="dt-published" datetime="2023-05-15T09:39:35.623Z" itemprop="datePublished">2023-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="what-to-do-today"><a href="#what-to-do-today" class="headerlink" title="what to do today:"></a>what to do today:</h3><p>make decisions for what to do and do it .</p>
<ol>
<li><p>make the pre for next week</p>
</li>
<li><p>finish the student work (no i wish)</p>
</li>
<li><p>finish reading for rl and marl(god)</p>
</li>
<li><p>buy shoes and belt and maybe towel</p>
</li>
</ol>
<h3 id="specific-direction"><a href="#specific-direction" class="headerlink" title="specific direction"></a>specific direction</h3><p>direction 1 : hierarchical traffic control</p>
<p>direction 2 : online learning</p>
<h3 id="survey-for-traffic-control"><a href="#survey-for-traffic-control" class="headerlink" title="survey for traffic control"></a>survey for traffic control</h3><ul>
<li><p>ask chatgpt to give the survey on traffic control</p>
<ul>
<li><p>way of control</p>
<ul>
<li>traditional: off-line: get a offline strategy, and implement it </li>
<li>dynamic: adjust based on real-time data</li>
</ul>
</li>
<li><p>systems in industry</p>
<ul>
<li>australia: SCATS</li>
<li>england: SCOOT</li>
<li>america: RHODES</li>
<li>italy: SPOT&#x2F;UTOPIA</li>
<li>China: NUTCS SMOOTH</li>
</ul>
</li>
<li><p>systems currently used </p>
<ul>
<li><p>ireland: SCATS</p>
</li>
<li><p>singapore: UTC (SCOOT algorithm)</p>
</li>
<li><p>Japan: TCIS (ALINEA（Adaptive Line Enhancement Algorithm）algorithm)</p>
</li>
<li><p>England: scoot MOVA UTC</p>
</li>
<li><p>Germany: centralized control : Zentrale Verkehrsleittechnik”（ZVL）</p>
<ul>
<li><p>scale: coordinate the whole country and cities</p>
</li>
<li><p>method:</p>
</li>
<li><p>resource</p>
<ul>
<li><p>Websites of traffic management companies:</p>
</li>
<li><p>Siemens Mobility: h</p>
</li>
<li><p>ttps:&#x2F;&#x2F;<a target="_blank" rel="noopener" href="http://www.siemens-mobility.com/de/de/themen/verkehrstechnik/verkehrsmanagement/">www.siemens-mobility.com/de/de/themen/verkehrstechnik/verkehrsmanagement/</a></p>
</li>
<li><p>Swarco Traffic Systems: <a target="_blank" rel="noopener" href="https://www.swarco.com/de/Verkehrssysteme/Verkehrsmanagement">https://www.swarco.com/de/Verkehrssysteme/Verkehrsmanagement</a></p>
</li>
<li><p>Imtech Traffic &amp; Infra: <a target="_blank" rel="noopener" href="https://www.imtechtraffic.com/de/">https://www.imtechtraffic.com/de/</a></p>
</li>
</ul>
<ol>
<li>Government websites:</li>
</ol>
<ul>
<li>Federal Ministry of Transport and Digital Infrastructure (BMVI): <a target="_blank" rel="noopener" href="https://www.bmvi.de/SharedDocs/DE/Home.html">https://www.bmvi.de/SharedDocs/DE/Home.html</a></li>
<li>State transportation agencies (examples):<ul>
<li>Hessen Mobil: <a target="_blank" rel="noopener" href="https://www.mobil.hessen.de/">https://www.mobil.hessen.de/</a></li>
<li>Landesbetrieb Straßenbau NRW: <a target="_blank" rel="noopener" href="https://www.strassen.nrw.de/">https://www.strassen.nrw.de/</a></li>
</ul>
</li>
</ul>
<ol>
<li>Research databases:</li>
</ol>
<ul>
<li>Google Scholar: <a target="_blank" rel="noopener" href="https://scholar.google.com/">https://scholar.google.com/</a></li>
<li>IEEE Xplore: <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/Xplore/home.jsp">https://ieeexplore.ieee.org/Xplore/home.jsp</a></li>
</ul>
<ol>
<li>Professional associations:</li>
</ol>
<ul>
<li>German Association for Traffic and Transportation (DVWG): <a target="_blank" rel="noopener" href="https://www.dvwg.de/">https://www.dvwg.de/</a></li>
<li>German Road and Transportation Research Association (FGSV): <a target="_blank" rel="noopener" href="https://www.fgsv.de/">https://www.fgsv.de/</a></li>
</ul>
</li>
</ul>
</li>
<li><p>US: scats, scoot, ATMS (Advanced Traffic Management System) and TSP (Transit Signal Priority).</p>
</li>
<li><p>China: NUTCS SMOOTH iurban I-sense</p>
</li>
</ul>
</li>
<li><p>frames and algorithms in currently used system</p>
<ul>
<li><p><strong>SCOOT</strong>	</p>
<ul>
<li><p><strong>“reserach on the transyt and scoot methods of signal coordination”: key ideas of transyt and scoot</strong></p>
<ul>
<li><p>Dennis I. Robertson, 1986</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=dfefb6e8b96b19ecb7693619fc92530e0a95e7a2">https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=dfefb6e8b96b19ecb7693619fc92530e0a95e7a2</a></p>
</li>
<li><p>genius of rrl invented combination method of coordinating signals</p>
<ul>
<li>reference: glasgow experiment in area traffic control</li>
<li>traffic model estimated average queue at all stop line </li>
<li>sum of queues are the optimization goal,</li>
</ul>
</li>
<li><p>for optimization , <strong>goal</strong> should be clear, and <strong>queue</strong> as goal is good </p>
<ul>
<li>sum of queues</li>
<li>if u optimize sum of queue, delay also tend to be lower</li>
<li>optimization goal also take stop times<ul>
<li>waste energy; cause danger</li>
<li>weight factor balance <strong>queue</strong> and <strong>stops</strong><ul>
<li>when queue is optimized, stops are also optimized, when stops are taken valued, agent tend to prolong cycle times</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>many engineers maximize bandwidth in time-distance graph, but bandwidth is not good when</strong></p>
<ul>
<li>not possible to calculate financial term</li>
<li>when congestion , bandwidth  concept starts to break down<br>because the growth of queues disrupts<br>the bands<ul>
<li>bandwidth:  maximum flow rate that can be achieved while maintaining a reasonable speed and avoiding congestion.</li>
</ul>
</li>
<li>signal coordination on a fixed time plan<ul>
<li><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230514185002902.png" alt="image-20230514185002902"></li>
</ul>
</li>
</ul>
</li>
<li><p>Transyt</p>
<ul>
<li>have real-time problem to use real-time data</li>
</ul>
</li>
<li><p><strong>scoot principle</strong></p>
<ul>
<li>measure CFP</li>
<li>online traffic model</li>
<li><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230514191147690.png" alt="image-20230514191147690"></li>
<li>incremental optimization<ul>
<li>elastic coordination<ul>
<li>before phase change, three options:<ul>
<li><ol>
<li>advance  change for 4 seconds</li>
<li>retard change for 4 seconds</li>
<li>unchanged</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>optimizing networks of traffic signals in real time- the scoot method</strong>: how scoot evloves</p>
<ul>
<li>Dennis I. Robertson, 1991</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=69966&tag=1">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=69966&amp;tag=1</a></li>
<li><h2 id="firstly-utc-system-time-distance-graph-ideal-green-wave-in-the-time-distance-diagram-it-is-hard-to-estimate-the-queues-with-TD-then-there-starts-to-be-transyt-transyt-developed-by-TRRL-prediction-assume-traffic-platoons-travel-at-a-known-speed-with-some-dispersion-一定分布下定速行驶-queue-discharge-full-rate-during-green-time-一定速率驶出路口-one-year-to-get-a-plan-for-offline-solution-princeple-of-scoot-mearsure-cfp-in-real-time-update-online-model-of-queue-incremental-optimization-of-signals-information-of-traffic-control-mode-in-toronto-https-www-toronto-ca-services-payments-streets-parking-transportation-traffic-management-traffic-signals-street-signs-traffic-signals-in-toronto-mode-of-control-TRL-software-https-trlsoftware-com-products-traffic-control-scoot-traffic-simulation-software-VISSIM-S-Paramics-sumo-aimsun-transmodeler-road-sumulation-ARCADY、PICADY-和-OSCADY"><a href="#firstly-utc-system-time-distance-graph-ideal-green-wave-in-the-time-distance-diagram-it-is-hard-to-estimate-the-queues-with-TD-then-there-starts-to-be-transyt-transyt-developed-by-TRRL-prediction-assume-traffic-platoons-travel-at-a-known-speed-with-some-dispersion-一定分布下定速行驶-queue-discharge-full-rate-during-green-time-一定速率驶出路口-one-year-to-get-a-plan-for-offline-solution-princeple-of-scoot-mearsure-cfp-in-real-time-update-online-model-of-queue-incremental-optimization-of-signals-information-of-traffic-control-mode-in-toronto-https-www-toronto-ca-services-payments-streets-parking-transportation-traffic-management-traffic-signals-street-signs-traffic-signals-in-toronto-mode-of-control-TRL-software-https-trlsoftware-com-products-traffic-control-scoot-traffic-simulation-software-VISSIM-S-Paramics-sumo-aimsun-transmodeler-road-sumulation-ARCADY、PICADY-和-OSCADY" class="headerlink" title="firstly : utc system,- time distance graph- - ideal green wave: in the time,distance diagram, it is hard to estimate the queues with TD, then there starts to be transyt- transyt:  - developed by TRRL  - prediction: assume traffic platoons travel at a known speed with some dispersion(一定分布下定速行驶), queue discharge full rate during green time(一定速率驶出路口)  - one year to get a plan for offline solution- princeple of scoot  - mearsure cfp in real time  -  update online model of queue  - incremental optimization of signals- information of traffic control mode in toronto  - https://www.toronto.ca/services-payments/streets-parking-transportation/traffic-management/traffic-signals-street-signs/traffic-signals-in-toronto/mode-of-control/- TRL software  - https://trlsoftware.com/products/traffic-control/scoot/  - traffic simulation software : VISSIM , S-Paramics, sumo, aimsun, transmodeler  - road sumulation: ARCADY、PICADY 和 OSCADY"></a>firstly : utc system,<br>- time distance graph<br>- <img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230514162227682.png" alt="image-20230514162227682"><br>- ideal green wave: in the time,distance diagram, it is hard to estimate the queues with TD, then there starts to be <strong>transyt</strong><br>- <strong>transyt</strong>:<br>  - developed by TRRL<br>  - prediction: assume traffic platoons travel at a known speed with some dispersion(一定分布下定速行驶), queue discharge full rate during green time(一定速率驶出路口)<br>  - one year to get a plan for offline solution<br>- princeple of scoot<br>  - mearsure cfp in real time<br>  -  update online model of queue<br>  - incremental optimization of signals<br>- information of traffic control mode in toronto<br>  - <a target="_blank" rel="noopener" href="https://www.toronto.ca/services-payments/streets-parking-transportation/traffic-management/traffic-signals-street-signs/traffic-signals-in-toronto/mode-of-control/">https://www.toronto.ca/services-payments/streets-parking-transportation/traffic-management/traffic-signals-street-signs/traffic-signals-in-toronto/mode-of-control/</a><br>- TRL software<br>  - <a target="_blank" rel="noopener" href="https://trlsoftware.com/products/traffic-control/scoot/">https://trlsoftware.com/products/traffic-control/scoot/</a><br>  - traffic simulation software : VISSIM , S-Paramics, sumo, aimsun, transmodeler<br>  - road sumulation: <strong>ARCADY、PICADY 和 OSCADY</strong></h2></li>
</ul>
</li>
<li><p>ALINEA</p>
<ul>
<li>predictive and control</li>
</ul>
</li>
<li><p><strong>SCATS</strong></p>
</li>
<li><p>RHODES</p>
</li>
<li><p>SPOT&#x2F;UTOPIA</p>
</li>
</ul>
</li>
<li><p>conclusion for algorithms used </p>
<ul>
<li>optimization method: OPAC, prodyn: ()</li>
<li>fuzzy control(approximate the strategy)</li>
<li>revolution method (search in strategy space)</li>
<li>reinforcement learning</li>
</ul>
</li>
<li><p>imagine the easiest way: simple traffic flow , like the water,</p>
</li>
</ul>
</li>
<li><p>ask chatgpt to give the survey on traffic control with rl</p>
</li>
<li><p>ask chatgpt to recommend surveys on traffic control</p>
</li>
<li><p>ask chatgpt to recommend surveys on traffic control with rl</p>
</li>
</ul>
<h3 id="the-structure-of-my-pre-first-gather-information-later-make-the-graph"><a href="#the-structure-of-my-pre-first-gather-information-later-make-the-graph" class="headerlink" title="the structure of my pre, first gather information, later make the graph"></a>the structure of my pre, first gather information, later make the graph</h3><ol>
<li>what problems there are : <ol>
<li>given urban traffic, control the traffic by intersection control</li>
<li></li>
</ol>
</li>
<li>what has been done for each problem</li>
<li>in which way they fix each problem:</li>
<li>what has not been done</li>
<li>what we could do:</li>
</ol>
<h3 id="rl-system-for-urban-traffic"><a href="#rl-system-for-urban-traffic" class="headerlink" title="rl system for urban traffic"></a>rl system for urban traffic</h3><ul>
<li><h2 id="preask-chatgpt-“Multi-agent-reinforcement-learning-for-traffic-signal-control”-by-Wiering-et-al-This-paper-proposes-a-multi-agent-reinforcement-learning-approach-for-traffic-signal-control-which-has-been-shown-to-outperform-traditional-signal-control-methods-in-large-scale-simulations-https-ieeexplore-ieee-org-document-6958095-survey-reference-5"><a href="#preask-chatgpt-“Multi-agent-reinforcement-learning-for-traffic-signal-control”-by-Wiering-et-al-This-paper-proposes-a-multi-agent-reinforcement-learning-approach-for-traffic-signal-control-which-has-been-shown-to-outperform-traditional-signal-control-methods-in-large-scale-simulations-https-ieeexplore-ieee-org-document-6958095-survey-reference-5" class="headerlink" title="preask chatgpt- “Multi-agent reinforcement learning for traffic signal control” by Wiering et al. This paper proposes a multi-agent reinforcement learning approach for traffic signal control, which has been shown to outperform traditional signal control methods in large-scale simulations.  - https://ieeexplore.ieee.org/document/6958095  - survey: reference 5"></a>preask chatgpt<br>- “Multi-agent reinforcement learning for traffic signal control” by Wiering et al. This paper proposes a multi-agent reinforcement learning approach for traffic signal control, which has been shown to outperform traditional signal control methods in large-scale simulations.<br>  - <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/6958095">https://ieeexplore.ieee.org/document/6958095</a><br>  - survey: reference 5</h2><ul>
<li>“Deep reinforcement learning for traffic signal control” by El-Tantawy et al. This paper proposes a deep reinforcement learning approach for traffic signal control that uses a convolutional neural network to represent the state of the traffic network.</li>
<li>“A survey of reinforcement learning applications in traffic signal control” by Li et al. This paper provides a comprehensive survey of the various reinforcement learning techniques that have been applied to traffic signal control, along with their strengths and limitations.</li>
<li>“Distributed reinforcement learning for adaptive traffic signal control” by Ma et al. This paper proposes a distributed reinforcement learning approach for traffic signal control that allows for coordination between traffic signals at different intersections.</li>
<li>“Machine learning for traffic signal control: A review” by Ma et al. This paper provides a review of the various machine learning techniques that have been applied to traffic signal control, including supervised learning, unsupervised learning, and reinforcement learning.</li>
</ul>
</li>
<li><p>as far as  i can remember</p>
<ul>
<li>thousand of junctions</li>
<li>tianshu chu</li>
<li>my master thesis: some classical papers</li>
</ul>
</li>
</ul>
<p><strong>what is the easiest relationship between the traffic phase of intersctions</strong></p>
<ul>
<li>give a influence on future queue of next intersection , and because there are two options , so the decision-making is either benefical or harmful, difference would be how much it is , or it is actually zero.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/15/workshop%20pre1:%20survey%20on%20traffic/" data-id="clhonv51l0002t9ot0isfam5t" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-how to make face fat" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/15/how%20to%20make%20face%20fat/" class="article-date">
  <time class="dt-published" datetime="2023-05-15T09:39:35.622Z" itemprop="datePublished">2023-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="how-to-make-face-fat"><a href="#how-to-make-face-fat" class="headerlink" title="how to make face fat"></a>how to make face fat</h2><ol>
<li><h4 id="chatgpt"><a href="#chatgpt" class="headerlink" title="chatgpt"></a>chatgpt</h4></li>
</ol>
<ul>
<li>增加整体的体重：</li>
<li>多摄入卡路里：坚果，牛奶，鱼</li>
<li>涂东西在脸上：蜂蜜，油脂（买的涂脸的）</li>
<li>多睡觉（每天十点睡觉，加中午午睡）</li>
<li>多喝水（每天就喝tap water就行，但是要多喝水</li>
</ul>
<h4 id="2-google"><a href="#2-google" class="headerlink" title="2 .google"></a>2 .google</h4><p><a target="_blank" rel="noopener" href="https://www.medicalnewstoday.com/articles/326265">https://www.medicalnewstoday.com/articles/326265</a></p>
<ul>
<li>face exercise：怎么做</li>
<li></li>
</ul>
<p>conclusion</p>
<p><strong>吃高热量食物（坚果，牛奶，鱼），喝水，吃苹果</strong>：每天都坚持</p>
<p><strong>涂霜（洗脸毛巾）</strong>：每天都坚持</p>
<p><strong>睡觉</strong>： 晚上十点睡，中午午睡：每天都坚持</p>
<p><strong>做脸部运动</strong>：每天都坚持</p>
<p>​	</p>
<ul>
<li>抿起双唇并微笑时，抬起<a target="_blank" rel="noopener" href="https://www.healthline.com/health/pursed-lip-breathing">脸颊</a>肌肉。然后，将每只手的手指放在嘴的两侧，并通过将手指向上滑动到脸颊顶部来抬起脸颊。保持姿势 20 秒。</li>
<li>闭上你的嘴，让你的脸颊充满尽可能多的空气。保持该姿势 45 秒，然后慢慢将空气吹出。</li>
<li>将嘴巴张开成“O”形，嘴唇放在牙齿上，微笑着。然后将每只手的手指放在相应脸颊的顶部，轻轻抬起和放下您的脸颊，持续 30 秒。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/15/how%20to%20make%20face%20fat/" data-id="clhonv51j0000t9otabq155c4" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-道" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/10/%E9%81%93/" class="article-date">
  <time class="dt-published" datetime="2023-05-10T19:06:59.000Z" itemprop="datePublished">2023-05-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/10/%E9%81%93/">道</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li><p><strong>保持work的clean</strong></p>
<ul>
<li>做一些简单clean的东西</li>
<li>clean地进行保存，<ul>
<li>日后可以简便地找到和复用</li>
</ul>
</li>
<li>用最简单，clean的方式</li>
</ul>
</li>
<li><p><strong>保持输出</strong></p>
<ul>
<li>找到一种clean的way进行输出</li>
<li>否则很多时候等于没有存在过</li>
</ul>
</li>
<li><p><strong>好的东西想想怎么记录</strong></p>
<ul>
<li>放在自己电脑本地的私人博客，然后推到github私人仓库</li>
<li>最后从里面摘出公共的游记，然后我就可以把我的单反带着。</li>
<li>这样看上去比较clean,实际数据还需要一个大硬盘或者云来备份。</li>
</ul>
</li>
<li><p><strong>每天不同时刻Energy 状态不同，基于work对energy需求的多少进行schedule</strong></p>
</li>
<li><p><strong>在工作和科研中，只做必须做的，不做别人已经做过了的，不做可以付费请别人做的</strong></p>
</li>
<li><p><strong>戒断生活刺激源，</strong></p>
<ul>
<li>周末出门探索，</li>
<li>平时 街道晃悠， 网安，breaking ， piano ，游泳桑拿冥想</li>
</ul>
</li>
<li><p><strong>Goal</strong>：</p>
<ul>
<li><strong>成为格得一样的巫师</strong><ul>
<li>在柔克岛专心学艺，结交一生的良师和友人</li>
<li>航向内极海，历历万乡</li>
</ul>
</li>
<li><strong>remote job</strong>, <ul>
<li>回大理北京生活，陪爸妈江南小院子</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>金钱</strong></p>
<ul>
<li>该花的钱花了就是，不要过分计较，重要的是把该做的事情都做好</li>
</ul>
</li>
<li><p><strong>一个人有一个人的状态，和他人是另外的状态</strong></p>
<ul>
<li>一个人： be cool：网安，breaking, piano, 攀岩，探索，</li>
<li>两个人，感受友谊和关怀：<ul>
<li>尝试些一些新的东西，去一些一个人不敢尝试的店铺，路边的奇奇怪怪的店，</li>
<li>一些大海游泳，冥想，露营探险，一起看电影，听歌</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Relationship</strong></p>
<ul>
<li>人生伴侣</li>
<li>简单的互相感受快乐的关系</li>
<li>不要traditional互相担负的关系</li>
</ul>
</li>
<li><p><strong>感到内在很空的时候</strong>：获得内心peace</p>
<ul>
<li><p>吃好喝好睡好，有氧，获得好的精神状态</p>
</li>
<li><p>往罐子里塞东西，避免通过寻求刺激来获得正反馈</p>
</li>
<li><p>peace：深呼吸，带着垫子去海边冥想，瑜伽，海中浸泡，open mind, peace, 避免刺激</p>
</li>
<li><p>不能指望伴侣解决自己的空：伴侣只能是共享快乐，我不希望让对方负担。</p>
</li>
</ul>
</li>
<li><p>- </p>
</li>
<li><p><strong>我是谁，人生要干嘛</strong>，<strong>我不帅气，近视，脚还不好</strong></p>
<ul>
<li><strong>being a kid, a boy is enough for me</strong> : <strong>just being nice to others and smile :)</strong></li>
<li>你是酷酷仔</li>
<li>管他人生要干嘛，想那么多p</li>
<li>酷就完事了，怎么酷怎么来，怎么做自己怎么来，</li>
<li></li>
</ul>
</li>
<li><p><strong>相信并持续追寻，最终会得到想要的</strong></p>
</li>
<li><p>追寻的过程中：</p>
<ul>
<li><p><strong>减少决策到行动的延迟</strong></p>
<ul>
<li><p>先决定是a还是b</p>
</li>
<li><p>决定了就立刻去做</p>
</li>
</ul>
</li>
<li><p><strong>理解 “道”，运用“道”</strong></p>
<ul>
<li>很多时候事物只是在那个环境下的作用结果，了解人的动机，优化和决策</li>
<li>基于所理解的道，相信一切都是有可能的，屏蔽世俗看法，用内心去行动</li>
</ul>
</li>
</ul>
</li>
<li><p>给芳和rie写邮件share</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/10/%E9%81%93/" data-id="clhonv51o0003t9ot33516luq" data-title="道" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-dublin_remote_work" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/dublin_remote_work/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T13:51:33.114Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="dublin适合远程办公的地点合集"><a href="#dublin适合远程办公的地点合集" class="headerlink" title="dublin适合远程办公的地点合集"></a>dublin适合远程办公的地点合集</h1><p>咖啡店晒晒太阳什么的，不知道，没有那么想在家待着，也不一定要一直去实验室，找个地方待着</p>
<ol>
<li>google search： places good for study outside with sunshine in dublin<ol>
<li><a target="_blank" rel="noopener" href="https://www.fordublinlovers.com/en/best/places-to-study-outdoors-in-dublin">https://www.fordublinlovers.com/en/best/places-to-study-outdoors-in-dublin</a></li>
<li><a target="_blank" rel="noopener" href="https://livstudent.com/7-study-spots-in-dublin-to-check-out/">https://livstudent.com/7-study-spots-in-dublin-to-check-out/</a></li>
<li><a target="_blank" rel="noopener" href="https://lovindublin.com/best-of/10-best-places-to-work-or-study-in-when-you-find-yourself-in-dublin-city-centre">https://lovindublin.com/best-of/10-best-places-to-work-or-study-in-when-you-find-yourself-in-dublin-city-centre</a></li>
<li><a target="_blank" rel="noopener" href="https://universitytimes.ie/2018/04/five-of-the-best-alternative-study-spaces/">https://universitytimes.ie/2018/04/five-of-the-best-alternative-study-spaces/</a></li>
</ol>
</li>
<li>Chatgpt,</li>
</ol>
<p>need wifi:</p>
<ol>
<li>Dublin City Libraries: Many libraries in Dublin have free Wi-Fi and offer a quiet environment to work in. Check with your local library for opening hours and availability.</li>
<li>The Bernard Shaw: This pub in Dublin has free Wi-Fi and plenty of tables to work at. It also has a garden area where you can enjoy the sunshine while you work.</li>
<li>The Fumbally: This cafe has free Wi-Fi and plenty of tables to work at. It’s a popular spot for freelancers and students in Dublin.</li>
<li>The Digital Hub: This is a co-working space in Dublin that offers Wi-Fi and desk space for a fee. It’s a great option if you need a more professional environment to work in.</li>
<li>Starbucks: Many Starbucks locations in Dublin offer free Wi-Fi, and they are a popular spot for studying or working remotely.</li>
<li>Costa Coffee: Similar to Starbucks, many Costa Coffee locations in Dublin also offer free Wi-Fi.</li>
</ol>
<p>no need wifi:</p>
<ol>
<li><p>Bewley’s Cafe: This cafe has several locations in Dublin and is known for its cozy atmosphere. It’s a great spot to grab a coffee or tea and enjoy a warm drink while people-watching.</p>
</li>
<li><p>The Pepper Pot Cafe: This is a quaint cafe located inside the Powerscourt Centre in Dublin. It’s a great spot to sit and relax with a book or chat with friends while enjoying a warm drink and some delicious food.</p>
</li>
<li><p>The Bald Barista: This cafe is a popular spot in Dublin for its cozy atmosphere and great coffee. It’s a great place to relax and enjoy some good company.</p>
</li>
<li><p>Brother Hubbard: This is another great cafe in Dublin that offers a cozy atmosphere and delicious food. It’s a popular spot for brunch, but you can also visit during the day to relax and unwind.</p>
</li>
<li><p>The Cake Cafe: This is a charming cafe located in a converted redbrick building in Dublin. It’s known for its homemade cakes and friendly atmosphere, making it a great spot to sit and chat with friends.</p>
</li>
<li><p>小红书</p>
</li>
</ol>
<p>2区星巴克</p>
<p>central library</p>
<p>han sung旁边的mind the step</p>
<p>dun laoghaire图书馆</p>
<p>blackrock library</p>
<p>google search library cafe等等</p>
<p>tcd exam hall</p>
<ol start="4">
<li><p>自己想到的</p>
<p>市中心教堂旁边的library</p>
<p>tcd学校里面</p>
<p>家里</p>
</li>
<li><p>补充</p>
<p>怀旧茶馆（一排小桌子对着窗外</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/dublin_remote_work/" data-id="clhamvswr0000p1ot4zj91qgb" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-dublin可做的事情" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T12:06:22.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/">dublin可做的事情+ tcd设施调研</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="dublin-可做的事情"><a href="#dublin-可做的事情" class="headerlink" title="dublin 可做的事情"></a>dublin 可做的事情</h1><h1 id="爱尔兰可做的事情"><a href="#爱尔兰可做的事情" class="headerlink" title="爱尔兰可做的事情"></a>爱尔兰可做的事情</h1><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li>love dublin：<a target="_blank" rel="noopener" href="https://lovindublin.com/">https://lovindublin.com/</a></li>
<li>totally dublin <a target="_blank" rel="noopener" href="https://www.totallydublin.ie/">https://www.totallydublin.ie/</a></li>
<li>visitdublin  <a target="_blank" rel="noopener" href="https://www.visitdublin.com/">https://www.visitdublin.com/</a></li>
<li>meetup <a target="_blank" rel="noopener" href="https://www.meetup.com/home/?suggested=true&source=EVENTS">https://www.meetup.com/home/?suggested=true&amp;source=EVENTS</a></li>
</ul>
<p>	</p>
<h3 id="活动"><a href="#活动" class="headerlink" title="活动"></a>活动</h3><ul>
<li><p>游客线：viking + 啤酒厂 + arts 博物馆</p>
</li>
<li><p>喝啤酒： 老头bar </p>
</li>
<li><p>tcd: 攀岩( the wall, gravity , ) + 图书馆， 游泳馆，桑拿</p>
</li>
<li><p>dun laoghaire + howth看海</p>
</li>
<li><p>meetup+couchsurfing 找活动</p>
</li>
<li><p>一个探险露营，城市探索，坐火车去没去过的地方</p>
</li>
</ul>
<h3 id="地点"><a href="#地点" class="headerlink" title="地点"></a>地点</h3><p><strong>谷歌地球提前侦察+熟人询问</strong></p>
<ul>
<li><p>公园： 凤凰公园 + herbert 公园</p>
</li>
<li><p>山： dublin山 + 威克洛山</p>
</li>
<li><p>海滩：bray + howth</p>
</li>
<li><p>向北： sutton ,malahide, greystones, bray 小丘</p>
</li>
</ul>
<h3 id="组织"><a href="#组织" class="headerlink" title="组织"></a>组织</h3><ul>
<li>教堂：church city center</li>
<li>学校：飞盘社团+棒球社团+meetup社团</li>
<li>学校： chinese group</li>
</ul>
<h3 id="旅行"><a href="#旅行" class="headerlink" title="旅行"></a>旅行</h3><ul>
<li>奥地利</li>
<li>挪威</li>
<li>冰岛</li>
<li>法国南部村庄</li>
<li>英国德国无感，直接北欧，法国南部，不着急，土耳其surgery</li>
</ul>
<h1 id="Tcd-facility-research"><a href="#Tcd-facility-research" class="headerlink" title="Tcd facility research"></a>Tcd facility research</h1><h3 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li>tcd  <a target="_blank" rel="noopener" href="https://www.tcd.ie/">https://www.tcd.ie/</a></li>
<li>library <a target="_blank" rel="noopener" href="https://www.tcd.ie/library/">https://www.tcd.ie/library/</a></li>
<li>clubs and Societies <a target="_blank" rel="noopener" href="https://www.tcd.ie/students/clubs-societies/index.php">https://www.tcd.ie/students/clubs-societies/index.php</a></li>
<li>living in dublin <a target="_blank" rel="noopener" href="https://www.tcd.ie/students/living-dublin/">https://www.tcd.ie/students/living-dublin/</a></li>
</ul>
<h3 id="sports-club"><a href="#sports-club" class="headerlink" title="sports club"></a>sports club</h3><p>飞盘 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/ultimate-frisbee.php">https://www.tcd.ie/Sport/student-sport/clubs/ultimate-frisbee.php</a></p>
<p>攀岩 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/climbing.php">https://www.tcd.ie/Sport/student-sport/clubs/climbing.php</a></p>
<p>帆船 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/wind-wake.php">https://www.tcd.ie/Sport/student-sport/clubs/wind-wake.php</a></p>
<p>板球 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/mens-cricket.php">https://www.tcd.ie/Sport/student-sport/clubs/mens-cricket.php</a></p>
<p>篮球 <a target="_blank" rel="noopener" href="https://www.tcd.ie/Sport/student-sport/clubs/basketball.php">https://www.tcd.ie/Sport/student-sport/clubs/basketball.php</a></p>
<h3 id="Societies"><a href="#Societies" class="headerlink" title="Societies"></a>Societies</h3><ul>
<li>教堂唱诗班 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=18">https://trinitysocieties.ie/society/?socid=18</a></li>
<li>合唱团 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=137">https://trinitysocieties.ie/society/?socid=137</a></li>
<li>计算机协会 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=151">https://trinitysocieties.ie/society/?socid=151</a></li>
<li>跳舞 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=29">https://trinitysocieties.ie/society/?socid=29</a></li>
<li>数字艺术 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=30">https://trinitysocieties.ie/society/?socid=30</a></li>
<li>ted <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=135">https://trinitysocieties.ie/society/?socid=135</a></li>
<li>心理学会 <a target="_blank" rel="noopener" href="https://trinitysocieties.ie/society/?socid=90">https://trinitysocieties.ie/society/?socid=90</a></li>
</ul>
<h3 id="校园设施"><a href="#校园设施" class="headerlink" title="校园设施"></a>校园设施</h3><ul>
<li>高性能计算： TCHPC <a target="_blank" rel="noopener" href="https://www.tchpc.tcd.ie/gettingstarted">https://www.tchpc.tcd.ie/gettingstarted</a></li>
<li>文具： stationery</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/dublin%E5%8F%AF%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/" data-id="clhaorwbj0000dvotbmgu1cuz" data-title="dublin可做的事情+ tcd设施调研" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/" rel="tag">life</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-都柏林租房攻略-狠人版" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/">都柏林租房攻略</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="都柏林租房攻略-狠人版"><a href="#都柏林租房攻略-狠人版" class="headerlink" title="都柏林租房攻略-狠人版"></a>都柏林租房攻略-狠人版</h1><ol>
<li><p>首先选区</p>
<p>参考这篇文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/112493998">https://zhuanlan.zhihu.com/p/112493998</a></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/mapa-dublin-660x479.png" alt="img"></p>
</li>
</ol>
<p>理想居住区：2 4 6 6w 13 14 16 18</p>
<p>还行的居住区：1 3 5 7 12 15 17 20</p>
<p>从2 4 6 6w 13 14 16 18 中排除掉交通时间过远的</p>
<ul>
<li>打开hosting power <a target="_blank" rel="noopener" href="https://hostingpower.ie/">https://hostingpower.ie/</a></li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230421113427078.png" alt="image-20230421113427078"></p>
<ul>
<li>选择area, 输入刚刚筛选出的区域结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230421113539429.png" alt="image-20230421113539429"></p>
<ul>
<li><p>选择max rent 点击apply</p>
</li>
<li><p>得到一系列搜索结果后，选择自己喜欢的房子</p>
</li>
<li><p>点进去根据界面提供的关键信息google search 房东的联系方式</p>
<ul>
<li>name</li>
<li>location</li>
<li>and so on</li>
<li>检索示范：”name” + ”location“ + site: airbnb.com</li>
</ul>
</li>
<li><p>自行联系房东签订合同等，可以省去hostingpower中介费890欧</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/%E9%83%BD%E6%9F%8F%E6%9E%97%E7%A7%9F%E6%88%BF%E6%94%BB%E7%95%A5-%E7%8B%A0%E4%BA%BA%E7%89%88/" data-id="clgw24ioo0003yc9f3oa462wn" data-title="都柏林租房攻略" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/" rel="tag">life</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-rl_envs_and_modeling" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/rl_envs_and_modeling/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/rl_envs_and_modeling/">rl_play</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="play-with-gym-env"><a href="#play-with-gym-env" class="headerlink" title="play with gym env"></a>play with gym env</h1><h2 id="cart-pole"><a href="#cart-pole" class="headerlink" title="cart pole"></a>cart pole</h2><p>control + pid: <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44562141/article/details/119700574">https://blog.csdn.net/weixin_44562141/article/details/119700574</a></p>
<p>document of gym <a target="_blank" rel="noopener" href="https://www.gymlibrary.dev/environments/classic_control/cart_pole/">https://www.gymlibrary.dev/environments/classic_control/cart_pole/</a></p>
<p>driving test <a target="_blank" rel="noopener" href="http://www.theory-tester.com/questions/358">http://www.theory-tester.com/questions/358</a></p>
<ul>
<li>state space:</li>
</ul>
<p>position</p>
<p>velocity</p>
<p>angle</p>
<p>angular velocity</p>
<ul>
<li><p>first understand problem, then understand reinforcement learning, u must understand the env, then you know why their study is like that, try to be a good teacher</p>
</li>
<li><p>for spare time, can play , for working time, only do things creating value to this project.</p>
<ul>
<li>first look for mappo implementation, then try doing it by myself</li>
</ul>
</li>
<li><p>if we want to control it with pid,</p>
</li>
<li><p>in this env, u are just study “action 要和夹角反着”+ 夹角和几个输入数据的关系，一部分是先验只是可以给的，所以我先原始地学习一下，再把state加工一下加进去，再试试一下把控制的东西加进去，对，我得先有想法，再实验，再读文献，再自己思考，再实验。我希望按照自己的想法来，这样我会沉迷于探索。我希望一直自己保有一些探索的时间，最后发现科研的乐趣。</p>
</li>
</ul>
<h2 id="frozen-lake"><a href="#frozen-lake" class="headerlink" title="frozen lake"></a>frozen lake</h2><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb#scrollTo=Y1tWn0tycWZ1">https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb#scrollTo=Y1tWn0tycWZ1</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qtable = np.zeros(state_space, action_space)</span><br><span class="line">action = argmax(qtable[state][:])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training parameters</span></span><br><span class="line">n_training_episodes = <span class="number">10000</span>  <span class="comment"># Total training episodes</span></span><br><span class="line">learning_rate = <span class="number">0.7</span>          <span class="comment"># Learning rate(这个不是梯度下降的learning rate,是td error的learning rate </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation parameters</span></span><br><span class="line">n_eval_episodes = <span class="number">100</span>        <span class="comment"># Total number of test episodes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">env_id = <span class="string">&quot;FrozenLake-v1&quot;</span>     <span class="comment"># Name of the environment</span></span><br><span class="line">max_steps = <span class="number">99</span>               <span class="comment"># Max steps per episode（防止回合死循环）</span></span><br><span class="line">gamma = <span class="number">0.95</span>                 <span class="comment"># Discounting rate （value的discounting）</span></span><br><span class="line">eval_seed = []               <span class="comment"># The evaluation seed of the environment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exploration parameters （刚开始探索大，后来探索小）</span></span><br><span class="line">max_epsilon = <span class="number">1.0</span>             <span class="comment"># Exploration probability at start</span></span><br><span class="line">min_epsilon = <span class="number">0.05</span>            <span class="comment"># Minimum exploration probability </span></span><br><span class="line">decay_rate = <span class="number">0.0005</span>            <span class="comment"># Exponential decay rate for exploration prob</span></span><br></pre></td></tr></table></figure>





<h2 id="lunarLand"><a href="#lunarLand" class="headerlink" title="lunarLand"></a>lunarLand</h2><p>安装</p>
<p>pip install box2d-py</p>
<h2 id="taxi"><a href="#taxi" class="headerlink" title="taxi"></a>taxi</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit3/introduction?fw=pt">https://huggingface.co/learn/deep-rl-course/unit3/introduction?fw=pt</a></p>
<h2 id="github-ball-game"><a href="#github-ball-game" class="headerlink" title="github ball game"></a>github ball game</h2><h2 id="hugging-face-tutorial"><a href="#hugging-face-tutorial" class="headerlink" title="hugging face tutorial"></a>hugging face tutorial</h2><h1 id="incoporate-mappo-with-sumo"><a href="#incoporate-mappo-with-sumo" class="headerlink" title="incoporate  mappo with sumo"></a>incoporate  mappo with sumo</h1><h2 id="1-look-for-existing-mappo-sumo"><a href="#1-look-for-existing-mappo-sumo" class="headerlink" title="1. look for existing mappo + sumo"></a>1. look for existing mappo + sumo</h2><ul>
<li>reference</li>
</ul>
<ol>
<li>github</li>
<li>paper</li>
<li>resource of course era, presentation, tutorial (osint)</li>
</ol>
<ul>
<li>resources found</li>
</ul>
<p>github</p>
<ol>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control">https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control</a></p>
</li>
<li><p>light mappo  <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/light_mappo">https://github.com/magiclucky1996/light_mappo</a> 基于这个写一下试试</p>
</li>
<li><p>q&#x2F;ac + sumo <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/deeprl_signal_control">https://github.com/magiclucky1996/deeprl_signal_control</a></p>
</li>
<li><p>mappo + mujoco <a target="_blank" rel="noopener" href="https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation">https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation</a></p>
</li>
<li><p>mappo&#x2F;qmix&#x2F;maddpg + mpe <a target="_blank" rel="noopener" href="https://github.com/Lizhi-sjtu/MARL-code-pytorch">https://github.com/Lizhi-sjtu/MARL-code-pytorch</a></p>
</li>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/YanivHacker/RLTrafficManager">https://github.com/YanivHacker/RLTrafficManager</a></p>
</li>
<li><p>noisy mappo+  <a target="_blank" rel="noopener" href="https://github.com/hijkzzz/noisy-mappo">https://github.com/hijkzzz/noisy-mappo</a></p>
</li>
</ol>
<p>papers</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors">https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors</a> 东北信息学院：提到了rl奖励稀疏的问题，然后他们给rl设计更多的reward 引导它学习。但是你怎么知道什么样的reward能够引导，这不是还是在reward function设计的范围里吗？</p>
<ul>
<li>insights<ol>
<li>sumo设计的问题，如果我们设一个控制周期之后的交通流状态为reward是不是不合理，怎么样去评价智能体schedule的好坏呢，怎么去评价智能体的action改善了交通呢，我是要搞交通呢，还是要搞rl呢，还是要搞啥，</li>
<li>上次会议的要点：1. insight可以给硕士做，但是要具体可行 2. 做一个oncoming 会议的scheduling(rl 多智能体 交通 计算机 人工智能) 3. sumo的模型可以封装好给本科生用 4.</li>
</ol>
</li>
<li>今天的计划（4.24）</li>
</ul>
<p>work 到12 点半，吃中饭，吃完中饭一点消化一会儿，回实验室一点半，回来继续work,work到两点多的时候睡午觉，五点跑路，去看看有没有吃的，不行就回家supervalu,晚上回家继续work一会儿，今天的弄完走之前deploy 和 push上去</p>
<ul>
<li>how other people implement marl training</li>
</ul>
<p>make contraction between these projects</p>
<p>分解成detailed steps</p>
<p>1.首先看下mappo的代码和deeprl sumo的代码以及ppo sumo代码（只做有必要做的事情）</p>
<ol start="2">
<li><p>做完1大概知道要干嘛，可以看看ppo trpo mappo</p>
</li>
<li><p>把每周4上午留作整理时间，所以我必须两天内搞定这个代码整定的事情，然后再用剩下的时间学习ppo trpo mappo，还要学sumo部分的东西，但是得非常快</p>
</li>
<li></li>
<li></li>
</ol>
<p>生活的star：</p>
<p>piano</p>
<p>上海美术厂</p>
<p>breaking</p>
<p>penetration testing</p>
<p>avoid social media</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/rl_envs_and_modeling/" data-id="clham94tj0000h1ot9idp3nf2" data-title="rl_play" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-week 6 share" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/week%206%20share/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/week%206%20share/">week6</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="week-6-amp-7-share"><a href="#week-6-amp-7-share" class="headerlink" title="week 6&amp;7 share"></a>week 6&amp;7 share</h1><h3 id="Algorithms-single-agent-单智能体算法总结"><a href="#Algorithms-single-agent-单智能体算法总结" class="headerlink" title="Algorithms-single agent(单智能体算法总结)"></a>Algorithms-single agent(单智能体算法总结)</h3><ul>
<li><strong>算法1： DQN</strong><ul>
<li>valued based (unstable because the update of strategy is not smooth)</li>
<li>poorly understood</li>
<li>rainbow is the best implementation version of DQN (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a>)</li>
</ul>
</li>
</ul>
<p><img src="https://www.researchgate.net/publication/333197086/figure/fig11/AS:941946201727001@1601588883526/Pseudo-code-of-DQN-with-experience-replay-method-12.png" alt="Pseudo-code of DQN with experience-replay method [12]"></p>
<ul>
<li><strong>算法2： REINFORCE</strong></li>
</ul>
<p><img src="https://i.stack.imgur.com/8Jn8l.png" alt="reinforcement learning - Why the $\gamma^t$ is needed here in REINFORCE:  Monte-Carlo Policy-Gradient Control (episodic) for $\pi_{*}$? - Cross  Validated"></p>
<ul>
<li><strong>算法3： VPG</strong><ul>
<li>poor data efficiency</li>
<li>poor robustness</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/262538f3077a7be8ce89066abbab523575132996.svg" alt="Vanilla Policy Gradient — Spinning Up documentation"></p>
<ul>
<li><strong>算法4： DDPG</strong><ul>
<li>sampled from replay buffer</li>
<li>good data sampling efficiency (why , just because of reuse of data in buffer)</li>
<li>what does the deterministic refer to?</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg" alt="Deep Deterministic Policy Gradient — Spinning Up documentation"></p>
<ul>
<li><strong>算法5： SAC</strong><ul>
<li>max entropy version of ddpg</li>
<li>entropy help to explore</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/c01f4994ae4aacf299a6b3ceceedfe0a14d4b874.svg" alt="Soft Actor-Critic — Spinning Up documentation"></p>
<ul>
<li><strong>算法6： TRPO</strong><ul>
<li>not compatible with frame including noise or parameter sharing (ppo paper)</li>
<li>a little bit complicated</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/5808864ea60ebc3702704717d9f4c3773c90540d.svg" alt="Trust Region Policy Optimization — Spinning Up documentation"></p>
<ul>
<li><p><strong>算法7 PPO</strong></p>
<ul>
<li><p>data efficiency is not as good as ddpg: why?</p>
</li>
<li><p>collect trajectory set, improve value function to be close to utility with multiple gradient descent</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg" alt="Proximal Policy Optimization — Spinning Up documentation"></p>
<h4 id="chart-for-comparison对比表格"><a href="#chart-for-comparison对比表格" class="headerlink" title="chart for comparison对比表格"></a>chart for comparison对比表格</h4><table>
<thead>
<tr>
<th></th>
<th>value and policy</th>
<th>improve method</th>
<th>sampling</th>
<th>update of model</th>
<th>facts</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>value: <strong>Q network</strong><br />policy: <strong>argmax Q</strong><br />sample: <strong>epsilon_greedy</strong></td>
<td><strong>Bootstrap</strong>: receive r, improve Q</td>
<td>sample along <strong>trajectory</strong> with <strong>epsilon_greedy</strong></td>
<td>one step interact + experience replay</td>
<td>off-policy(replay buffer)<br />discrete action</td>
</tr>
<tr>
<td>DDPG</td>
<td>value: <strong>Q network</strong><br />policy: **actor ** <br />sample: <strong>actor</strong></td>
<td><strong>Bootstrap</strong>: receive r, update Q network , imporve</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>one step interact + experience replay</td>
<td>off-policy;(replay buffer)<br />continuous action space<br /></td>
</tr>
<tr>
<td>SAC</td>
<td>value: <strong>Q network</strong><br />policy: <strong>actor</strong><br />sample: <strong>actor</strong></td>
<td><strong>Boorstrap</strong>: receive r, update Q network, improve <strong>actor</strong></td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>one step interact+experience replay</td>
<td>off-policy(buffer)<br />continuous or discrete action space</td>
</tr>
<tr>
<td>VPG</td>
<td>value: <strong>Q network</strong><br />policy: <strong>actor</strong><br />sample:<strong>actor</strong></td>
<td><strong>MC</strong>: collect set of traj, improve <strong>Actor</strong> with PG, then improve <strong>value</strong> network</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>interact for trajs+ update <strong>actor</strong> with PG(advantage by <strong>critic</strong>); update <strong>critic</strong> with MC</td>
<td><strong>on-policy</strong>;<br />discrete+continuous action(policy network output could be action)<br /><br />easy to trapped in local optima</td>
</tr>
<tr>
<td>TRPO</td>
<td>value: <strong>MC</strong><br />policy: <strong>actor</strong><br />sample: <strong>actor</strong></td>
<td><strong>MC</strong>: collect set of traj, improve <strong>Actor</strong> with PG, then improve <strong>value</strong> network</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>interact for trajs+ update <strong>actor</strong> with PG(advantage by <strong>critic</strong>); update <strong>critic</strong> with MC</td>
<td>on policy</td>
</tr>
<tr>
<td>PPO</td>
<td>value: <strong>MC</strong><br />policy: <strong>actor</strong><br />sample: <strong>actor</strong></td>
<td><strong>MC</strong>: collect set of traj, imporve <strong>actor</strong> with PG, then improve <strong>value</strong> network</td>
<td>sample along trajectory with <strong>actor</strong></td>
<td>interact for trajs+ update <strong>actor</strong> with PG(advantage by <strong>critic</strong>); update <strong>critic</strong> with MC</td>
<td>on policy</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="analysis-for-rl-frame-rl框架的分析"><a href="#analysis-for-rl-frame-rl框架的分析" class="headerlink" title="analysis for rl frame(rl框架的分析)"></a>analysis for rl frame(rl框架的分析)</h3><p><strong>Bootstrap</strong>: collect transition , store in the replay buffer, update model every state in the env, <strong>(DQN, DDPG, SAC)</strong>, it’s usually <strong>off-policy, good data efficiency</strong>(data in buffer could be reused), the update of model is more frequent( every step in the env)</p>
<p><strong>MC</strong>: Collect trajs with current policy, improve <strong>actor</strong> with utility through PG, then improve value with utility <strong>(VPG\ TRPO\PPO)</strong>, it’s usually <strong>on-policy</strong>, <strong>data efficiency</strong> is worse than <strong>ddpg and sac</strong>, update of model is not that frequent, </p>
<p><strong>solving iid problem</strong>: </p>
<ul>
<li>replay buffer: off-policy method, applied for <strong>bootstrap</strong>.</li>
</ul>
<p><strong>solving explore problem</strong>: </p>
<ul>
<li>epsilon-greedy: applied for value based rl </li>
<li>max entropy: applied for policy based rl</li>
</ul>
<h3 id="Basic-frame（rl的基础框架）"><a href="#Basic-frame（rl的基础框架）" class="headerlink" title="Basic frame（rl的基础框架）"></a>Basic frame（rl的基础框架）</h3><p><strong>Value based</strong> vs <strong>Policy gradient</strong> </p>
<p>the policy of DQN is <strong>argmax Q</strong>, which is <strong>discrete</strong></p>
<p>the policy of PG is <strong>actor network</strong>, which is <strong>continuous</strong></p>
<p><strong>Result</strong>：</p>
<p>Policy gradient <strong>lose</strong> at sample efficiency:  DQN is more sample efficient</p>
<p>policy gradient <strong>lose</strong> at stability: gradient of policy could be noisy and high-variance</p>
<p>Policy gradient <strong>win</strong> at smooth update: model update of PG is more smooth</p>
<p>policy gradient <strong>win</strong> at complex and continuous problem: env with non-differentiable reward functions and continuous action space</p>
<p><strong>Policy gradient</strong> VS <strong>Actor critic</strong>:</p>
<p>the policy update of <strong>PG</strong> is through <strong>bootstrap</strong>: which learn faster </p>
<p>the policy update of <strong>AC</strong> is through <strong>MC</strong>, which  learn slower</p>
<p><strong>Result</strong>：</p>
<p>Actor critic <strong>win</strong> at sample efficiency</p>
<h3 id="Comparasion（rl算法的对比）"><a href="#Comparasion（rl算法的对比）" class="headerlink" title="Comparasion（rl算法的对比）"></a>Comparasion（rl算法的对比）</h3><table>
<thead>
<tr>
<th>Algorithm</th>
<th>Reward</th>
<th>Convergence Speed</th>
<th>Sample Efficiency</th>
<th>Robustness</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>9</td>
<td>8</td>
<td>9</td>
<td>7</td>
</tr>
<tr>
<td>A3C</td>
<td>8</td>
<td>9</td>
<td>6</td>
<td>8</td>
</tr>
<tr>
<td>PPO</td>
<td>7</td>
<td>7</td>
<td>8</td>
<td>6</td>
</tr>
<tr>
<td>TRPO</td>
<td>7</td>
<td>6</td>
<td>7</td>
<td>9</td>
</tr>
<tr>
<td>AC</td>
<td>6</td>
<td>6</td>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>VPG</td>
<td>7</td>
<td>5</td>
<td>7</td>
<td>7</td>
</tr>
<tr>
<td>DDPG</td>
<td>8</td>
<td>7</td>
<td>9</td>
<td>8</td>
</tr>
<tr>
<td>SAC</td>
<td>9</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
</tbody></table>
<ul>
<li>comparison between ppo and sac</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504102441608.png" alt="image-20230504102441608"></p>
<ul>
<li><p>comparison between ppo and sac</p>
</li>
<li><p>DQN</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094736482.png" alt="image-20230504094736482"></p>
<ul>
<li>A3C</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094802502.png" alt="image-20230504094802502"></p>
<ul>
<li>ppo</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094818441.png" alt="image-20230504094818441"></p>
<ul>
<li>trpo</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094833674.png" alt="image-20230504094833674"></p>
<ul>
<li>AC</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504094856363.png" alt="image-20230504094856363"></p>
<ul>
<li>vpg</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504095430731.png" alt="image-20230504095430731"></p>
<ul>
<li>DDPG</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504095442568.png" alt="image-20230504095442568"></p>
<ul>
<li>sac</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504095454177.png" alt="image-20230504095454177"></p>
<h2 id="Questions（问题）"><a href="#Questions（问题）" class="headerlink" title="Questions（问题）"></a>Questions（问题）</h2><p><em><strong>Q1: why cannot replay buffer be applied to ppo? why max entropy cannot be applied to ppo?</strong></em></p>
<ol>
<li>the policy optimization of ppo rely on current policy, ppo use  a rolling buffer to store the most recent trajectories and samples them</li>
<li>policy update objective in ppo already includes an entropy term</li>
</ol>
<p><em><strong>Q2: what is the influence that whether the reward function is differentiable or not in reinforcement learning?</strong></em></p>
<p>gradient-based optimization techniques are needed to update the policy and value function based on the observed rewards and states.</p>
<p><em><strong>Q3:in neural network ,how much does the update of model influence the prediction of next state, if it matters , maybe we just prefer the most important state?</strong></em></p>
<p>*<strong>Q4. when update Q,  will it be more stable that the action of next Q is chosen based on the probability distribution of action?*</strong></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230504112741947.png" alt="image-20230504112741947"></p>
<p><em><strong>Q5 we use epsilon decay or entropy to explore the env, so what if we  mark the action we choose in a table, and next time choose the action we haven’t choose?</strong></em></p>
<p><em><strong>Q6. professor Vinny said MC is not same as sample a  set of trajectories</strong></em></p>
<h2 id="Insights（想法）"><a href="#Insights（想法）" class="headerlink" title="Insights（想法）"></a>Insights（想法）</h2><ul>
<li>maybe can view sample efficiency, robustness  and convergence from the perspective of machine learning: we want to learn faster and learn from useful information), then maybe absorb some experience from existing papers on gradient descent and machine learning.</li>
</ul>
<h1 id="Algorithms-multi-agent（多智能体算法）"><a href="#Algorithms-multi-agent（多智能体算法）" class="headerlink" title="Algorithms-multi agent（多智能体算法）"></a>Algorithms-multi agent（多智能体算法）</h1><p><a target="_blank" rel="noopener" href="https://jianzhnie.github.io/machine-learning-wiki/#/deep-rl/papers/Overview">https://jianzhnie.github.io/machine-learning-wiki/#/deep-rl/papers/Overview</a></p>
<p>centralized critic network</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/MARL_cooperation_algo.png" alt="../_images/MARL_cooperation_algo.png"></p>
<p>Valued-based MARL</p>
<p>roma</p>
<p>Qmix</p>
<p>Actor-Critic MARL</p>
<p>maac</p>
<p>coma</p>
<p>maddpg</p>
<p>mappo</p>
<h1 id="Upcoming-conferences（会议总结）"><a href="#Upcoming-conferences（会议总结）" class="headerlink" title="Upcoming conferences（会议总结）"></a>Upcoming conferences（会议总结）</h1><p><em><strong>later do a complete notion chart: 之后做一个完整的notion表格</strong></em></p>
<ul>
<li>machine learning</li>
</ul>
<ol>
<li>Conference on Neural Information Processing Systems (NeurIPS) (A category) <a target="_blank" rel="noopener" href="https://nips.cc/Conferences/2023/CallForPapers">https://nips.cc/Conferences/2023/CallForPapers</a></li>
<li>International Conference on Machine Learning (ICML) (A category)  <a target="_blank" rel="noopener" href="https://icml.cc/">https://icml.cc/</a></li>
<li>Conference on Robot Learning (CoRL)  <a target="_blank" rel="noopener" href="https://www.corl2023.org/">https://www.corl2023.org/</a></li>
<li>IEEE Intelligent Transportation Systems Conference (ITSC) <a target="_blank" rel="noopener" href="https://ieee-itss.org/event/itsc2023/">https://ieee-itss.org/event/itsc2023/</a></li>
<li>Transportation Research Board Annual Meeting (TRB)  <a target="_blank" rel="noopener" href="https://www.trb.org/AnnualMeeting/AnnualMeeting.aspx">https://www.trb.org/AnnualMeeting/AnnualMeeting.aspx</a></li>
<li>International Joint Conference on Artificial Intelligence (IJCAI) (A category)  <a target="_blank" rel="noopener" href="https://www.ijcai.org/">https://www.ijcai.org/</a>  IJCAI-PRICAI-24: shanghai: out of date for 2023</li>
<li>European Conference on Artificial Intelligence (ECAI) <a target="_blank" rel="noopener" href="https://ecai2023.eu/ECAI2023">https://ecai2023.eu/ECAI2023</a></li>
<li>International Conference on Automated Planning and Scheduling (ICAPS) <a target="_blank" rel="noopener" href="https://icaps23.icaps-conference.org/">https://icaps23.icaps-conference.org/</a> : out of date for 2023</li>
<li>International Conference on Learning Representations (ICLR)<a target="_blank" rel="noopener" href="https://iclr.cc/">https://iclr.cc/</a> out of date for 2023</li>
<li>International Conference on Robotics and Automation (ICRA) <a target="_blank" rel="noopener" href="https://www.icra2023.org/">https://www.icra2023.org/</a> out of date for 2023</li>
<li>International Symposium on Transportation and Traffic Theory (ISTTT) <a target="_blank" rel="noopener" href="https://limos.engin.umich.edu/isttt25/">https://limos.engin.umich.edu/isttt25/</a></li>
<li>IEEE Conference on Decision and Control (CDC) <a target="_blank" rel="noopener" href="https://cdc2023.ieeecss.org/">https://cdc2023.ieeecss.org/</a> out of date for 2023</li>
<li>IEEE International Conference on Intelligent Transportation Systems (ITSC) <a target="_blank" rel="noopener" href="https://ieee-itss.org/event/itsc2023/">https://ieee-itss.org/event/itsc2023/</a>  <a target="_blank" rel="noopener" href="https://2023.ieee-itsc.org/">https://2023.ieee-itsc.org/</a> out of date for 2023</li>
<li>International Conference on Control, Automation and Information Sciences (ICCAIS) <a target="_blank" rel="noopener" href="http://iccais2023.org/">http://iccais2023.org/</a> </li>
<li>International Conference on Control, Automation, Robotics and Vision (ICARCV) <a target="_blank" rel="noopener" href="https://www.intelligentautomation.network/events-intelligent-automation/agenda-mc?utm_campaign=27031.007_BLUE_GPPC&extTreatId=7576989&gclid=Cj0KCQjw6cKiBhD5ARIsAKXUdyY_SSzgGhuf4T7L6NxsscqfgI6HypsBEBtUoK1KGE28nwelmmOX-oIaAvveEALw_wcB">https://www.intelligentautomation.network/events-intelligent-automation/agenda-mc?utm_campaign=27031.007_BLUE_GPPC&amp;extTreatId=7576989&amp;gclid=Cj0KCQjw6cKiBhD5ARIsAKXUdyY_SSzgGhuf4T7L6NxsscqfgI6HypsBEBtUoK1KGE28nwelmmOX-oIaAvveEALw_wcB</a></li>
</ol>
<ul>
<li>traffic</li>
</ul>
<ol>
<li>World Conference on Transport Research Society (WCTRS) <a target="_blank" rel="noopener" href="http://wctr2023.ca/">http://wctr2023.ca/</a></li>
<li>International Association of Traffic and Safety Sciences (IATSS)</li>
<li>IEEE Intelligent Vehicles Symposium (IV) <a target="_blank" rel="noopener" href="https://2023.ieee-iv.org/">https://2023.ieee-iv.org/</a></li>
<li>Transportation Science and Logistics Society (TSL)</li>
<li>International Conference on Transport and Health (ICTH)</li>
<li>International Symposium on Transportation Network Reliability (INSTR) <a target="_blank" rel="noopener" href="https://easychair.org/cfp/instr2023">https://easychair.org/cfp/instr2023</a></li>
<li>IEEE International Conference on Intelligent Transportation Systems (ITSC)</li>
<li>European Transport Conference (ETC) <a target="_blank" rel="noopener" href="https://aetransport.org/etc">https://aetransport.org/etc</a></li>
<li>International Conference on Traffic and Transport Psychology (ICTTP) ICTTP 8 2024 Tel Aviv, Israel.</li>
<li>ITS World Congress <a target="_blank" rel="noopener" href="https://itsworldcongress.com/">https://itsworldcongress.com/</a> 2024 dubai</li>
</ol>
<ul>
<li>smart city</li>
</ul>
<ol>
<li>IEEE International Smart Cities Conference (ISC2)</li>
<li>ACM International Conference on Ubiquitous Computing and Communications (UbiComp)</li>
<li>International Conference on Smart Cities and Green ICT Systems (SMARTGREENS)</li>
<li>Smart Cities Symposium Prague (SCSP)</li>
<li>IEEE International Conference on Smart City Innovations (SCI)</li>
<li>International Workshop on Smart Cities and Urban Analytics (UrbanGIS)</li>
<li>International Conference on Smart Data and Smart Cities (SDSC)</li>
<li>Smart City Symposium (SCS)</li>
<li>International Conference on Sustainable Smart Cities and Territories (SSCt)</li>
<li>European Conference on Smart Objects, Systems and Technologies (Smart SysTech)</li>
</ol>
<ul>
<li>intelligent connected vehicle</li>
</ul>
<ol>
<li>International Conference on Connected Vehicles and Expo (ICCVE)</li>
<li>IEEE Vehicular Technology Conference (VTC)</li>
<li>International Conference on Vehicle Technology and Intelligent Transport Systems (VEHITS)</li>
<li>IEEE Conference on Control Technology and Applications (CCTA)</li>
<li>International Conference on Vehicle Engineering and Intelligent Transportation Systems (VEITS)</li>
<li>IEEE International Conference on Connected and Autonomous Vehicles (ICCAV)</li>
</ol>
<h4 id="May"><a href="#May" class="headerlink" title="May"></a>May</h4><p><strong>May 8</strong></p>
<p>ECAI 2023 <a target="_blank" rel="noopener" href="https://ecai2023.eu/ECAI2023">https://ecai2023.eu/ECAI2023</a></p>
<p><strong>May 11</strong></p>
<p>NIPS 2023  <a target="_blank" rel="noopener" href="https://nips.cc/Conferences/2023/CallForPapers">https://nips.cc/Conferences/2023/CallForPapers</a></p>
<p><strong>May 15</strong></p>
<p>ITSC 2023 <a target="_blank" rel="noopener" href="https://2023.ieee-itsc.org/">https://2023.ieee-itsc.org/</a></p>
<h4 id="June"><a href="#June" class="headerlink" title="June"></a>June</h4><p><strong>June  8</strong> </p>
<p>CoRL 2023 <a target="_blank" rel="noopener" href="https://www.corl2023.org/">https://www.corl2023.org/</a></p>
<h4 id="July"><a href="#July" class="headerlink" title="July"></a>July</h4><p><strong>July 15</strong></p>
<p>ICCAIS 2023 <a target="_blank" rel="noopener" href="http://iccais2023.org/">http://iccais2023.org/</a></p>
<h4 id="August"><a href="#August" class="headerlink" title="August"></a>August</h4><p><strong>August 1</strong></p>
<p>TRB 2024 <a target="_blank" rel="noopener" href="https://trb.secure-platform.com/a/page/TRBPaperReview#Instructions">https://trb.secure-platform.com/a/page/TRBPaperReview#Instructions</a></p>
<h4 id="Sepetmber"><a href="#Sepetmber" class="headerlink" title="Sepetmber"></a>Sepetmber</h4><h4 id="October"><a href="#October" class="headerlink" title="October"></a>October</h4><h4 id="November"><a href="#November" class="headerlink" title="November"></a>November</h4><h4 id="December"><a href="#December" class="headerlink" title="December"></a>December</h4><h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><ol>
<li><h4 id="Trpo-PPO-MAPPO"><a href="#Trpo-PPO-MAPPO" class="headerlink" title="Trpo PPO MAPPO"></a>Trpo PPO MAPPO</h4></li>
</ol>
<p>1.1 Trpo</p>
<p><strong>(1). Approximation</strong></p>
<ul>
<li>the cost function</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426101337008.png" alt="image-20230426101337008"></p>
<ul>
<li>the V</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426101210597.png" alt="image-20230426101210597"></p>
<ul>
<li>so it could be approximated by</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426105145352.png" alt="image-20230426105145352"></p>
<ul>
<li><p>therefore J could be transferred to </p>
<p><img src="/home/sky/.config/Typora/typora-user-images/image-20230426101435439.png" alt="image-20230426101435439"></p>
</li>
<li><p>try to find the <strong>theta</strong> which could maximize the <strong>J</strong></p>
</li>
<li><p><strong>S</strong> follows the trajectory of steps, but are seen as stochastic sampling from the env</p>
</li>
<li><p>A is also sampled by the strategy Pi</p>
</li>
<li><p>collect this trajectory by interacting with the env</p>
</li>
<li><p>then it would be like this</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426104602682.png" alt="image-20230426104602682"></p>
<h4 id="2-optimization"><a href="#2-optimization" class="headerlink" title="(2). optimization"></a>(2). optimization</h4><ul>
<li>in trust region, update parameter,</li>
</ul>
<p><img src="/home/sky/.config/Typora/typora-user-images/image-20230426105534532.png" alt="image-20230426105534532"></p>
<p>it’s an optimization problem, we construct the optimization problem, then throw to optimization solver to solve it.</p>
<h4 id="3-pseudocode"><a href="#3-pseudocode" class="headerlink" title="(3). pseudocode"></a>(3). pseudocode</h4><p><img src="/home/sky/.config/Typora/typora-user-images/image-20230426120749975.png" alt="image-20230426120749975"></p>
<p>In one cycle, the strategy network is updated each time, and one game is played to obtain a trajectory. However, in maximization, there are multiple inner cycles required by optimization problems , which are usually solved by gradient projection algorithm.</p>
<ul>
<li>2 hyperparameters 4 maximization: <ul>
<li>Step size of gradient descent, </li>
<li>radius of confidence region</li>
</ul>
</li>
</ul>
<h3 id="1-2-PPO"><a href="#1-2-PPO" class="headerlink" title="1.2 PPO"></a>1.2 PPO</h3><ul>
<li>PPO version 1: add constraint into cost function</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426120941839.png" alt="image-20230426120941839"></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426120839455.png" alt="image-20230426120839455"></p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426123355644.png" alt="image-20230426123355644"></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
<table>
<thead>
<tr>
<th></th>
<th>cons</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>1. fails on simple problems; <br />2. poorly understood</td>
</tr>
<tr>
<td>VPG</td>
<td>1. poor data efficiency <br />2. poor robustness</td>
</tr>
<tr>
<td>trpo</td>
<td>1. complicated <br />2. not compatible with noise ( like dropout)+ data sharing</td>
</tr>
<tr>
<td>ppo</td>
<td>1. good data efficiency<br />2. reliable profermance<br />3. only first -order optimization</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<ol start="2">
<li><h3 id="pieter-abbeel-rl-course"><a href="#pieter-abbeel-rl-course" class="headerlink" title="pieter abbeel rl course"></a>pieter abbeel rl course</h3></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2GwBez0D20A&t=130s">https://www.youtube.com/watch?v=2GwBez0D20A&amp;t=130s</a></p>
<p>2.1 : MDP</p>
<p><em><strong>insight: group of robot learn faster: data sharing, more efficient sampling of the env</strong></em></p>
<ul>
<li>groups of robots learn faster, they can share date, more efficient sampling of the env, save wall time</li>
<li>gamma (discount factor) is also designed based on what our goal is, if we want the agent of care more about things happen in closer steps, then …</li>
<li>if gamma is introduced, state take less  steps to the reward is with higher value, it’s like the “time” of game world. but it should not be same as the future evaluated in our real world.</li>
<li>update: in grid world,  we swap a time for all the grid , what if we use different way to swap all the states?<ul>
<li>like along trajectory</li>
<li>like importance sampling</li>
</ul>
</li>
<li>Discount factor influence convergence: 0: faster 1: longer</li>
<li>why it converge</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426134554214.png" alt="image-20230426134554214"></p>
<h5 id="effect-of-discount-and-noise"><a href="#effect-of-discount-and-noise" class="headerlink" title="effect of discount and noise"></a>effect of discount and noise</h5><p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426150334001.png" alt="image-20230426150334001"></p>
<p>(a)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426171844052.png" alt="image-20230426171844052"></p>
<p>(b)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426171913044.png" alt="image-20230426171913044"></p>
<p>(c)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426171946200.png" alt="image-20230426171946200"></p>
<p>(d)</p>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426172016159.png" alt="image-20230426172016159"></p>
<ul>
<li>update for Q*  , as default,  the agent thereafter acting optimally</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426172142184.png" alt="image-20230426172142184"></p>
<ul>
<li>policy evaluation</li>
</ul>
<h3 id="Max-entropy"><a href="#Max-entropy" class="headerlink" title="Max entropy"></a>Max entropy</h3><ul>
<li><p>how do we collect data</p>
<ul>
<li>use current policy to collect data	, if policy is deterministic , data collection would not be interesting</li>
<li>with entropy, policy will be with more variation in how the data is collected</li>
</ul>
</li>
<li><p>entropy</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230426173232834.png" alt="image-20230426173232834"></p>
<ul>
<li><p>a distribution over near-optimal solution</p>
<ul>
<li><strong>robust policy</strong>:  the env could change, if it’s distribution instead of deterministic it’s more robust</li>
<li><strong>robust learning</strong> : we can keep collecting exploratory data during learning</li>
</ul>
</li>
<li><p>collect along learning, or collect with exploration , then off-policy update</p>
</li>
<li><p>insights: how about when the best action changed in a state , increase the   possibility of explore and update to former states,( like in my last paper, in board are , increase explore, in narrow area, reduce explore)</p>
</li>
<li><p>insights:  after update the value of a state , trace back and update all the former state (read the trace chapter of sutton book)</p>
</li>
</ul>
<h3 id="2-2-Q-learning"><a href="#2-2-Q-learning" class="headerlink" title="2.2 : Q learning"></a>2.2 : Q learning</h3><p><strong>properties</strong></p>
<ul>
<li>converge even if act suboptimal (epsilon greedy)</li>
<li>epsilon decay: if not do decay , latest experience will make you hop around</li>
<li>epsilon: u need to make it small eventually </li>
<li>epsilon: cannot decay too fast: cannot update enough</li>
</ul>
<p><strong>requirement</strong></p>
<ul>
<li>state and actions are visited infinitely often: doesn’t matter how u select actions</li>
<li>learning rate schedule: <ul>
<li>reference: <a target="_blank" rel="noopener" href="https://dspace.mit.edu/bitstream/handle/1721.1/7205/AIM-1441.pdf?sequence=2">On the Convergence of Stochastic Iterative Dynamic …</a></li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230501182944684.png" alt="image-20230501182944684"></p>
<h3 id="play-with-rl"><a href="#play-with-rl" class="headerlink" title="play with rl"></a>play with rl</h3><ul>
<li>rl playground</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://rlplaygrounds.com/">https://rlplaygrounds.com/</a></p>
<ul>
<li><p>openai gym</p>
</li>
<li><p>deep mind control</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/deepmind/dm_control">https://github.com/deepmind/dm_control</a></p>
<ul>
<li>Unity ML-Agents (</li>
</ul>
<p> <a target="_blank" rel="noopener" href="https://github.com/Unity-Technologies/ml-agents">https://github.com/Unity-Technologies/ml-agents</a> )</p>
<ul>
<li>course from neptune</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses">https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses</a></p>
<ul>
<li>easy game to visualize reinforcement learning</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/reinforcement-learning-and-visualisation-with-a-simple-game-a1fe725f0509">https://towardsdatascience.com/reinforcement-learning-and-visualisation-with-a-simple-game-a1fe725f0509</a></p>
<p>when state space is large, the update of qtable will be very slow…</p>
<h3 id="paper-reading-list"><a href="#paper-reading-list" class="headerlink" title="paper reading list:"></a><em>paper reading list:</em></h3><ul>
<li><em>playing atari with drl</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.5602">https://arxiv.org/abs/1312.5602</a></li>
<li><em>rainbow</em>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></li>
<li><em>ppo</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></li>
<li><em>mappo</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01955">https://arxiv.org/abs/2103.01955</a></li>
<li><em>maven</em> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.07483">https://arxiv.org/abs/1910.07483</a></li>
<li>qmix <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11485">https://arxiv.org/abs/1803.11485</a></li>
<li>maddpg <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02275">https://arxiv.org/abs/1706.02275</a></li>
<li>coma <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.08926">https://arxiv.org/abs/1705.08926</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/week%206%20share/" data-id="clham94tk0001h1ot9v43b4jy" data-title="week6" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-week 8" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/05/week%208/" class="article-date">
  <time class="dt-published" datetime="2023-05-05T09:19:40.000Z" itemprop="datePublished">2023-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/05/week%208/">week8&amp;9</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="week-8-amp-9"><a href="#week-8-amp-9" class="headerlink" title="week 8&amp;9"></a>week 8&amp;9</h1><h3 id="Goal-这周的目标"><a href="#Goal-这周的目标" class="headerlink" title="Goal(这周的目标)"></a>Goal(这周的目标)</h3><ul>
<li><p>rl核心</p>
<ul>
<li><strong>基于梯度下降的内核</strong></li>
<li><strong>q表怎么扫，环境中什么轨迹，怎么采样，怎么更新</strong></li>
<li><strong>神经网络本身特性带来的一些东西</strong></li>
<li><strong>神经网络的作用就是状态和动作空间太大，那我们给他缩小，最后我们用缩小的q表去采取动作（generalize function）</strong><ul>
<li>generalize func的性能没有NN好</li>
</ul>
</li>
<li>而pg只是基于价值大小提高动作概率，只是argmax的一种连续版本</li>
</ul>
</li>
<li><p>群体学习的解决方式（我的目的不是发论文，而是探索，mastering）</p>
</li>
<li><p>随机性的问题，也就是探索的问题，探索环境状态的问题</p>
</li>
<li><p>playing atari中的提到的神经网络的优点</p>
</li>
<li><p>huggingface中提到的深度神经网络的优点</p>
<ul>
<li>q表在大型空间中无效（为啥，可以缩小，模糊神经网络）</li>
</ul>
</li>
<li><p>rlbook中提到神经网络的优点</p>
</li>
<li><p>把搜索方法应用到对mdp空间的探索中，比如树搜索，比如rrt，比如采样方法</p>
<ul>
<li>我们基于简单的mdp找出最佳的探索方法，再应用到复杂的，无法画出关系图的mdp中</li>
<li><img src="https://raw.githubusercontent.com/magiclucky1996/picgo/main/image-20230508115549166.png" alt="image-20230508115549166"></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left"></th>
<th>星期一</th>
<th>星期二</th>
<th>星期三</th>
<th>星期四</th>
</tr>
</thead>
<tbody><tr>
<td align="left">抱脸虫教程（往后放，先解决问题）</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">在env测试不同算法进行对比</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">做一个图对比各种采样的策略：softmax，epsilon greedy，fully explore</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">做一个图对比各种对模型的更新</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">对比使用神经网路和不使用神经网络（缩小的q表，q也可以根据价值选取动作概率，，所以为什么使用神经网络）</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Q1：-why-neural-network？"><a href="#Q1：-why-neural-network？" class="headerlink" title="Q1： why neural network？"></a>Q1： why neural network？</h3><h6 id="q-table-could-be-approximate-actor-could-be-like-argmax"><a href="#q-table-could-be-approximate-actor-could-be-like-argmax" class="headerlink" title="q table could be approximate, actor could be like argmax"></a>q table could be approximate, actor could be like argmax</h6><ul>
<li><h2 id="rlbook-Generalize-problems-has-already-been-studied-generalization-when-a-single-state-is-updated-the-change-generalizes-from-that-state-to-a↵ect-the-values-of-many-other-states-supervised-learning-from-examples-including-artiﬁcial-neural-networks-decision-trees-and-various-kinds-of-multivariate-regression-not-all-function-approximation-methods-well-suited-for-rl-If-online-needed-to-learn-efficiently-from-incrementally-acquired-data-target-is-nonstationery（policy-is-not-stationery-td-is-not-stationery）"><a href="#rlbook-Generalize-problems-has-already-been-studied-generalization-when-a-single-state-is-updated-the-change-generalizes-from-that-state-to-a↵ect-the-values-of-many-other-states-supervised-learning-from-examples-including-artiﬁcial-neural-networks-decision-trees-and-various-kinds-of-multivariate-regression-not-all-function-approximation-methods-well-suited-for-rl-If-online-needed-to-learn-efficiently-from-incrementally-acquired-data-target-is-nonstationery（policy-is-not-stationery-td-is-not-stationery）" class="headerlink" title="rlbook- Generalize problems has already been studied- generalization: when a single state is updated, the change generalizes from that state to a↵ect the values of many other states- supervised learning from examples, including artiﬁcial neural networks, decision trees, and various kinds of multivariate regression- not all function approximation methods well suited for rl  - If online, needed to learn efficiently from incrementally acquired data  - target is nonstationery（policy is not stationery, td is not stationery）"></a>rlbook<br>- Generalize problems has already been studied<br>- generalization: when a single state is updated, the change generalizes from that state to a↵ect the values of many other states<br>- supervised learning from examples, including artiﬁcial neural networks, decision trees, and various kinds of multivariate regression<br>- not all function approximation methods well suited for rl<br>  - If online, needed to learn efficiently from incrementally acquired data<br>  - target is nonstationery（policy is not stationery, td is not stationery）</h2></li>
<li><p>DQN paper</p>
</li>
<li><p>chatgpt</p>
</li>
<li><p>nn could represent more complex relationships: so it’s more complex than just 在q 表中取近似值</p>
</li>
</ul>
<h3 id="Q2-the-explore-of-state-action-space"><a href="#Q2-the-explore-of-state-action-space" class="headerlink" title="Q2: the explore of  state-action space:"></a>Q2: the explore of  state-action space:</h3><h6 id="why-actor-used-to-sample-softmax-epsilon-greedy-fully-explore-contraction-between-them"><a href="#why-actor-used-to-sample-softmax-epsilon-greedy-fully-explore-contraction-between-them" class="headerlink" title="why actor used to sample(softmax, epsilon greedy, fully explore), contraction between them"></a>why actor used to sample(softmax, epsilon greedy, fully explore), contraction between them</h6><h3 id="Q3-the-explore-of-state-action-space"><a href="#Q3-the-explore-of-state-action-space" class="headerlink" title="Q3: the explore of  state-action space:"></a>Q3: the explore of  state-action space:</h3><h6 id="Can-search-methods-be-applied-like-A-RRT"><a href="#Can-search-methods-be-applied-like-A-RRT" class="headerlink" title="Can search methods be applied? like A*, RRT"></a>Can search methods be applied? like A*, RRT</h6><h4 id="Q4-the-explore-of-state-action-space"><a href="#Q4-the-explore-of-state-action-space" class="headerlink" title="Q4: the explore of state-action space:"></a>Q4: the explore of state-action space:</h4><h6 id="how-to-scan-the-q-table"><a href="#how-to-scan-the-q-table" class="headerlink" title="how to scan the q table?"></a>how to scan the q table?</h6><h3 id="Questions"><a href="#Questions" class="headerlink" title="Questions:"></a>Questions:</h3><ol>
<li><p>why q table replaced by NN, what is the discipline to represent? （q表换成q神经网络的意义是什么）</p>
</li>
<li><p>difference b<br>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>
</li>
<li><p>fully explore 和 epsilon greedy</p>
</li>
<li><h1 id="why-actor-better-than-argmax"><a href="#why-actor-better-than-argmax" class="headerlink" title="why actor better than argmax"></a>why actor better than argmax</h1></li>
<li><p>fully explore 和 epsilon greedy（一开始完全探索）</p>
</li>
<li><p>why actor better than argmax（actor网络和argmax1的区别是什么）</p>
</li>
<li><p>神经网络本身的特性会带来哪些东西，神经网络的优点是什么</p>
</li>
<li><p>q learning能否动作是概率分布，为什么不行?</p>
<ol>
<li>可以，使用softmax作为探索的策略，<strong>和epsilon greedy的区别</strong>：会按照q值选取动作，不能保证对动作空间的充分探索</li>
<li>在探索时，选取最优动作，和选取没选取过动作的区别：</li>
<li><strong>探索的衡量指标</strong>：做一个图对比各种采样的策略</li>
</ol>
</li>
</ol>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>9c004c5 (modify week 8)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h3 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h3><ul>
<li>fully explore（完全探索）</li>
</ul>
<p>This approach is called pure exploration and exploitation (PEE) and can be used in some cases where exploration is very costly or where the environment is very simple. However, in most real-world scenarios, PEE is not an optimal approach.</p>
<p>The problem with PEE is that the agent spends a lot of time exploring and collecting data, but not enough time exploiting that data to improve its policy. This can lead to slow learning and poor performance, especially in complex environments where there are many possible actions and states.</p>
<p>In contrast, most RL algorithms use a balance between exploration and exploitation, where the agent takes actions that are likely to yield high rewards based on its current policy while also occasionally exploring new actions or states. This allows the agent to learn quickly while still exploring new possibilities, leading to faster learning and better performance.</p>
<p>Furthermore, in many RL problems, the environment is dynamic and can change over time. In such cases, it is important for the agent to continuously explore and adapt to changes in the environment to maintain optimal performance. This requires a balance between exploration and exploitation, as well as the ability to update the policy based on new data and experiences.</p>
<p>In summary, while PEE can be a useful approach in some cases, a balanced approach between exploration and exploitation is generally more effective for most RL problems.</p>
<p>我用fully explore探索，然后得到的数据存在buffer里，然后用我的策略进行学习，差策略探测到的数据和提升后的策略探索到的数据的区别是什么，数据是一个transition（s a r s ）好的策略往往会偏向于采取好的动作，转移到好的动作</p>
<ul>
<li>好的策略会选好的动作，到好的效率上，efficiency会很高，比如我一直用随机策略采样，然后用</li>
<li>坏的策略会选坏的动作，到不好的轨迹上，</li>
</ul>
<p>所以现在两个最核心的问题：</p>
<ol>
<li>Fully 随机策略就只是影响轨迹，进而影响采样效率吗</li>
<li>看看rlbook里资格迹的概念</li>
<li>看看里面q表怎么扫的</li>
<li>基于q的方法和基于policy的方法本质是一样的，都是value iteration，通过r来提升value，最后通过value去驱动actor，其实本质仍然是用r去驱动一切，核心就在于r怎么去提升策略，即每个状态做什么动作，如何用r去帮助这个状态做什么动作，本质就是看看哪个动作得到的r多，一种是累积r，一种是最终r，现实世界可能都是最终r，但是这样很难学习，所以我们一般先给累积r，然后最后通过最终r去实现，，，</li>
</ol>
<p>所以在这个状态选哪个好呢，一种是sample一下，一种是sample到尽头，sample到一下，sample到一步的r，那我必须bootstrap，来得到当前状态选取某个动作的value，也可以sample轨迹，几个轨迹平均一下，看看动作的value，都是计算动作的价值，如果是连续动作空间怎么办呢，，，，那其实两个在参数上相近的（s，a）对应的价值可能差的很大</p>
<h1 id="lt-lt-lt-lt-lt-lt-lt-HEAD-Incoporate-mappo-with-sumo"><a href="#lt-lt-lt-lt-lt-lt-lt-HEAD-Incoporate-mappo-with-sumo" class="headerlink" title="&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD# Incoporate  mappo with sumo"></a>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD<br># Incoporate  mappo with sumo</h1><blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>9c004c5 (modify week 8)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h2 id="incoporate-mappo-with-sumo（代码部分：在sumo中实验信号灯控制）"><a href="#incoporate-mappo-with-sumo（代码部分：在sumo中实验信号灯控制）" class="headerlink" title="incoporate  mappo with sumo（代码部分：在sumo中实验信号灯控制）"></a>incoporate  mappo with sumo（代码部分：在sumo中实验信号灯控制）</h2><h3 id="1-look-for-existing-mappo-sumo"><a href="#1-look-for-existing-mappo-sumo" class="headerlink" title="1. look for existing mappo + sumo"></a>1. look for existing mappo + sumo</h3><ul>
<li>reference</li>
</ul>
<ol>
<li>github</li>
<li>paper</li>
<li>resource of course era, presentation, tutorial (osint)</li>
</ol>
<ul>
<li>resources found</li>
</ul>
<p>github</p>
<ol>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control">https://github.com/maxbren/Multi-Agent-Distributed-PPO-Traffc-light-control</a></p>
</li>
<li><p>light mappo  <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/light_mappo">https://github.com/magiclucky1996/light_mappo</a> 基于这个写一下试试</p>
</li>
<li><p>q&#x2F;ac + sumo <a target="_blank" rel="noopener" href="https://github.com/magiclucky1996/deeprl_signal_control">https://github.com/magiclucky1996/deeprl_signal_control</a></p>
</li>
<li><p>mappo + mujoco <a target="_blank" rel="noopener" href="https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation">https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation</a></p>
</li>
<li><p>mappo&#x2F;qmix&#x2F;maddpg + mpe <a target="_blank" rel="noopener" href="https://github.com/Lizhi-sjtu/MARL-code-pytorch">https://github.com/Lizhi-sjtu/MARL-code-pytorch</a></p>
</li>
<li><p>ppo + sumo <a target="_blank" rel="noopener" href="https://github.com/YanivHacker/RLTrafficManager">https://github.com/YanivHacker/RLTrafficManager</a></p>
</li>
<li><p>noisy mappo+  <a target="_blank" rel="noopener" href="https://github.com/hijkzzz/noisy-mappo">https://github.com/hijkzzz/noisy-mappo</a></p>
</li>
</ol>
<p>papers</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors">https://ieeexplore.ieee.org/abstract/document/9549970/authors#authors</a> 东北信息学院：提到了rl奖励稀疏的问题，然后他们给rl设计更多的reward 引导它学习。但是你怎么知道什么样的reward能够引导，这不是还是在reward function设计的范围里吗？</p>
<ul>
<li>insights<ol>
<li>sumo设计的问题，如果我们设一个控制周期之后的交通流状态为reward是不是不合理，怎么样去评价智能体schedule的好坏呢，怎么去评价智能体的action改善了交通呢，我是要搞交通呢，还是要搞rl呢，还是要搞啥，</li>
<li>上次会议的要点：1. insight可以给硕士做，但是要具体可行 2. 做一个oncoming 会议的scheduling(rl 多智能体 交通 计算机 人工智能) 3. sumo的模型可以封装好给本科生用 4.</li>
</ol>
</li>
<li>今天的计划（4.24）</li>
</ul>
<p>work 到12 点半，吃中饭，吃完中饭一点消化一会儿，回实验室一点半，回来继续work,work到两点多的时候睡午觉，五点跑路，去看看有没有吃的，不行就回家supervalu,晚上回家继续work一会儿，今天的弄完走之前deploy 和 push上去</p>
<ul>
<li>how other people implement marl training</li>
</ul>
<p>make contraction between these projects</p>
<p>分解成detailed steps</p>
<p>1.首先看下mappo的代码和deeprl sumo的代码以及ppo sumo代码（只做有必要做的事情）</p>
<ol start="2">
<li><p>做完1大概知道要干嘛，可以看看ppo trpo mappo</p>
</li>
<li><p>把每周4上午留作整理时间，所以我必须两天内搞定这个代码整定的事情，然后再用剩下的时间学习ppo trpo mappo，还要学sumo部分的东西，但是得非常快</p>
</li>
<li></li>
<li></li>
</ol>
<p><strong>reading of the rlbook</strong></p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>
<p>how does dopamine give people motivation?</p>
<p>if prediction is different from truth, we will have positive feedback or negative feedback?</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h3 id="2-make-it-with-materials-found"><a href="#2-make-it-with-materials-found" class="headerlink" title="2. make it with materials found"></a>2. make it with materials found</h3><blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>9c004c5 (modify week 8)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<p><strong>生活的star：</strong></p>
<p>piano</p>
<p>上海美术厂</p>
<p>breaking</p>
<p>penetration testing</p>
<p>avoid social media</p>
<h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><h3 id="抱脸虫教程"><a href="#抱脸虫教程" class="headerlink" title="抱脸虫教程"></a>抱脸虫教程</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff?fw=pt">https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff?fw=pt</a></p>
<ul>
<li>explore&#x2F;exploit权衡：本质是什么：如果exploit，就会一直选择目前的最优解，但是目前的最优解怎么得来，初始化的时候怎么选择，，所以用epsilong decay的epsilon greedy</li>
<li>如果我一开始完全探索，像value表格一样，如何扫过去（也就是如何更新q值），看看rl book，</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://magiclucky1996.github.io/2023/05/05/week%208/" data-id="clham94tm0004h1ot2tk2g0wv" data-title="week8&amp;9" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/work/" rel="tag">work</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/work/" style="font-size: 20px;">work</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/05/15/try%20hack%20me-%20penetration%20tester/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/15/workshop%20pre1:%20survey%20on%20traffic/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/15/how%20to%20make%20face%20fat/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/05/10/%E9%81%93/">道</a>
          </li>
        
          <li>
            <a href="/2023/05/05/dublin_remote_work/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 tianxiang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>